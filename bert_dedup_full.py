#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
BERT å»é‡å®Œæ•´ç‰ˆ
ä½¿ç”¨ BERT è¨ˆç®—æ›¸ç±æ¨™é¡Œç›¸ä¼¼åº¦é€²è¡Œå»é‡
è™•ç†æ‰€æœ‰åˆ†ç¾¤æª”æ¡ˆ
"""
import pandas as pd
import numpy as np
import os
import re
import time
import logging
from datetime import datetime
from tqdm import tqdm
from sentence_transformers import SentenceTransformer, util

print("=" * 80)
print("BERT å»é‡ç³»çµ± - å®Œæ•´ç‰ˆ")
print("=" * 80)

# ==================== è¨­å®š ====================
CLUSTERED_DATA_DIR = "clustered_data"
OUTPUT_FILE = "final_merged_bert_processed.csv"
SIMILARITY_THRESHOLD = 0.99  # ç›¸ä¼¼åº¦é–¾å€¼ï¼ˆ0-1ï¼‰ï¼Œè¶…éæ­¤å€¼è¦–ç‚ºåŒä¸€æœ¬æ›¸
LOG_FILE = f"bert_processing_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"

# ==================== æ—¥èªŒè¨­å®š ====================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(message)s',
    handlers=[
        logging.FileHandler(LOG_FILE, encoding='utf-8'),
    ]
)

def log_and_print(message):
    """åŒæ™‚è¼¸å‡ºåˆ°çµ‚ç«¯å’Œ log"""
    print(message)
    logging.info(message)

# ==================== è¼‰å…¥ BERT æ¨¡å‹ ====================
log_and_print("\n[1/5] è¼‰å…¥ BERT æ¨¡å‹...")
log_and_print("ä½¿ç”¨æ¨¡å‹: paraphrase-multilingual-MiniLM-L12-v2 (æ”¯æ´ä¸­æ–‡)")

try:
    model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
    log_and_print(">> æ¨¡å‹è¼‰å…¥å®Œæˆï¼")
except Exception as e:
    log_and_print(f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
    log_and_print("è«‹å…ˆåŸ·è¡Œ: pip install sentence-transformers")
    exit(1)

# ==================== æ¸…ç†æ¨™é¡Œå‡½æ•¸ ====================
def clean_title(title):
    """
    æ¸…ç†æ›¸ç±æ¨™é¡Œï¼Œç§»é™¤ã€Œé›»å­æ›¸ã€ç­‰å­—æ¨£
    """
    if pd.isna(title) or not str(title).strip():
        return ""
    
    title = str(title).strip()
    
    # ç§»é™¤å¸¸è¦‹çš„é›»å­æ›¸æ¨™è¨˜
    patterns_to_remove = [
        r'\(é›»å­æ›¸\)',
        r'ï¼ˆé›»å­æ›¸ï¼‰',
        r'\[é›»å­æ›¸\]',
        r'ã€é›»å­æ›¸ã€‘',
        r'é›»å­æ›¸',
        r'\(ebook\)',
        r'ï¼ˆebookï¼‰',
        r'ebook',
        r'e-book',
    ]
    
    for pattern in patterns_to_remove:
        title = re.sub(pattern, ' ', title, flags=re.IGNORECASE)
    
    # æ¸…ç†å¤šé¤˜ç©ºæ ¼
    title = ' '.join(title.split())
    
    return title.strip()

# ==================== BERT ç›¸ä¼¼åº¦æ¯”è¼ƒ ====================
def check_same_book_with_bert(title1, title2, model, threshold=SIMILARITY_THRESHOLD):
    """ä½¿ç”¨ BERT è¨ˆç®—å…©å€‹æ¨™é¡Œçš„ç›¸ä¼¼åº¦"""
    if not title1 or not title2:
        return False, 0.0
    
    # æ¸…ç†æ¨™é¡Œ
    cleaned_title1 = clean_title(title1)
    cleaned_title2 = clean_title(title2)
    
    if not cleaned_title1 or not cleaned_title2:
        return False, 0.0
    
    # è¨ˆç®— embeddings
    embedding1 = model.encode(cleaned_title1, convert_to_tensor=True)
    embedding2 = model.encode(cleaned_title2, convert_to_tensor=True)
    
    # è¨ˆç®— cosine ç›¸ä¼¼åº¦
    similarity = util.cos_sim(embedding1, embedding2).item()
    
    # åˆ¤æ–·æ˜¯å¦ç‚ºåŒä¸€æœ¬æ›¸
    is_same = similarity >= threshold
    
    return is_same, similarity

# ==================== åˆä½µæ›¸ç±å‡½æ•¸ ====================
def merge_books_properly(book1, book2):
    """
    æ­£ç¢ºåˆä½µå…©æœ¬æ›¸çš„è³‡æ–™ï¼ˆå¾ process_books.py è¤‡è£½çš„é‚è¼¯ï¼‰
    book1: è¢«åˆä½µè€…
    book2: åˆä½µè€…
    """
    merged = {}
    
    # TAICCA_ID ç³»åˆ—ï¼šä»¥æ–œç·šåˆ†éš”
    for col in ['NEW_TAICCA_ID', '1106äº¦å¼ID', 'TAICCA_ID']:
        val1 = str(book1.get(col, '')).strip()
        val2 = str(book2.get(col, '')).strip()
        if pd.notna(book1.get(col)) and pd.notna(book2.get(col)):
            if val1 and val2 and val1 != val2:
                merged[col] = f"{val1} / {val2}"
            elif val1:
                merged[col] = val1
            elif val2:
                merged[col] = val2
        elif pd.notna(book1.get(col)):
            merged[col] = val1
        elif pd.notna(book2.get(col)):
            merged[col] = val2
        else:
            merged[col] = ''
    
    # isbn ç³»åˆ—ï¼šç‰¹æ®Šè™•ç†
    for col in ['isbn', 'eisbn']:
        val1 = str(book1.get(col, '')).strip() if pd.notna(book1.get(col)) else ''
        val2 = str(book2.get(col, '')).strip() if pd.notna(book2.get(col)) else ''
        
        if val1 and val2 and val1 != val2:
            merged[col] = f"{val1} / {val2}"
        elif val1 and not val2:
            merged[col] = f"{val1} / ï¼ˆç©ºç™½ï¼‰"
        elif not val1 and val2:
            merged[col] = f"ï¼ˆç©ºç™½ï¼‰/ {val2}"
        elif val1:
            merged[col] = val1
        else:
            merged[col] = ''
    
    # ç›´æ¥å¡«è£œçš„æ¬„ä½
    fill_cols = [
        'bookscom_isbn', 'kobo_isbn', 'readmoo_isbn', 'bookscom_eisbn', 'kobo_eisbn', 'readmoo_eisbn',
        'production_id', 'bookscom_production_id', 'kobo_production_id', 'readmoo_production_id',
        'bookscom_title', 'kobo_title', 'readmoo_title',
        'bookscom_processed_title', 'kobo_processed_title', 'readmoo_processed_title',
        'bookscom_original_title', 'kobo_original_title', 'readmoo_original_title',
        'bookscom_author', 'kobo_author', 'readmoo_author',
        'bookscom_translator', 'kobo_translator', 'readmoo_translator',
        'bookscom_publisher', 'kobo_publisher', 'readmoo_publisher',
        'bookscom_publish_date', 'kobo_publish_date', 'readmoo_publish_date',
        'bookscom_original_price', 'kobo_original_price', 'readmoo_original_price',
        'bookscom_category', 'kobo_category', 'readmoo_category',
        'kobo_type_ebook', 'readmoo_type_ebook',
        'bookscom_url', 'kobo_url', 'readmoo_url'
    ]
    
    for col in fill_cols:
        if pd.notna(book1.get(col)) and str(book1.get(col)).strip():
            merged[col] = book1[col]
        elif pd.notna(book2.get(col)) and str(book2.get(col)).strip():
            merged[col] = book2[col]
        else:
            merged[col] = ''
    
    # ä¿ç•™è¢«åˆä½µè€…çš„å…§å®¹
    keep_from_book1 = [
        'title', 'å‚™è¨»', 'processed_title', 'original_title',
        'author', 'translator', 'publisher'
    ]
    
    for col in keep_from_book1:
        merged[col] = book1.get(col, '')
    
    merged['Clean_publisher'] = book1.get('Clean_publisher', '')
    merged['æœªç´å…¥æ›¸ç›®FIND'] = book1.get('æœªç´å…¥æ›¸ç›®FIND', '')
    
    # min_publish_dateï¼šæœ€æ—©æ—¥æœŸ
    dates = []
    for col in ['min_publish_date', 'bookscom_publish_date', 'kobo_publish_date', 'readmoo_publish_date']:
        for book in [book1, book2]:
            if pd.notna(book.get(col)) and str(book.get(col)).strip():
                try:
                    date_str = str(book[col]).strip()
                    if '/' in date_str:
                        date_obj = datetime.strptime(date_str, '%Y/%m/%d')
                    elif '-' in date_str:
                        date_obj = datetime.strptime(date_str, '%Y-%m-%d')
                    else:
                        continue
                    dates.append(date_obj)
                except:
                    pass
    
    if dates:
        merged['min_publish_date'] = min(dates).strftime('%Y-%m-%d')
        merged['max_publish_date'] = max(dates).strftime('%Y-%m-%d')
    else:
        merged['min_publish_date'] = book1.get('min_publish_date', '')
        merged['max_publish_date'] = book1.get('max_publish_date', '')
    
    # priceï¼šæœ€å¤§å€¼
    prices = []
    for col in ['price', 'bookscom_original_price', 'kobo_original_price', 'readmoo_original_price']:
        for book in [book1, book2]:
            if pd.notna(book.get(col)):
                try:
                    price = float(book[col])
                    prices.append(price)
                except:
                    pass
    
    if prices:
        merged['price'] = max(prices)
    else:
        merged['price'] = book1.get('price', '')
    
    return merged

# ==================== è™•ç†åˆ†ç¾¤æª”æ¡ˆ ====================
def process_cluster_file_bert(csv_file, model):
    """
    ä½¿ç”¨ BERT è™•ç†å–®å€‹åˆ†ç¾¤æª”æ¡ˆ
    """
    filename = os.path.basename(csv_file)
    
    df = pd.read_csv(csv_file, encoding='utf-8-sig')
    
    if len(df) == 0:
        logging.warning(f"{filename}: æ²’æœ‰è³‡æ–™")
        return [], 0, 0
    
    # å¦‚æœåªæœ‰ 1 ç­†è³‡æ–™ï¼Œç›´æ¥è¿”å›
    if len(df) <= 1:
        logging.info(f"{filename}: åªæœ‰ {len(df)} ç­†è³‡æ–™ï¼Œç›´æ¥è¼¸å‡º")
        return df.to_dict('records'), 0, 0
    
    logging.info(f"é–‹å§‹è™•ç†: {filename}, {len(df)} ç­†è³‡æ–™")
    
    books = df.to_dict('records')
    merged_indices = set()
    result_books = []
    comparisons = 0
    merges = 0
    
    for i in range(len(books)):
        if i in merged_indices:
            continue
        
        current_book = books[i]
        found_match = False
        
        for j in range(i + 1, len(books)):
            if j in merged_indices:
                continue
            
            compare_book = books[j]
            
            # ä½¿ç”¨ processed_title æˆ– title
            title1 = str(current_book.get('processed_title', current_book.get('title', ''))).strip()
            title2 = str(compare_book.get('processed_title', compare_book.get('title', ''))).strip()
            
            if not title1 or not title2:
                continue
            
            comparisons += 1
            is_same, similarity = check_same_book_with_bert(title1, title2, model)
            
            if is_same:
                # è¨˜éŒ„æ‰¾åˆ°çš„ç›¸åŒæ›¸ç±
                logging.info(f"{filename}: æ‰¾åˆ°ç›¸åŒæ›¸ç± (ç›¸ä¼¼åº¦: {similarity:.6f})")
                logging.info(f"  [{i}] {clean_title(title1)}")
                logging.info(f"  [{j}] {clean_title(title2)}")
                logging.info(f"  ID: {current_book.get('NEW_TAICCA_ID', '')} + {compare_book.get('NEW_TAICCA_ID', '')}")
                
                # ä½¿ç”¨æ­£ç¢ºçš„åˆä½µé‚è¼¯
                merged_book = merge_books_properly(compare_book, current_book)
                result_books.append(merged_book)
                
                merged_indices.add(i)
                merged_indices.add(j)
                merges += 1
                found_match = True
                break
        
        if not found_match:
            result_books.append(current_book)
    
    logging.info(f"{filename}: å®Œæˆ - {len(df)} ç­† â†’ {len(result_books)} ç­† (æ¯”è¼ƒ {comparisons} æ¬¡, åˆä½µ {merges} æ¬¡)")
    
    return result_books, comparisons, merges

# ==================== è®€å–æ‰€æœ‰åˆ†ç¾¤æª”æ¡ˆ ====================
log_and_print(f"\n[2/5] è®€å–åˆ†ç¾¤æª”æ¡ˆ...")

import glob
cluster_files = sorted(glob.glob(os.path.join(CLUSTERED_DATA_DIR, "cluster_*.csv")))
cluster_files = [f for f in cluster_files if 'full' not in f]

log_and_print(f">> æ‰¾åˆ° {len(cluster_files)} å€‹åˆ†ç¾¤æª”æ¡ˆ")
logging.info(f"æ‰¾åˆ° {len(cluster_files)} å€‹åˆ†ç¾¤æª”æ¡ˆ")

if len(cluster_files) == 0:
    log_and_print("âŒ æ²’æœ‰æ‰¾åˆ°åˆ†ç¾¤æª”æ¡ˆ")
    exit(1)

# å–å¾—åŸå§‹æ¬„ä½é †åº
original_columns = pd.read_csv(cluster_files[0], encoding='utf-8-sig').columns.tolist()

# ==================== è™•ç†æ‰€æœ‰åˆ†ç¾¤ ====================
log_and_print(f"\n[3/5] é–‹å§‹è™•ç†æ‰€æœ‰åˆ†ç¾¤...")
log_and_print(f"ç›¸ä¼¼åº¦é–¾å€¼: {SIMILARITY_THRESHOLD}")
log_and_print(f"è¼¸å‡ºæª”æ¡ˆ: {OUTPUT_FILE}")
log_and_print(f"Log æª”æ¡ˆ: {LOG_FILE}")

start_time = time.time()
total_original = 0
total_output = 0
total_comparisons = 0
total_merges = 0

# è™•ç†æ¯å€‹åˆ†ç¾¤æª”æ¡ˆä¸¦å³æ™‚å¯«å…¥
for idx, cluster_file in enumerate(tqdm(cluster_files, desc="è™•ç†åˆ†ç¾¤")):
    original_count = len(pd.read_csv(cluster_file, encoding='utf-8-sig'))
    total_original += original_count
    
    # å™ªéŸ³æª”æ¡ˆç›´æ¥å¯«å…¥
    is_noise_file = 'noise' in os.path.basename(cluster_file).lower()
    
    if is_noise_file:
        filename = os.path.basename(cluster_file)
        logging.info(f"è™•ç†å™ªéŸ³æª”æ¡ˆ: {filename}")
        
        df = pd.read_csv(cluster_file, encoding='utf-8-sig')
        df = df[[col for col in original_columns if col in df.columns]]
        
        if idx == 0:
            df.to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig', mode='w')
        else:
            df.to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig', mode='a', header=False)
        
        total_output += len(df)
        logging.info(f"{filename}: å™ªéŸ³æª”æ¡ˆï¼Œç›´æ¥å¯«å…¥ {len(df)} ç­†")
        continue
    
    # ä¸€èˆ¬åˆ†ç¾¤æª”æ¡ˆï¼šä½¿ç”¨ BERT æ¯”è¼ƒ
    results, comparisons, merges = process_cluster_file_bert(cluster_file, model)
    
    if results:
        result_df = pd.DataFrame(results)
        result_df = result_df[[col for col in original_columns if col in result_df.columns]]
        
        if idx == 0 and total_output == 0:
            result_df.to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig', mode='w')
        else:
            result_df.to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig', mode='a', header=False)
        
        total_output += len(result_df)
        total_comparisons += comparisons
        total_merges += merges

elapsed_time = time.time() - start_time

# ==================== çµ±è¨ˆå ±å‘Š ====================
log_and_print("\n[4/5] è™•ç†å®Œæˆï¼")
log_and_print("=" * 80)
log_and_print("çµ±è¨ˆå ±å‘Š")
log_and_print("=" * 80)
log_and_print(f"è™•ç†åˆ†ç¾¤æ•¸: {len(cluster_files)}")
log_and_print(f"åŸå§‹ç¸½ç­†æ•¸: {total_original}")
log_and_print(f"è¼¸å‡ºè³‡æ–™ç­†æ•¸: {total_output}")
log_and_print(f"åˆä½µæ¸›å°‘: {total_original - total_output} ç­† ({(total_original - total_output) / total_original * 100:.2f}%)")
log_and_print(f"ç¸½æ¯”è¼ƒæ¬¡æ•¸: {total_comparisons:,}")
log_and_print(f"å¯¦éš›åˆä½µæ¬¡æ•¸: {total_merges}")
log_and_print(f"ç›¸ä¼¼åº¦é–¾å€¼: {SIMILARITY_THRESHOLD}")
log_and_print(f"è™•ç†æ™‚é–“: {elapsed_time:.2f} ç§’ ({elapsed_time/60:.2f} åˆ†é˜ / {elapsed_time/3600:.2f} å°æ™‚)")

logging.info("=" * 80)
logging.info(f"è™•ç†å®Œæˆ - ç¸½æ™‚é–“: {elapsed_time:.2f} ç§’")
logging.info(f"åŸå§‹: {total_original} ç­† â†’ è¼¸å‡º: {total_output} ç­†")
logging.info(f"æ¸›å°‘: {total_original - total_output} ç­†")
logging.info("=" * 80)

# ==================== é©—è­‰è¼¸å‡º ====================
log_and_print(f"\n[5/5] é©—è­‰è¼¸å‡ºæª”æ¡ˆ...")

try:
    output_df = pd.read_csv(OUTPUT_FILE, encoding='utf-8-sig')
    log_and_print(f">> è¼¸å‡ºæª”æ¡ˆ: {OUTPUT_FILE}")
    log_and_print(f">> é©—è­‰ç­†æ•¸: {len(output_df)} ç­†")
    
    if len(output_df) == total_output:
        log_and_print(">> âœ… ç­†æ•¸é©—è­‰é€šéï¼")
    else:
        log_and_print(f">> âš ï¸ ç­†æ•¸ä¸ç¬¦: é æœŸ {total_output}, å¯¦éš› {len(output_df)}")
    
    logging.info(f"è¼¸å‡ºæª”æ¡ˆé©—è­‰: {len(output_df)} ç­†")
    
except Exception as e:
    log_and_print(f">> âŒ é©—è­‰å¤±æ•—: {e}")
    logging.error(f"è¼¸å‡ºæª”æ¡ˆé©—è­‰å¤±æ•—: {e}")

# ==================== å®Œæˆ ====================
log_and_print("\n" + "=" * 80)
log_and_print("ğŸ‰ BERT å»é‡è™•ç†å®Œæˆï¼")
log_and_print("=" * 80)
log_and_print(f"è¼¸å‡ºæª”æ¡ˆ: {OUTPUT_FILE}")
log_and_print(f"Log æª”æ¡ˆ: {LOG_FILE}")
log_and_print("=" * 80)

