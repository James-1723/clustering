import pandas as pd
import os
import glob
from openai import OpenAI
from tqdm import tqdm
import json
from datetime import datetime
import logging
import time

# ==================== è¨­å®š ====================
OPENAI_API_KEY = "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"  # è«‹æ›¿æ›æˆä½ çš„ API Key
client = OpenAI(api_key=OPENAI_API_KEY)

CLUSTERED_DATA_DIR = "clustered_data"
OUTPUT_FILE = "output.csv"

# Log æª”æ¡ˆè¨­å®šï¼ˆä½¿ç”¨æ™‚é–“æˆ³è¨˜ï¼‰
LOG_FILE = f"processing_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"

# å…¨åŸŸè¨ˆæ•¸å™¨
api_call_count = 0
merge_count = 0

# ==================== è¼”åŠ©å‡½æ•¸ ====================

def log_and_print(message, level='info'):
    """åŒæ™‚è¼¸å‡ºåˆ°çµ‚ç«¯å’Œ log æª”æ¡ˆ"""
    print(message)
    if level == 'info':
        logging.info(message)
    elif level == 'warning':
        logging.warning(message)
    elif level == 'error':
        logging.error(message)

# ==================== å‡½æ•¸å®šç¾© ====================

def check_same_book_with_llm(title1, title2):
    """
    ä½¿ç”¨ OpenAI LLM åˆ¤æ–·å…©æœ¬æ›¸æ˜¯å¦ç›¸åŒ
    
    å›å‚³: True (ç›¸åŒ) æˆ– False (ä¸åŒ)
    """
    global api_call_count
    
    prompt = f"""è«‹åˆ¤æ–·ä»¥ä¸‹å…©æœ¬æ›¸çš„æ¨™é¡Œæ˜¯å¦æŒ‡å‘åŒä¸€æœ¬æ›¸ã€‚
è«‹åªå›ç­” "YES" æˆ– "NO"ï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—ã€‚

æ›¸ç±1: {title1}
æ›¸ç±2: {title2}

åˆ¤æ–·æ¨™æº–ï¼š
- æ¨™é¡Œå®Œå…¨ç›¸åŒæˆ–åªæœ‰ç´°å¾®å·®ç•°ï¼ˆå¦‚æ¨™é»ç¬¦è™Ÿã€ç©ºæ ¼ï¼‰â†’ YES
- åŒä¸€ç³»åˆ—ä½†ä¸åŒé›†æ•¸ â†’ NO
- å®Œå…¨ä¸åŒçš„æ›¸ â†’ NO

å›ç­” (YES/NO):"""

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",  # ä½¿ç”¨è¼ƒä¾¿å®œçš„æ¨¡å‹
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€å€‹åœ–æ›¸é¤¨ç®¡ç†å°ˆå®¶ï¼Œå°ˆé–€åˆ¤æ–·æ›¸ç±æ˜¯å¦ç›¸åŒã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0,
            max_tokens=10
        )
        
        api_call_count += 1
        answer = response.choices[0].message.content.strip().upper()
        result = "YES" in answer
        
        # è¨˜éŒ„ API å‘¼å«çµæœ
        logging.info(f"API å‘¼å« #{api_call_count}: æ¯”è¼ƒ '{title1[:50]}...' vs '{title2[:50]}...' â†’ {result}")
        
        return result
        
    except Exception as e:
        error_msg = f"  âš ï¸ LLM åˆ¤æ–·éŒ¯èª¤: {e}"
        log_and_print(error_msg, 'error')
        return False


def merge_two_books(book1, book2):
    """
    åˆä½µå…©æœ¬æ›¸çš„è³‡æ–™
    book1: è¢«åˆä½µè€…ï¼ˆä¿ç•™å¤§éƒ¨åˆ†è³‡æ–™ï¼‰
    book2: åˆä½µè€…ï¼ˆæä¾›éƒ¨åˆ†è³‡æ–™ï¼‰
    
    å›å‚³: åˆä½µå¾Œçš„è³‡æ–™ï¼ˆdictï¼‰
    """
    global merge_count
    merge_count += 1
    
    # è¨˜éŒ„åˆä½µè³‡è¨Š
    logging.info(f"åˆä½µ #{merge_count}:")
    logging.info(f"  è¢«åˆä½µè€… TAICCA_ID: {book1.get('NEW_TAICCA_ID', 'N/A')}, Title: {book1.get('title', 'N/A')[:50]}")
    logging.info(f"  åˆä½µè€… TAICCA_ID: {book2.get('NEW_TAICCA_ID', 'N/A')}, Title: {book2.get('title', 'N/A')[:50]}")
    
    merged = {}
    
    # TAICCA_ID ç³»åˆ—ï¼šä»¥æ–œç·šåˆ†éš”
    for col in ['NEW_TAICCA_ID', '1106äº¦å¼ID', 'TAICCA_ID']:
        val1 = str(book1.get(col, '')).strip()
        val2 = str(book2.get(col, '')).strip()
        if pd.notna(book1.get(col)) and pd.notna(book2.get(col)):
            if val1 and val2 and val1 != val2:
                merged[col] = f"{val1} / {val2}"
            elif val1:
                merged[col] = val1
            elif val2:
                merged[col] = val2
        elif pd.notna(book1.get(col)):
            merged[col] = val1
        elif pd.notna(book2.get(col)):
            merged[col] = val2
        else:
            merged[col] = ''
    
    # isbn ç³»åˆ—ï¼šç‰¹æ®Šè™•ç†ï¼ˆç©ºç™½è¦æ¨™è¨»ï¼‰
    for col in ['isbn', 'eisbn']:
        val1 = str(book1.get(col, '')).strip() if pd.notna(book1.get(col)) else ''
        val2 = str(book2.get(col, '')).strip() if pd.notna(book2.get(col)) else ''
        
        if val1 and val2 and val1 != val2:
            merged[col] = f"{val1} / {val2}"
        elif val1 and not val2:
            merged[col] = f"{val1} / ï¼ˆç©ºç™½ï¼‰"
        elif not val1 and val2:
            merged[col] = f"ï¼ˆç©ºç™½ï¼‰/ {val2}"
        elif val1:
            merged[col] = val1
        else:
            merged[col] = ''
    
    # ç›´æ¥å¡«è£œçš„æ¬„ä½
    fill_cols = [
        'bookscom_isbn', 'kobo_isbn', 'readmoo_isbn', 'bookscom_eisbn', 'kobo_eisbn', 'readmoo_eisbn',
        'production_id', 'bookscom_production_id', 'kobo_production_id', 'readmoo_production_id',
        'bookscom_title', 'kobo_title', 'readmoo_title',
        'bookscom_processed_title', 'kobo_processed_title', 'readmoo_processed_title',
        'bookscom_original_title', 'kobo_original_title', 'readmoo_original_title',
        'bookscom_author', 'kobo_author', 'readmoo_author',
        'bookscom_translator', 'kobo_translator', 'readmoo_translator',
        'bookscom_publisher', 'kobo_publisher', 'readmoo_publisher',
        'bookscom_publish_date', 'kobo_publish_date', 'readmoo_publish_date',
        'bookscom_original_price', 'kobo_original_price', 'readmoo_original_price',
        'bookscom_category', 'kobo_category', 'readmoo_category',
        'kobo_type_ebook', 'readmoo_type_ebook',
        'bookscom_url', 'kobo_url', 'readmoo_url'
    ]
    
    for col in fill_cols:
        # å„ªå…ˆä½¿ç”¨ book1ï¼ˆè¢«åˆä½µè€…ï¼‰çš„è³‡æ–™ï¼Œå¦‚æœæ²’æœ‰æ‰ç”¨ book2
        if pd.notna(book1.get(col)) and str(book1.get(col)).strip():
            merged[col] = book1[col]
        elif pd.notna(book2.get(col)) and str(book2.get(col)).strip():
            merged[col] = book2[col]
        else:
            merged[col] = ''
    
    # ä¿ç•™ã€Œè¢«åˆä½µè€…ã€(book1) çš„å…§å®¹
    keep_from_book1 = [
        'title', 'å‚™è¨»', 'processed_title', 'original_title',
        'author', 'translator', 'publisher'
    ]
    
    for col in keep_from_book1:
        merged[col] = book1.get(col, '')
    
    # Clean_publisherï¼šå‹¿å‹•ï¼ˆä¿ç•™ book1 çš„ï¼‰
    merged['Clean_publisher'] = book1.get('Clean_publisher', '')
    
    # æœªç´å…¥æ›¸ç›®FINDï¼šä¿ç•™ book1 çš„
    merged['æœªç´å…¥æ›¸ç›®FIND'] = book1.get('æœªç´å…¥æ›¸ç›®FIND', '')
    
    # min_publish_dateï¼šæœ€æ—©æ—¥æœŸ
    dates = []
    for col in ['min_publish_date', 'bookscom_publish_date', 'kobo_publish_date', 'readmoo_publish_date']:
        for book in [book1, book2]:
            if pd.notna(book.get(col)) and str(book.get(col)).strip():
                try:
                    date_str = str(book[col]).strip()
                    # å˜—è©¦è§£ææ—¥æœŸ
                    if '/' in date_str:
                        date_obj = datetime.strptime(date_str, '%Y/%m/%d')
                    elif '-' in date_str:
                        date_obj = datetime.strptime(date_str, '%Y-%m-%d')
                    else:
                        continue
                    dates.append(date_obj)
                except:
                    pass
    
    if dates:
        merged['min_publish_date'] = min(dates).strftime('%Y-%m-%d')
    else:
        merged['min_publish_date'] = book1.get('min_publish_date', '')
    
    # max_publish_dateï¼šæœ€æ™šæ—¥æœŸ
    if dates:
        merged['max_publish_date'] = max(dates).strftime('%Y-%m-%d')
    else:
        merged['max_publish_date'] = book1.get('max_publish_date', '')
    
    # priceï¼šæœ€å¤§å€¼
    prices = []
    for col in ['price', 'bookscom_original_price', 'kobo_original_price', 'readmoo_original_price']:
        for book in [book1, book2]:
            if pd.notna(book.get(col)):
                try:
                    price = float(book[col])
                    prices.append(price)
                except:
                    pass
    
    if prices:
        merged['price'] = max(prices)
    else:
        merged['price'] = book1.get('price', '')
    
    # è¨˜éŒ„åˆä½µçµæœ
    logging.info(f"  åˆä½µå¾Œ TAICCA_ID: {merged.get('NEW_TAICCA_ID', 'N/A')}")
    logging.info(f"  åˆä½µå¾Œ ISBN: {merged.get('isbn', 'N/A')}")
    
    return merged


def process_cluster_file(csv_file):
    """
    è™•ç†å–®å€‹åˆ†ç¾¤æª”æ¡ˆ
    
    å›å‚³: è™•ç†å¾Œçš„è³‡æ–™åˆ—è¡¨
    """
    filename = os.path.basename(csv_file)
    log_and_print(f"\nğŸ“‚ è™•ç†æª”æ¡ˆ: {filename}")
    logging.info(f"é–‹å§‹è™•ç†: {csv_file}")
    
    df = pd.read_csv(csv_file)
    log_and_print(f"  - è®€å– {len(df)} ç­†è³‡æ–™")
    
    if len(df) == 0:
        logging.warning(f"{filename} æ²’æœ‰è³‡æ–™")
        return []
    
    # è½‰æ›æˆå­—å…¸åˆ—è¡¨æ–¹ä¾¿è™•ç†
    books = df.to_dict('records')
    
    # æ¨™è¨˜å“ªäº›æ›¸å·²ç¶“è¢«åˆä½µ
    merged_indices = set()
    result_books = []
    
    # å…©å…©æ¯”è¼ƒ
    for i in tqdm(range(len(books)), desc="  æ¯”è¼ƒæ›¸ç±"):
        if i in merged_indices:
            continue  # å·²ç¶“è¢«åˆä½µéï¼Œè·³é
        
        current_book = books[i]
        found_match = False
        
        # èˆ‡å¾Œé¢çš„æ›¸æ¯”è¼ƒ
        for j in range(i + 1, len(books)):
            if j in merged_indices:
                continue
            
            compare_book = books[j]
            
            # ä½¿ç”¨ LLM åˆ¤æ–·æ˜¯å¦ç‚ºåŒä¸€æœ¬æ›¸
            title1 = str(current_book.get('title', '')).strip()
            title2 = str(compare_book.get('title', '')).strip()
            
            if not title1 or not title2:
                continue
            
            is_same = check_same_book_with_llm(title1, title2)
            
            if is_same:
                log_and_print(f"    âœ… æ‰¾åˆ°ç›¸åŒæ›¸ç±:")
                log_and_print(f"       [{i}] {title1}")
                log_and_print(f"       [{j}] {title2}")
                
                # åˆä½µå…©æœ¬æ›¸ï¼ˆcompare_book æ˜¯è¢«åˆä½µè€…ï¼Œä¿ç•™å…¶è³‡æ–™ï¼‰
                merged_book = merge_two_books(compare_book, current_book)
                result_books.append(merged_book)
                
                # æ¨™è¨˜å…©æœ¬éƒ½å·²è™•ç†
                merged_indices.add(i)
                merged_indices.add(j)
                found_match = True
                break
        
        # å¦‚æœæ²’æœ‰æ‰¾åˆ°é…å°ï¼Œä¿ç•™åŸæ›¸
        if not found_match:
            result_books.append(current_book)
    
    log_and_print(f"  âœ… è™•ç†å®Œæˆ: {len(result_books)} ç­†è³‡æ–™")
    logging.info(f"{filename} è™•ç†çµæœ: {len(df)} ç­† â†’ {len(result_books)} ç­†")
    
    return result_books


# ==================== ä¸»ç¨‹å¼ ====================

def main():
    global api_call_count, merge_count
    
    # åˆå§‹åŒ– logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S',
        handlers=[
            logging.FileHandler(LOG_FILE, encoding='utf-8'),
        ]
    )
    
    # è¨˜éŒ„é–‹å§‹æ™‚é–“
    start_time = time.time()
    start_datetime = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    
    # è¼¸å‡ºæ¨™é¡Œ
    title = "=" * 80 + "\nğŸ“š æ›¸ç±å»é‡åˆä½µç³»çµ± (ä½¿ç”¨ OpenAI LLM) - å³æ™‚å¯«å…¥æ¨¡å¼\n" + "=" * 80
    log_and_print(title)
    logging.info(f"é–‹å§‹æ™‚é–“: {start_datetime}")
    logging.info(f"Log æª”æ¡ˆ: {LOG_FILE}")
    logging.info(f"è¼¸å‡ºæª”æ¡ˆ: {OUTPUT_FILE}")
    
    # è®€å–æ‰€æœ‰åˆ†ç¾¤æª”æ¡ˆ
    cluster_files = glob.glob(os.path.join(CLUSTERED_DATA_DIR, "cluster_*.csv"))
    
    # æ’é™¤ full_data_with_clusters.csv
    cluster_files = [f for f in cluster_files if 'full_data' not in f]
    
    log_and_print(f"\næ‰¾åˆ° {len(cluster_files)} å€‹åˆ†ç¾¤æª”æ¡ˆ:")
    for f in cluster_files:
        log_and_print(f"  - {os.path.basename(f)}")
    
    # å–å¾—åŸå§‹æ¬„ä½é †åº
    if cluster_files:
        original_columns = pd.read_csv(cluster_files[0]).columns.tolist()
    else:
        log_and_print("\nâš ï¸ æ²’æœ‰æ‰¾åˆ°ä»»ä½•åˆ†ç¾¤æª”æ¡ˆ", 'warning')
        return
    
    # çµ±è¨ˆè³‡è¨Š
    total_original = 0
    total_output = 0
    
    # è™•ç†æ¯å€‹åˆ†ç¾¤æª”æ¡ˆä¸¦å³æ™‚å¯«å…¥
    for idx, cluster_file in enumerate(cluster_files):
        # è¨˜éŒ„åŸå§‹ç­†æ•¸
        original_count = len(pd.read_csv(cluster_file))
        total_original += original_count
        
        # åˆ¤æ–·æ˜¯å¦ç‚º noise æª”æ¡ˆ
        is_noise_file = 'noise' in os.path.basename(cluster_file).lower()
        
        if is_noise_file:
            # cluster_noise.csv ç›´æ¥è®€å–ä¸¦å¯«å…¥ï¼Œä¸é€²è¡Œæ¯”è¼ƒ
            filename = os.path.basename(cluster_file)
            log_and_print(f"\nğŸ“‚ è™•ç†æª”æ¡ˆ: {filename}")
            logging.info(f"é–‹å§‹è™•ç†å™ªéŸ³æª”æ¡ˆ: {cluster_file}")
            
            df = pd.read_csv(cluster_file)
            log_and_print(f"  - è®€å– {len(df)} ç­†è³‡æ–™")
            log_and_print(f"  âš¡ å™ªéŸ³æª”æ¡ˆï¼Œç›´æ¥å¯«å…¥ï¼ˆè·³éæ¯”è¼ƒï¼‰")
            
            if len(df) > 0:
                # ç¢ºä¿æ¬„ä½é †åºèˆ‡åŸå§‹æª”æ¡ˆç›¸åŒ
                df = df[[col for col in original_columns if col in df.columns]]
                
                # ç¬¬ä¸€å€‹æª”æ¡ˆï¼šå‰µå»ºæ–°æª”æ¡ˆä¸¦å¯«å…¥ header
                # å¾ŒçºŒæª”æ¡ˆï¼šè¿½åŠ æ¨¡å¼ï¼Œä¸å¯«å…¥ header
                if idx == 0:
                    df.to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig', mode='w')
                    log_and_print(f"  ğŸ’¾ å·²å¯«å…¥ {len(df)} ç­†è³‡æ–™åˆ° {OUTPUT_FILE} (æ–°å»ºæª”æ¡ˆ)")
                else:
                    df.to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig', mode='a', header=False)
                    log_and_print(f"  ğŸ’¾ å·²è¿½åŠ  {len(df)} ç­†è³‡æ–™åˆ° {OUTPUT_FILE}")
                
                logging.info(f"{filename}: ç›´æ¥å¯«å…¥ {len(df)} ç­†è³‡æ–™ï¼ˆå™ªéŸ³æª”æ¡ˆï¼‰")
                total_output += len(df)
            else:
                log_and_print(f"  âš ï¸ æ­¤ç¾¤çµ„æ²’æœ‰è³‡æ–™", 'warning')
        else:
            # ä¸€èˆ¬åˆ†ç¾¤æª”æ¡ˆï¼šé€²è¡Œæ¯”è¼ƒè™•ç†
            results = process_cluster_file(cluster_file)
            
            if results:
                # è½‰æ›æˆ DataFrame
                result_df = pd.DataFrame(results)
                
                # ç¢ºä¿æ¬„ä½é †åºèˆ‡åŸå§‹æª”æ¡ˆç›¸åŒ
                result_df = result_df[[col for col in original_columns if col in result_df.columns]]
                
                # ç¬¬ä¸€å€‹æª”æ¡ˆï¼šå‰µå»ºæ–°æª”æ¡ˆä¸¦å¯«å…¥ header
                # å¾ŒçºŒæª”æ¡ˆï¼šè¿½åŠ æ¨¡å¼ï¼Œä¸å¯«å…¥ header
                if idx == 0:
                    result_df.to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig', mode='w')
                    log_and_print(f"  ğŸ’¾ å·²å¯«å…¥ {len(result_df)} ç­†è³‡æ–™åˆ° {OUTPUT_FILE} (æ–°å»ºæª”æ¡ˆ)")
                else:
                    result_df.to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig', mode='a', header=False)
                    log_and_print(f"  ğŸ’¾ å·²è¿½åŠ  {len(result_df)} ç­†è³‡æ–™åˆ° {OUTPUT_FILE}")
                
                logging.info(f"å·²å¯«å…¥ {len(result_df)} ç­†è³‡æ–™åˆ° {OUTPUT_FILE}")
                total_output += len(result_df)
            else:
                log_and_print(f"  âš ï¸ æ­¤ç¾¤çµ„æ²’æœ‰è³‡æ–™è¼¸å‡º", 'warning')
    
    # è¨ˆç®—åŸ·è¡Œæ™‚é–“
    end_time = time.time()
    end_datetime = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    elapsed_time = end_time - start_time
    
    # æœ€çµ‚çµ±è¨ˆ
    log_and_print(f"\n" + "=" * 80)
    log_and_print(f"ğŸ“Š æœ€çµ‚çµ±è¨ˆ")
    log_and_print(f"=" * 80)
    log_and_print(f"  - åŸå§‹ç¸½ç­†æ•¸: {total_original}")
    log_and_print(f"  - è¼¸å‡ºè³‡æ–™ç­†æ•¸: {total_output}")
    log_and_print(f"  - åˆä½µæ¸›å°‘: {total_original - total_output} ç­†")
    log_and_print(f"  - LLM API å‘¼å«æ¬¡æ•¸: {api_call_count}")
    log_and_print(f"  - å¯¦éš›åˆä½µæ¬¡æ•¸: {merge_count}")
    log_and_print(f"  - è™•ç†æ™‚é–“: {elapsed_time:.2f} ç§’ ({elapsed_time/60:.2f} åˆ†é˜)")
    log_and_print(f"\nâœ… çµæœå·²å„²å­˜è‡³: {OUTPUT_FILE}")
    log_and_print(f"ğŸ“„ Log æª”æ¡ˆå·²å„²å­˜è‡³: {LOG_FILE}")
    
    # è¨˜éŒ„çµæŸæ™‚é–“åˆ° log
    logging.info("=" * 80)
    logging.info(f"çµæŸæ™‚é–“: {end_datetime}")
    logging.info(f"ç¸½åŸ·è¡Œæ™‚é–“: {elapsed_time:.2f} ç§’")
    logging.info("=" * 80)
    
    log_and_print(f"\n" + "=" * 80)
    log_and_print("ğŸ‰ è™•ç†å®Œæˆï¼")
    log_and_print("=" * 80)


if __name__ == "__main__":
    main()
