{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "552b9bdd",
   "metadata": {
    "id": "552b9bdd"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import glob\n",
    "import datetime\n",
    "import ast\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "try:\n",
    "    import cn2an\n",
    "except ImportError:\n",
    "    cn2an = None\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer, util\n",
    "    BERT_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"è­¦å‘Š: sentence-transformers æœªå®‰è£ï¼Œå°‡ç„¡æ³•ä½¿ç”¨ BERT\")\n",
    "    BERT_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oAKuMgvhDFRR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "oAKuMgvhDFRR",
    "outputId": "218c9590-26b5-48a7-b946-83f6487bbabe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3f6417",
   "metadata": {},
   "outputs": [],
   "source": [
    "HAS_ID = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "62c36e4b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "62c36e4b",
    "outputId": "dd03cb0f-e260-469a-e0e2-949bed6202db"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('input_data/ebook_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc8acb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========== æ·»åŠ å…¶ä»–å¤§æ¬„ä½çš„ consolidate å‡½æ•¸ (é›»å­æ›¸ç‰ˆ) ==========\n",
    "\n",
    "def consolidate_isbn(row):\n",
    "    \"\"\"å¾æ›¸å•†æ¬„ä½ä¸­æå– isbn\"\"\"\n",
    "    # é †åºå¯èª¿æ•´ï¼Œé€šå¸¸ Books.com è³‡æ–™è¼ƒé½Šå…¨\n",
    "    isbn_cols = ['bookscom_isbn', 'readmoo_isbn', 'kobo_isbn']\n",
    "    for col in isbn_cols:\n",
    "        if col in row.index and pd.notna(row[col]) and str(row[col]).strip() != '':\n",
    "            return row[col]\n",
    "    try:\n",
    "        if pd.notna(row.get('isbn')) and str(row['isbn']).strip() != '':\n",
    "             return row['isbn']\n",
    "    except:\n",
    "        pass\n",
    "    return pd.NA\n",
    "\n",
    "def consolidate_original_title(row):\n",
    "    \"\"\"å¾æ›¸å•†æ¬„ä½ä¸­æå– original_title\"\"\"\n",
    "    title_cols = ['bookscom_original_title', 'readmoo_original_title', 'kobo_original_title']\n",
    "    for col in title_cols:\n",
    "        if col in row.index and pd.notna(row[col]) and str(row[col]).strip() != '':\n",
    "            return row[col]\n",
    "    return pd.NA\n",
    "\n",
    "def consolidate_author(row):\n",
    "    \"\"\"å¾æ›¸å•†æ¬„ä½ä¸­æå– author\"\"\"\n",
    "    author_cols = ['bookscom_author', 'readmoo_author', 'kobo_author']\n",
    "    for col in author_cols:\n",
    "        if col in row.index and pd.notna(row[col]) and str(row[col]).strip() != '':\n",
    "            return row[col]\n",
    "    return pd.NA\n",
    "\n",
    "def consolidate_translator(row):\n",
    "    \"\"\"å¾æ›¸å•†æ¬„ä½ä¸­æå– translator\"\"\"\n",
    "    translator_cols = ['bookscom_translator', 'readmoo_translator', 'kobo_translator']\n",
    "    for col in translator_cols:\n",
    "        if col in row.index and pd.notna(row[col]) and str(row[col]).strip() != '':\n",
    "            return row[col]\n",
    "    return pd.NA\n",
    "\n",
    "def consolidate_publisher(row):\n",
    "    \"\"\"å¾æ›¸å•†æ¬„ä½ä¸­æå– publisher\"\"\"\n",
    "    publisher_cols = ['bookscom_publisher', 'readmoo_publisher', 'kobo_publisher']\n",
    "    for col in publisher_cols:\n",
    "        if col in row.index and pd.notna(row[col]) and str(row[col]).strip() != '':\n",
    "            return row[col]\n",
    "    return pd.NA\n",
    "\n",
    "# æ‡‰ç”¨é€™äº›å‡½æ•¸åˆ° DataFrame\n",
    "print(\"é–‹å§‹å»ºç«‹å¤§æ¬„ä½...\")\n",
    "\n",
    "# ç¢ºä¿æ¬„ä½å­˜åœ¨ï¼Œé¿å… KeyError\n",
    "for col in ['bookscom_isbn', 'readmoo_isbn', 'kobo_isbn',\n",
    "            'bookscom_original_title', 'readmoo_original_title', 'kobo_original_title',\n",
    "            'bookscom_author', 'readmoo_author', 'kobo_author',\n",
    "            'bookscom_translator', 'readmoo_translator', 'kobo_translator',\n",
    "            'bookscom_publisher', 'readmoo_publisher', 'kobo_publisher']:\n",
    "    if col not in df.columns:\n",
    "        df[col] = pd.NA\n",
    "\n",
    "df['isbn'] = df.apply(consolidate_isbn, axis=1)\n",
    "df['original_title'] = df.apply(consolidate_original_title, axis=1)\n",
    "df['author'] = df.apply(consolidate_author, axis=1)\n",
    "df['translator'] = df.apply(consolidate_translator, axis=1)\n",
    "df['publisher'] = df.apply(consolidate_publisher, axis=1)\n",
    "\n",
    "print(\"å¤§æ¬„ä½å»ºç«‹å®Œæˆï¼çµ±è¨ˆè³‡è¨Šï¼š\")\n",
    "print(f\"  isbn æœ‰å€¼çš„ç­†æ•¸: {df['isbn'].notna().sum()} / {len(df)} ({df['isbn'].notna().sum()/len(df)*100:.1f}%)\")\n",
    "print(f\"  original_title æœ‰å€¼çš„ç­†æ•¸: {df['original_title'].notna().sum()} / {len(df)} ({df['original_title'].notna().sum()/len(df)*100:.1f}%)\")\n",
    "print(f\"  author æœ‰å€¼çš„ç­†æ•¸: {df['author'].notna().sum()} / {len(df)} ({df['author'].notna().sum()/len(df)*100:.1f}%)\")\n",
    "print(f\"  translator æœ‰å€¼çš„ç­†æ•¸: {df['translator'].notna().sum()} / {len(df)} ({df['translator'].notna().sum()/len(df)*100:.1f}%)\")\n",
    "print(f\"  publisher æœ‰å€¼çš„ç­†æ•¸: {df['publisher'].notna().sum()} / {len(df)} ({df['publisher'].notna().sum()/len(df)*100:.1f}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ba4c6fc7",
   "metadata": {
    "id": "ba4c6fc7"
   },
   "outputs": [],
   "source": [
    "# OpenAI è¨­å®š\n",
    "OPENAI_API_KEY = \"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\"\n",
    "\n",
    "openai_client = None  # æ·»åŠ é€™ä¸€è¡Œ\n",
    "\n",
    "# OpenAI Embedding è¨­å®šï¼ˆåƒ…ç”¨æ–¼ç¬¬ä¸€éšæ®µï¼‰\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "EPS = 0.15           # DBSCAN é„°åŸŸåŠå¾‘\n",
    "MIN_SAMPLES = 2     # DBSCAN æœ€å°æ¨£æœ¬æ•¸\n",
    "\n",
    "# BERT è¨­å®š\n",
    "BERT_MODEL = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "SIMILARITY_THRESHOLD = 0.99  # BERT ç›¸ä¼¼åº¦é–¾å€¼\n",
    "\n",
    "FINAL_OUTPUT_FILE = \"/Users/alioth1225/Documents/College/merge/clustering/output_data/ebook_output.csv\"\n",
    "\n",
    "merge_count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698c83ac",
   "metadata": {
    "id": "698c83ac"
   },
   "source": [
    "### **Cleaning Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "303acb2b",
   "metadata": {
    "id": "303acb2b"
   },
   "outputs": [],
   "source": [
    "def clean_ebook_text(title):\n",
    "    \"\"\"ç§»é™¤æ¨™é¡Œä¸­çš„ã€Œé›»å­æ›¸ã€ç›¸é—œå­—æ¨£\"\"\"\n",
    "    if pd.isna(title) or not str(title).strip():\n",
    "        return title\n",
    "\n",
    "    title = str(title)\n",
    "    patterns_to_remove = [\n",
    "        r'\\(é›»å­æ›¸\\)',\n",
    "        r'ï¼ˆé›»å­æ›¸ï¼‰',\n",
    "        r'\\[é›»å­æ›¸\\]',\n",
    "        r'ã€é›»å­æ›¸ã€‘',\n",
    "        r'é›»å­æ›¸',\n",
    "        r'\\(ebook\\)',\n",
    "        r'ï¼ˆebookï¼‰',\n",
    "        r'ebook',\n",
    "        r'e-book',\n",
    "        r'é™',\n",
    "        r'é™åˆ¶ç´š',\n",
    "        r'\\(é™\\)',\n",
    "        r'ï¼ˆé™ï¼‰',\n",
    "    ]\n",
    "\n",
    "    for pattern in patterns_to_remove:\n",
    "        title = re.sub(pattern, '', title, flags=re.IGNORECASE)\n",
    "\n",
    "    # æ¸…ç†å¤šé¤˜ç©ºæ ¼\n",
    "    title = ' '.join(title.split())\n",
    "    return title.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "I81UTDK6Ef15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I81UTDK6Ef15",
    "outputId": "ab361361-b492-459c-9808-fce8e00d02e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title consolidation complete. First 5 rows of updated 'title' column:\n",
      "0                  è“®è¯é Œ(ä¸€) (é›»å­æ›¸)\n",
      "1                  è“®è¯é Œ(äºŒ) (é›»å­æ›¸)\n",
      "2    è“®è¯é Œï¼ˆä¸€ï¼‰        ï¼šå¥³Alphaçš„é³³ç´‹ç¥•æ³•\n",
      "3    è“®è¯é Œï¼ˆäºŒï¼‰        ï¼šå¥³Alphaçš„é³³ç´‹ç¥•æ³•\n",
      "4                       ç†Ÿå¥³è¨˜(49)\n",
      "Name: title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def consolidate_title(row):\n",
    "    title_cols = ['bookscom_title', 'kobo_title', 'readmoo_title']\n",
    "    for col in title_cols:\n",
    "        # Check if the column exists in the row and if the value is not NaN and not an empty string\n",
    "        if col in row.index and pd.notna(row[col]) and str(row[col]).strip() != '':\n",
    "            return row[col]\n",
    "    return pd.NA # Return pandas Not-a-Number if no title is found\n",
    "\n",
    "# Apply the function to create or update the 'title' column\n",
    "df['title'] = df.apply(consolidate_title, axis=1)\n",
    "\n",
    "print(\"Title consolidation complete. First 5 rows of updated 'title' column:\")\n",
    "print(df['title'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "XGixKEcgFkWa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XGixKEcgFkWa",
    "outputId": "70f3c7d6-d66e-496c-fdff-1fcac034d61b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title consolidation complete. First 5 rows of updated 'title' column:\n",
      "0            è“®è¯é Œä¸€é›»å­æ›¸\n",
      "1            è“®è¯é ŒäºŒé›»å­æ›¸\n",
      "2    è“®è¯é Œä¸€å¥³Alphaçš„é³³ç´‹ç¥•æ³•\n",
      "3    è“®è¯é ŒäºŒå¥³Alphaçš„é³³ç´‹ç¥•æ³•\n",
      "4              ç†Ÿå¥³è¨˜49\n",
      "Name: processed_title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def consolidate_processed_title(row):\n",
    "    title_cols = ['bookscom_processed_title', 'kobo_processed_title', 'readmoo_processed_title']\n",
    "    for col in title_cols:\n",
    "        # Check if the column exists in the row and if the value is not NaN and not an empty string\n",
    "        if col in row.index and pd.notna(row[col]) and str(row[col]).strip() != '':\n",
    "            return row[col]\n",
    "    return pd.NA # Return pandas Not-a-Number if no title is found\n",
    "\n",
    "# Apply the function to create or update the 'title' column\n",
    "df['processed_title'] = df.apply(consolidate_processed_title, axis=1)\n",
    "\n",
    "print(\"Title consolidation complete. First 5 rows of updated 'title' column:\")\n",
    "print(df['processed_title'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "32534fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            è“®è¯é Œä¸€é›»å­æ›¸\n",
       "1            è“®è¯é ŒäºŒé›»å­æ›¸\n",
       "2    è“®è¯é Œä¸€å¥³Alphaçš„é³³ç´‹ç¥•æ³•\n",
       "3    è“®è¯é ŒäºŒå¥³Alphaçš„é³³ç´‹ç¥•æ³•\n",
       "4              ç†Ÿå¥³è¨˜49\n",
       "5              ç†Ÿå¥³è¨˜50\n",
       "6              ç†Ÿå¥³è¨˜49\n",
       "7              ç†Ÿå¥³è¨˜50\n",
       "8             ç†Ÿå¥³è¨˜49é™\n",
       "9             ç†Ÿå¥³è¨˜50é™\n",
       "Name: processed_title, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['processed_title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "45bebabe",
   "metadata": {
    "id": "45bebabe"
   },
   "outputs": [],
   "source": [
    "df['processed_title_new'] = df['processed_title'].apply(clean_ebook_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0c63fe77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TAICCA_ID</th>\n",
       "      <th>1106äº¦å¼ID</th>\n",
       "      <th>bookscom_production_id</th>\n",
       "      <th>kobo_production_id</th>\n",
       "      <th>readmoo_production_id</th>\n",
       "      <th>isbn</th>\n",
       "      <th>bookscom_isbn</th>\n",
       "      <th>kobo_isbn</th>\n",
       "      <th>readmoo_isbn</th>\n",
       "      <th>eisbn</th>\n",
       "      <th>...</th>\n",
       "      <th>readmoo_category</th>\n",
       "      <th>kobo_type_ebook</th>\n",
       "      <th>readmoo_type_ebook</th>\n",
       "      <th>bookscom_url</th>\n",
       "      <th>kobo_url</th>\n",
       "      <th>readmoo_url</th>\n",
       "      <th>FALSE</th>\n",
       "      <th>production_id</th>\n",
       "      <th>1029TAICCA_ID</th>\n",
       "      <th>processed_title_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E253-06244</td>\n",
       "      <td>E253-05254</td>\n",
       "      <td>E050293567</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.books.com.tw/products/E050293567</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>è“®è¯é Œä¸€</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E253-06246</td>\n",
       "      <td>E253-05255</td>\n",
       "      <td>E050293568</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.books.com.tw/products/E050293568</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>è“®è¯é ŒäºŒ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E253-06245</td>\n",
       "      <td>E253-05604</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.104110e+14</td>\n",
       "      <td>9.786270e+12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.786270e+12</td>\n",
       "      <td>9.786270e+12</td>\n",
       "      <td>...</td>\n",
       "      <td>è¼•å°èªª-å¥‡å¹»å†’éšª</td>\n",
       "      <td>NaN</td>\n",
       "      <td>æµå‹•ç‰ˆé¢ EPUB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://readmoo.com/book/210410954000101</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>è“®è¯é Œä¸€å¥³Alphaçš„é³³ç´‹ç¥•æ³•</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E253-06247</td>\n",
       "      <td>E253-05605</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.104110e+14</td>\n",
       "      <td>9.786270e+12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.786270e+12</td>\n",
       "      <td>9.786270e+12</td>\n",
       "      <td>...</td>\n",
       "      <td>è¼•å°èªª-å¥‡å¹»å†’éšª</td>\n",
       "      <td>NaN</td>\n",
       "      <td>æµå‹•ç‰ˆé¢ EPUB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://readmoo.com/book/210410962000101</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>è“®è¯é ŒäºŒå¥³Alphaçš„é³³ç´‹ç¥•æ³•</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E253-12552</td>\n",
       "      <td>NEW1250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MIdxPwuezjyYhCoHKoekzQ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.786263e+12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EPUB 3 (Adobe DRM)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.kobo.com/tw/zh/ebook/MIdxPwuezjyYh...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MIdxPwuezjyYhCoHKoekzQ</td>\n",
       "      <td>E253-10552</td>\n",
       "      <td>ç†Ÿå¥³è¨˜49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    TAICCA_ID    1106äº¦å¼ID bookscom_production_id      kobo_production_id  \\\n",
       "0  E253-06244  E253-05254             E050293567                     NaN   \n",
       "1  E253-06246  E253-05255             E050293568                     NaN   \n",
       "2  E253-06245  E253-05604                    NaN                     NaN   \n",
       "3  E253-06247  E253-05605                    NaN                     NaN   \n",
       "4  E253-12552     NEW1250                    NaN  MIdxPwuezjyYhCoHKoekzQ   \n",
       "\n",
       "   readmoo_production_id          isbn  bookscom_isbn     kobo_isbn  \\\n",
       "0                    NaN           NaN            NaN           NaN   \n",
       "1                    NaN           NaN            NaN           NaN   \n",
       "2           2.104110e+14  9.786270e+12            NaN           NaN   \n",
       "3           2.104110e+14  9.786270e+12            NaN           NaN   \n",
       "4                    NaN           NaN            NaN  9.786263e+12   \n",
       "\n",
       "   readmoo_isbn         eisbn  ...  readmoo_category     kobo_type_ebook  \\\n",
       "0           NaN           NaN  ...               NaN                 NaN   \n",
       "1           NaN           NaN  ...               NaN                 NaN   \n",
       "2  9.786270e+12  9.786270e+12  ...          è¼•å°èªª-å¥‡å¹»å†’éšª                 NaN   \n",
       "3  9.786270e+12  9.786270e+12  ...          è¼•å°èªª-å¥‡å¹»å†’éšª                 NaN   \n",
       "4           NaN           NaN  ...               NaN  EPUB 3 (Adobe DRM)   \n",
       "\n",
       "   readmoo_type_ebook                                  bookscom_url  \\\n",
       "0                 NaN  https://www.books.com.tw/products/E050293567   \n",
       "1                 NaN  https://www.books.com.tw/products/E050293568   \n",
       "2           æµå‹•ç‰ˆé¢ EPUB                                           NaN   \n",
       "3           æµå‹•ç‰ˆé¢ EPUB                                           NaN   \n",
       "4                 NaN                                           NaN   \n",
       "\n",
       "                                            kobo_url  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4  https://www.kobo.com/tw/zh/ebook/MIdxPwuezjyYh...   \n",
       "\n",
       "                                readmoo_url  FALSE           production_id  \\\n",
       "0                                       NaN  False                     NaN   \n",
       "1                                       NaN  False                     NaN   \n",
       "2  https://readmoo.com/book/210410954000101  False                     NaN   \n",
       "3  https://readmoo.com/book/210410962000101  False                     NaN   \n",
       "4                                       NaN    NaN  MIdxPwuezjyYhCoHKoekzQ   \n",
       "\n",
       "  1029TAICCA_ID processed_title_new  \n",
       "0           NaN                è“®è¯é Œä¸€  \n",
       "1           NaN                è“®è¯é ŒäºŒ  \n",
       "2           NaN     è“®è¯é Œä¸€å¥³Alphaçš„é³³ç´‹ç¥•æ³•  \n",
       "3           NaN     è“®è¯é ŒäºŒå¥³Alphaçš„é³³ç´‹ç¥•æ³•  \n",
       "4    E253-10552               ç†Ÿå¥³è¨˜49  \n",
       "\n",
       "[5 rows x 60 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7ea3e793",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ea3e793",
    "outputId": "80b7ef9e-527e-4bdc-b807-2e9cdc5c1146"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è“®è¯é Œä¸€\n",
      "è“®è¯é ŒäºŒ\n",
      "è“®è¯é Œä¸€å¥³Alphaçš„é³³ç´‹ç¥•æ³•\n",
      "è“®è¯é ŒäºŒå¥³Alphaçš„é³³ç´‹ç¥•æ³•\n",
      "ç†Ÿå¥³è¨˜49\n",
      "ç†Ÿå¥³è¨˜50\n",
      "ç†Ÿå¥³è¨˜49\n",
      "ç†Ÿå¥³è¨˜50\n",
      "ç†Ÿå¥³è¨˜49\n",
      "ç†Ÿå¥³è¨˜50\n"
     ]
    }
   ],
   "source": [
    "for i in df['processed_title_new']:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587a9d04",
   "metadata": {
    "id": "587a9d04"
   },
   "source": [
    "### **Calculating Embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b23b70ba",
   "metadata": {
    "id": "b23b70ba"
   },
   "outputs": [],
   "source": [
    "# åˆå§‹åŒ– OpenAI å®¢æˆ¶ç«¯è®Šæ•¸ï¼ˆå¦‚æœå°šæœªå®šç¾©ï¼‰\n",
    "if 'openai_client' not in globals():\n",
    "    openai_client = None\n",
    "\n",
    "def get_embedding(text, model=EMBEDDING_MODEL):\n",
    "    \"\"\"å–å¾—æ–‡å­—çš„ embedding å‘é‡ï¼ˆä½¿ç”¨ OpenAI APIï¼‰\"\"\"\n",
    "    global openai_client\n",
    "\n",
    "    if pd.isna(text) or not str(text).strip():\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # åˆå§‹åŒ– OpenAI å®¢æˆ¶ç«¯ï¼ˆåªåˆå§‹åŒ–ä¸€æ¬¡ï¼‰\n",
    "        if openai_client is None:\n",
    "            from openai import OpenAI\n",
    "            openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "        text = str(text).replace(\"\\n\", \" \")\n",
    "        response = openai_client.embeddings.create(\n",
    "            input=text,\n",
    "            model=model\n",
    "        )\n",
    "        return response.data[0].embedding\n",
    "    except ImportError:\n",
    "        print(\"âŒ éŒ¯èª¤: æ‰¾ä¸åˆ° openai å¥—ä»¶ã€‚è«‹åŸ·è¡Œ `pip install openai`\")\n",
    "        raise # ä¸­æ–·åŸ·è¡Œä»¥ä¾¿æ‚¨çœ‹åˆ°éŒ¯èª¤\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Embedding éŒ¯èª¤: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1e0658e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0               è“®è¯é Œä¸€\n",
       "1               è“®è¯é ŒäºŒ\n",
       "2    è“®è¯é Œä¸€å¥³Alphaçš„é³³ç´‹ç¥•æ³•\n",
       "3    è“®è¯é ŒäºŒå¥³Alphaçš„é³³ç´‹ç¥•æ³•\n",
       "4              ç†Ÿå¥³è¨˜49\n",
       "5              ç†Ÿå¥³è¨˜50\n",
       "6              ç†Ÿå¥³è¨˜49\n",
       "7              ç†Ÿå¥³è¨˜50\n",
       "8              ç†Ÿå¥³è¨˜49\n",
       "9              ç†Ÿå¥³è¨˜50\n",
       "Name: processed_title_new, dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['processed_title_new']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d61a26a1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d61a26a1",
    "outputId": "2aea3877-189b-46f3-fff3-9bfc1a023732"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ç”Ÿæˆ Embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:06<00:00,  1.66it/s]\n"
     ]
    }
   ],
   "source": [
    "from typing import Any\n",
    "\n",
    "prefix_embeddings = []\n",
    "# ç§»é™¤ [Any]\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"ç”Ÿæˆ Embeddings\"):\n",
    "    # ä½¿ç”¨æ¸…ç†å¾Œçš„æ¨™é¡Œè¨ˆç®— embedding\n",
    "    prefix_embedding = get_embedding(row['processed_title_new'])\n",
    "    prefix_embeddings.append(prefix_embedding)\n",
    "\n",
    "df['prefix_embedding'] = prefix_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e4b_y7cA6kU2",
   "metadata": {
    "id": "e4b_y7cA6kU2"
   },
   "outputs": [],
   "source": [
    "df.to_csv('embedding_files/paper_embedding.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "Eg-1FhSjXvDZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Eg-1FhSjXvDZ",
    "outputId": "57a5e64d-9774-4666-df48-5577a8178135"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('embedding_files/paper_embedding.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d7c1f4de",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d7c1f4de",
    "outputId": "b869ca5f-77bc-4d43-ed9b-bab3f3531579"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> è§£æ prefix_embedding å‘é‡...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "è§£æä¸­: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 3200.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Embedding çŸ©é™£å½¢ç‹€: (10, 1536)\n",
      "  âœ… DBSCAN åˆ†ç¾¤å®Œæˆ\n",
      "\n",
      "âœ… åˆ†ç¾¤å®Œæˆï¼\n",
      "  - è­˜åˆ¥å‡ºçš„ç¾¤æ•¸: 3\n",
      "  - å™ªéŸ³é»: 0 ç­†\n",
      "åˆ†ç¾¤çµæœ: 3 å€‹ç¾¤, 0 å€‹å™ªéŸ³é»\n",
      "\n",
      "ğŸ“Š å„ç¾¤çš„è³‡æ–™ç­†æ•¸ï¼š\n",
      "  ç¾¤ 0:     2 ç­† ( 20.0%)\n",
      "  ç¾¤ 1:     2 ç­† ( 20.0%)\n",
      "  ç¾¤ 2:     6 ç­† ( 60.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_valid = df[df['prefix_embedding'].notna()].copy()\n",
    "\n",
    "# è§£æ embeddingï¼ˆå¾å­—ä¸²è½‰ç‚ºåˆ—è¡¨ï¼‰\n",
    "print(\">> è§£æ prefix_embedding å‘é‡...\")\n",
    "embeddings_list = []\n",
    "for idx, row in tqdm(df_valid.iterrows(), total=len(df_valid), desc=\"è§£æä¸­\"):\n",
    "    try:\n",
    "        emb = row['prefix_embedding']\n",
    "        # å¦‚æœæ˜¯å­—ä¸²ï¼Œéœ€è¦è§£æç‚ºåˆ—è¡¨\n",
    "        if isinstance(emb, str):\n",
    "            emb = json.loads(emb)\n",
    "        embeddings_list.append(emb)\n",
    "    except Exception as e:\n",
    "        print(f\"è­¦å‘Š: ç¬¬ {idx} ç­†è³‡æ–™è§£æå¤±æ•—: {e}\")\n",
    "        embeddings_list.append(None)\n",
    "\n",
    "df_valid['embedding_parsed'] = embeddings_list\n",
    "df_valid = df_valid[df_valid['embedding_parsed'].notna()]\n",
    "\n",
    "embeddings_array = np.array(df_valid['embedding_parsed'].tolist())\n",
    "print(f\">> Embedding çŸ©é™£å½¢ç‹€: {embeddings_array.shape}\")\n",
    "\n",
    "# æ­¥é©Ÿ 5: åŸ·è¡Œ DBSCAN åˆ†ç¾¤ï¼ˆä½¿ç”¨ recluster_only.py çš„é‚è¼¯ï¼‰\n",
    "\n",
    "EPS = 0.15           # DBSCAN é„°åŸŸåŠå¾‘\n",
    "MIN_SAMPLES = 2     # DBSCAN æœ€å°æ¨£æœ¬æ•¸\n",
    "\n",
    "try:\n",
    "    dbscan = DBSCAN(eps=EPS, min_samples=MIN_SAMPLES, metric='cosine', n_jobs=-1)\n",
    "    cluster_labels = dbscan.fit_predict(embeddings_array)\n",
    "    df_valid['cluster'] = cluster_labels\n",
    "    print(f\"  âœ… DBSCAN åˆ†ç¾¤å®Œæˆ\")\n",
    "except Exception as e:\n",
    "    print(f\"  âŒ DBSCAN åˆ†ç¾¤å¤±æ•—: {e}\", 'error')\n",
    "    raise\n",
    "\n",
    "# çµ±è¨ˆåˆ†ç¾¤çµæœ\n",
    "n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "n_noise = list(cluster_labels).count(-1)\n",
    "\n",
    "print(f\"\\nâœ… åˆ†ç¾¤å®Œæˆï¼\")\n",
    "print(f\"  - è­˜åˆ¥å‡ºçš„ç¾¤æ•¸: {n_clusters}\")\n",
    "print(f\"  - å™ªéŸ³é»: {n_noise} ç­†\")\n",
    "print(f\"åˆ†ç¾¤çµæœ: {n_clusters} å€‹ç¾¤, {n_noise} å€‹å™ªéŸ³é»\")\n",
    "\n",
    "# é¡¯ç¤ºå„ç¾¤çµ±è¨ˆ\n",
    "if n_clusters > 0:\n",
    "    print(f\"\\nğŸ“Š å„ç¾¤çš„è³‡æ–™ç­†æ•¸ï¼š\")\n",
    "    for cluster_id in sorted(df_valid['cluster'].unique()):\n",
    "        if cluster_id == -1:\n",
    "            continue\n",
    "        count = (df_valid['cluster'] == cluster_id).sum()\n",
    "        percentage = (count / len(df_valid)) * 100\n",
    "        print(f\"  ç¾¤ {cluster_id}: {count:>5} ç­† ({percentage:>5.1f}%)\")\n",
    "\n",
    "    if n_noise > 0:\n",
    "        percentage = (n_noise / len(df_valid)) * 100\n",
    "        print(f\"  å™ªéŸ³: {n_noise:>5} ç­† ({percentage:>5.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d69a730b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "CLUSTERED_DATA_DIR = '/Users/alioth1225/Documents/College/merge/clustering/clustered_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "86d5710a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "86d5710a",
    "outputId": "88c81f41-64a4-4693-dfff-886128325ee9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ… ç¾¤ 0: 2 ç­† â†’ /Users/alioth1225/Documents/College/merge/clustering/ebook_clustered_data/cluster_0.csv\n",
      "å„²å­˜åˆ†ç¾¤æª”æ¡ˆ: /Users/alioth1225/Documents/College/merge/clustering/ebook_clustered_data/cluster_0.csv, ç­†æ•¸: 2\n",
      "  âœ… ç¾¤ 1: 2 ç­† â†’ /Users/alioth1225/Documents/College/merge/clustering/ebook_clustered_data/cluster_1.csv\n",
      "å„²å­˜åˆ†ç¾¤æª”æ¡ˆ: /Users/alioth1225/Documents/College/merge/clustering/ebook_clustered_data/cluster_1.csv, ç­†æ•¸: 2\n",
      "  âœ… ç¾¤ 2: 6 ç­† â†’ /Users/alioth1225/Documents/College/merge/clustering/ebook_clustered_data/cluster_2.csv\n",
      "å„²å­˜åˆ†ç¾¤æª”æ¡ˆ: /Users/alioth1225/Documents/College/merge/clustering/ebook_clustered_data/cluster_2.csv, ç­†æ•¸: 6\n",
      "\n",
      "  ğŸ“Š å®Œæ•´è³‡æ–™ï¼ˆå«åˆ†ç¾¤æ¨™ç±¤ï¼‰: /Users/alioth1225/Documents/College/merge/clustering/ebook_clustered_data/full_data_with_clusters.csv\n",
      "\n",
      "âœ… ç¬¬ä¸€éšæ®µå®Œæˆï¼åˆ†ç¾¤æª”æ¡ˆå·²å„²å­˜ã€‚\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "CLUSTERED_DATA_DIR = '/Users/alioth1225/Documents/College/merge/clustering/ebook_clustered_data'\n",
    "\n",
    "os.makedirs(CLUSTERED_DATA_DIR, exist_ok=True)\n",
    "\n",
    "df_to_save = df_valid.drop(columns=['prefix_embedding'])\n",
    "saved_files = []\n",
    "\n",
    "for cluster_id in sorted(df_to_save['cluster'].unique()):\n",
    "    cluster_data = df_to_save[df_to_save['cluster'] == cluster_id]\n",
    "    cluster_data_original = cluster_data.drop(columns=['cluster'])\n",
    "\n",
    "    if cluster_id == -1:\n",
    "        output_file = os.path.join(CLUSTERED_DATA_DIR, \"cluster_noise.csv\")\n",
    "        label = \"å™ªéŸ³é»\"\n",
    "    else:\n",
    "        output_file = os.path.join(CLUSTERED_DATA_DIR, f\"cluster_{cluster_id}.csv\")\n",
    "        label = f\"ç¾¤ {cluster_id}\"\n",
    "\n",
    "    cluster_data_original.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "    saved_files.append(output_file)\n",
    "    print(f\"  âœ… {label}: {len(cluster_data)} ç­† â†’ {output_file}\")\n",
    "    print(f\"å„²å­˜åˆ†ç¾¤æª”æ¡ˆ: {output_file}, ç­†æ•¸: {len(cluster_data)}\")\n",
    "\n",
    "# å„²å­˜å®Œæ•´è³‡æ–™\n",
    "full_output_file = os.path.join(CLUSTERED_DATA_DIR, \"full_data_with_clusters.csv\")\n",
    "df_to_save.to_csv(full_output_file, index=False, encoding='utf-8-sig')\n",
    "print(f\"\\n  ğŸ“Š å®Œæ•´è³‡æ–™ï¼ˆå«åˆ†ç¾¤æ¨™ç±¤ï¼‰: {full_output_file}\")\n",
    "\n",
    "print(\"\\nâœ… ç¬¬ä¸€éšæ®µå®Œæˆï¼åˆ†ç¾¤æª”æ¡ˆå·²å„²å­˜ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156740c2",
   "metadata": {},
   "source": [
    "### **Bert**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "84cedc31",
   "metadata": {
    "id": "84cedc31"
   },
   "outputs": [],
   "source": [
    "class UnionFind:\n",
    "    \"\"\"ä¸¦æŸ¥é›†ï¼Œç”¨æ–¼ç®¡ç†æ›¸ç±åˆ†çµ„\"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.parent = list(range(n))\n",
    "        self.rank = [0] * n\n",
    "\n",
    "    def find(self, x):\n",
    "        if self.parent[x] != x:\n",
    "            self.parent[x] = self.find(self.parent[x])\n",
    "        return self.parent[x]\n",
    "\n",
    "    def union(self, x, y):\n",
    "        px, py = self.find(x), self.find(y)\n",
    "        if px == py:\n",
    "            return\n",
    "        if self.rank[px] < self.rank[py]:\n",
    "            px, py = py, px\n",
    "        self.parent[py] = px\n",
    "        if self.rank[px] == self.rank[py]:\n",
    "            self.rank[px] += 1\n",
    "\n",
    "    def get_groups(self):\n",
    "        \"\"\"å–å¾—æ‰€æœ‰åˆ†çµ„\"\"\"\n",
    "        groups = {}\n",
    "        for i in range(len(self.parent)):\n",
    "            root = self.find(i)\n",
    "            if root not in groups:\n",
    "                groups[root] = []\n",
    "            groups[root].append(i)\n",
    "        return list(groups.values())\n",
    "\n",
    "def normalize_numbers_in_title(title):\n",
    "    \"\"\"\n",
    "    å°‡æ¨™é¡Œä¸­çš„æ•¸å­—çµ±ä¸€è½‰æ›ç‚ºé˜¿æ‹‰ä¼¯æ•¸å­—æ ¼å¼ï¼ˆç”¨æ–¼æ¯”è¼ƒï¼‰\n",
    "    è™•ç†ï¼šä¸­æ–‡æ•¸å­—ï¼ˆä¸€äºŒä¸‰ï¼‰ã€é˜¿æ‹‰ä¼¯æ•¸å­—ï¼ˆ1 2 3ï¼‰ã€å…¨å½¢æ•¸å­—ï¼ˆï¼‘ï¼’ï¼“ï¼‰\n",
    "    \"\"\"\n",
    "    if not cn2an or not title:\n",
    "        return title\n",
    "\n",
    "    normalized = title\n",
    "\n",
    "    try:\n",
    "        # 1. è½‰æ›å…¨å½¢æ•¸å­—ç‚ºåŠå½¢\n",
    "        full_to_half = str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™', '0123456789')\n",
    "        normalized = normalized.translate(full_to_half)\n",
    "\n",
    "        # 2. æ‰¾å‡ºæ‰€æœ‰ä¸­æ–‡æ•¸å­—æ¨¡å¼ä¸¦è½‰æ›\n",
    "        # åŒ¹é…ï¼šç¬¬ä¸€é›†ã€ç¬¬äºŒåä¸‰ç« ã€å·ä¸‰ã€Vol.äº”ã€ç­‰ç­‰\n",
    "        chinese_num_pattern = r'[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒè¬é›¶å£¹è²³åƒè‚†ä¼é™¸æŸ’æŒç–æ‹¾ä½°ä»Ÿ]+'\n",
    "\n",
    "        def replace_chinese_num(match):\n",
    "            chinese_num = match.group(0)\n",
    "            try:\n",
    "                # ä½¿ç”¨ cn2an è½‰æ›ä¸­æ–‡æ•¸å­—ç‚ºé˜¿æ‹‰ä¼¯æ•¸å­—\n",
    "                arabic_num = cn2an.cn2an(chinese_num, \"smart\")\n",
    "                return str(arabic_num)\n",
    "            except:\n",
    "                return chinese_num  # è½‰æ›å¤±æ•—å‰‡ä¿æŒåŸæ¨£\n",
    "\n",
    "        normalized = re.sub(chinese_num_pattern, replace_chinese_num, normalized)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"æ•¸å­—æ¨™æº–åŒ–å¤±æ•—: {e}, æ¨™é¡Œ: {title[:50]}\")\n",
    "        return title\n",
    "\n",
    "    return normalized\n",
    "\n",
    "def clean_title_for_bert(title):\n",
    "    \"\"\"æ¸…ç†æ¨™é¡Œç”¨æ–¼ BERT æ¯”è¼ƒï¼ˆç§»é™¤é›»å­æ›¸ã€é™åˆ¶ç´šç­‰æ¨™è¨˜ï¼‰\"\"\"\n",
    "    if pd.isna(title) or not str(title).strip():\n",
    "        return \"\"\n",
    "\n",
    "    title = str(title).strip()\n",
    "\n",
    "    # ç§»é™¤å¸¸è¦‹çš„æ¨™è¨˜å’Œå¹²æ“¾å­—æ¨£\n",
    "    patterns_to_remove = [\n",
    "        # é›»å­æ›¸ç›¸é—œ\n",
    "        r'\\(é›»å­æ›¸\\)',\n",
    "        r'ï¼ˆé›»å­æ›¸ï¼‰',\n",
    "        r'\\[é›»å­æ›¸\\]',\n",
    "        r'ã€é›»å­æ›¸ã€‘',\n",
    "        r'é›»å­æ›¸',\n",
    "        r'\\(ebook\\)',\n",
    "        r'ï¼ˆebookï¼‰',\n",
    "        r'ebook',\n",
    "        r'e-book',\n",
    "        # é™åˆ¶ç´šç›¸é—œ\n",
    "        r'\\(é™\\)',\n",
    "        r'ï¼ˆé™ï¼‰',\n",
    "        r'\\[é™\\]',\n",
    "        r'ã€é™ã€‘',\n",
    "        r'é™$',  # çµå°¾çš„ã€Œé™ã€\n",
    "        r'é™åˆ¶ç´š',\n",
    "        r'18\\+',\n",
    "        r'18ç¦',\n",
    "        # å…¶ä»–å¸¸è¦‹å¹²æ“¾å­—æ¨£\n",
    "        r'\\(å®Œ\\)',\n",
    "        r'ï¼ˆå®Œï¼‰',\n",
    "        r'\\(æ–°ç‰ˆ\\)',\n",
    "        r'ï¼ˆæ–°ç‰ˆï¼‰',\n",
    "        r'\\(ä¿®è¨‚ç‰ˆ\\)',\n",
    "        r'ï¼ˆä¿®è¨‚ç‰ˆï¼‰',\n",
    "        r'\\(å…¨\\)',\n",
    "        r'ï¼ˆå…¨ï¼‰',\n",
    "        r'\\(å°èªª\\)',\n",
    "        r'ï¼ˆå°èªªï¼‰',\n",
    "        r'\\[å°èªª\\]',\n",
    "        r'ã€å°èªªã€‘',\n",
    "        r'å°èªª',\n",
    "        r'å°èªªç‰ˆ',\n",
    "    ]\n",
    "\n",
    "    for pattern in patterns_to_remove:\n",
    "        title = re.sub(pattern, ' ', title, flags=re.IGNORECASE)\n",
    "\n",
    "    # æ¸…ç†å¤šé¤˜ç©ºæ ¼\n",
    "    title = ' '.join(title.split())\n",
    "    return title.strip()\n",
    "\n",
    "def check_same_book_with_bert(title1, title2):\n",
    "    \"\"\"ä½¿ç”¨ BERT åˆ¤æ–·å…©æœ¬æ›¸æ˜¯å¦ç›¸åŒï¼ˆæ¯”è¼ƒå‰å…ˆæ¨™æº–åŒ–æ•¸å­—ï¼‰\"\"\"\n",
    "    global bert_model\n",
    "\n",
    "    if not BERT_AVAILABLE or bert_model is None:\n",
    "        print(\"BERT æ¨¡å‹æœªè¼‰å…¥\")\n",
    "        return False\n",
    "\n",
    "    if not title1 or not title2:\n",
    "        return False\n",
    "\n",
    "    # æ¸…ç†æ¨™é¡Œ\n",
    "    # cleaned_title1 = clean_title_for_bert(title1)\n",
    "    # cleaned_title2 = clean_title_for_bert(title2)\n",
    "\n",
    "    # if not cleaned_title1 or not cleaned_title2:\n",
    "    #     return False\n",
    "\n",
    "    # æ¨™æº–åŒ–æ•¸å­—å¾Œå†æ¯”è¼ƒ\n",
    "    normalized_title1 = normalize_numbers_in_title(title1)\n",
    "    normalized_title2 = normalize_numbers_in_title(title2)\n",
    "\n",
    "    try:\n",
    "        # è¨ˆç®— embeddings\n",
    "        embedding1 = bert_model.encode(normalized_title1, convert_to_tensor=True)\n",
    "        embedding2 = bert_model.encode(normalized_title2, convert_to_tensor=True)\n",
    "\n",
    "        # è¨ˆç®— cosine ç›¸ä¼¼åº¦\n",
    "        similarity = util.cos_sim(embedding1, embedding2).item()\n",
    "\n",
    "        # åˆ¤æ–·æ˜¯å¦ç‚ºåŒä¸€æœ¬æ›¸\n",
    "        is_same = similarity >= SIMILARITY_THRESHOLD\n",
    "\n",
    "        if is_same:\n",
    "            print(f\"BERT æ¯”è¼ƒ: '{title1[:50]}...' vs '{title2[:50]}...' â†’ ç›¸ä¼¼åº¦: {similarity:.4f} â†’ ç›¸åŒ\")\n",
    "            if normalized_title1 != title1 or normalized_title2 != title2:\n",
    "                print(f\"  æ¨™æº–åŒ–å¾Œ: '{normalized_title1[:50]}...' vs '{normalized_title2[:50]}...'\")\n",
    "\n",
    "        return is_same\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"  âš ï¸ BERT åˆ¤æ–·éŒ¯èª¤: {e}\"\n",
    "        print(error_msg)\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f7da6f",
   "metadata": {
    "id": "f2f7da6f"
   },
   "outputs": [],
   "source": [
    "def merge_two_books(book1, book2):\n",
    "    \"\"\"åˆä½µå…©æœ¬æ›¸çš„è³‡æ–™ï¼ˆå…§éƒ¨ä½¿ç”¨ï¼Œä¸å¢åŠ è¨ˆæ•¸å™¨ï¼‰\"\"\"\n",
    "    merged = {}\n",
    "\n",
    "    if (HAS_ID):\n",
    "        # TAICCA_ID ç³»åˆ—ï¼šä»¥æ–œç·šåˆ†éš”\n",
    "        for col in ['TAICCA_ID']:\n",
    "            val1 = str(book1.get(col, '')).strip()\n",
    "            val2 = str(book2.get(col, '')).strip()\n",
    "            if pd.notna(book1.get(col)) and pd.notna(book2.get(col)):\n",
    "                if val1 and val2 and val1 != val2:\n",
    "                    merged[col] = f\"{val1} / {val2}\"\n",
    "                elif val1:\n",
    "                    merged[col] = val1\n",
    "                elif val2:\n",
    "                    merged[col] = val2\n",
    "            elif pd.notna(book1.get(col)):\n",
    "                merged[col] = val1\n",
    "            elif pd.notna(book2.get(col)):\n",
    "                merged[col] = val2\n",
    "            else:\n",
    "                merged[col] = ''\n",
    "\n",
    "    # isbn ç³»åˆ—ï¼šä¾å¾ªç´™æœ¬æ›¸é‚è¼¯ (Paper version)\n",
    "    for col in ['isbn']:\n",
    "        val1 = str(book1.get(col, '')).strip() if pd.notna(book1.get(col)) else ''\n",
    "        val2 = str(book2.get(col, '')).strip() if pd.notna(book2.get(col)) else ''\n",
    "\n",
    "        # å…©æœ¬éƒ½æœ‰å€¼\n",
    "        if val1 and val2:\n",
    "            if val1 != val2:\n",
    "                # ä¸åŒçš„ ISBNï¼Œç”¨æ–œç·šåˆ†éš”\n",
    "                merged[col] = f\"{val1} / {val2}\"\n",
    "            else:\n",
    "                # ç›¸åŒçš„ ISBNï¼Œåªä¿ç•™ä¸€å€‹\n",
    "                merged[col] = val1\n",
    "        # åªæœ‰ä¸€æœ¬æœ‰å€¼ï¼Œå¦ä¸€æœ¬æ¨™è¨˜ç©ºç™½\n",
    "        elif val1 and not val2:\n",
    "            merged[col] = f\"{val1} / ï¼ˆç©ºç™½ï¼‰\"\n",
    "        elif not val1 and val2:\n",
    "            merged[col] = f\"ï¼ˆç©ºç™½ï¼‰/ {val2}\"\n",
    "        # éƒ½ç‚ºç©º\n",
    "        elif val1:\n",
    "            merged[col] = val1\n",
    "        else:\n",
    "            merged[col] = ''\n",
    "\n",
    "    # ç›´æ¥å¡«è£œçš„æ¬„ä½ - Ebook Specific (readmoo, kobo, bookscom)\n",
    "    fill_cols = [\n",
    "        'bookscom_isbn', 'readmoo_isbn', 'kobo_isbn',\n",
    "        'bookscom_production_id', 'readmoo_production_id', 'kobo_production_id',\n",
    "        'bookscom_title', 'readmoo_title', 'kobo_title',\n",
    "        'bookscom_processed_title', 'readmoo_processed_title', 'kobo_processed_title',\n",
    "        'bookscom_original_title', 'readmoo_original_title', 'kobo_original_title',\n",
    "        'bookscom_author', 'readmoo_author', 'kobo_author',\n",
    "        'bookscom_translator', 'readmoo_translator', 'kobo_translator',\n",
    "        'bookscom_publisher', 'readmoo_publisher', 'kobo_publisher',\n",
    "        'bookscom_publish_date', 'readmoo_publish_date', 'kobo_publish_date',\n",
    "        'bookscom_original_price', 'readmoo_original_price', 'kobo_original_price',\n",
    "        'bookscom_category', 'readmoo_category', 'kobo_category',\n",
    "        'bookscom_url', 'readmoo_url', 'kobo_url'\n",
    "    ]\n",
    "\n",
    "    for col in fill_cols:\n",
    "        # å„ªå…ˆä½¿ç”¨ book1\n",
    "        if pd.notna(book1.get(col)) and str(book1.get(col)).strip():\n",
    "            merged[col] = book1[col]\n",
    "        # å¦å‰‡ä½¿ç”¨ book2\n",
    "        elif pd.notna(book2.get(col)) and str(book2.get(col)).strip():\n",
    "            merged[col] = book2[col]\n",
    "        else:\n",
    "            merged[col] = ''\n",
    "\n",
    "    # ä¿ç•™è¢«åˆä½µè€…çš„å…§å®¹ï¼ˆä½¿ç”¨éè£œé‚è¼¯ï¼‰- Same as Paper\n",
    "    keep_from_book1 = [\n",
    "        'title', 'processed_title', 'original_title',\n",
    "        'author', 'translator', 'publisher'\n",
    "    ]\n",
    "\n",
    "    for col in keep_from_book1:\n",
    "        val1 = book1.get(col, '')\n",
    "        val2 = book2.get(col, '')\n",
    "        \n",
    "        # å„ªå…ˆä½¿ç”¨ book1\n",
    "        if pd.notna(val1) and str(val1).strip():\n",
    "            merged[col] = val1\n",
    "        elif pd.notna(val2) and str(val2).strip():\n",
    "            merged[col] = val2\n",
    "        else:\n",
    "            merged[col] = ''\n",
    "\n",
    "    # min_publish_date å’Œ max_publish_dateï¼šåªå¾ä¸‰é–“æ›¸å•†çš„æ—¥æœŸä¸­æ¯”è¼ƒ\n",
    "    dates = []\n",
    "    for col in ['bookscom_publish_date', 'readmoo_publish_date', 'kobo_publish_date']:\n",
    "        for book in [book1, book2]:\n",
    "            val = book.get(col)\n",
    "            if pd.notna(val) and str(val).strip() and str(val).strip().lower() != 'nan':\n",
    "                try:\n",
    "                    date_str = str(val).strip()\n",
    "                    date_obj = None\n",
    "                    # å˜—è©¦å¤šç¨®æ—¥æœŸæ ¼å¼\n",
    "                    for fmt in ['%Y/%m/%d', '%Y-%m-%d', '%m/%d/%y', '%m/%d/%Y', '%d/%m/%y', '%d/%m/%Y']:\n",
    "                        try:\n",
    "                            date_obj = datetime.datetime.strptime(date_str, fmt)\n",
    "                            break\n",
    "                        except:\n",
    "                            continue\n",
    "                    \n",
    "                    if date_obj:\n",
    "                        dates.append(date_obj)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "    if dates:\n",
    "        merged['min_publish_date'] = min(dates).strftime('%Y-%m-%d')\n",
    "        merged['max_publish_date'] = max(dates).strftime('%Y-%m-%d')\n",
    "    else:\n",
    "        merged['min_publish_date'] = ''\n",
    "        merged['max_publish_date'] = ''\n",
    "\n",
    "    # priceï¼šæœ€å¤§å€¼\n",
    "    prices = []\n",
    "    for col in ['price', 'bookscom_original_price', 'readmoo_original_price', 'kobo_original_price']:\n",
    "        for book in [book1, book2]:\n",
    "            if pd.notna(book.get(col)):\n",
    "                try:\n",
    "                    price = float(book[col])\n",
    "                    prices.append(price)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "    if prices:\n",
    "        merged['price'] = max(prices)\n",
    "    else:\n",
    "        merged['price'] = book1.get('price', '')\n",
    "\n",
    "    return merged\n",
    "\n",
    "def merge_multiple_books(books):\n",
    "    \"\"\"åˆä½µå¤šæœ¬æ›¸çš„è³‡æ–™\"\"\"\n",
    "    global merge_count\n",
    "\n",
    "    if len(books) == 0:\n",
    "        return None\n",
    "    if len(books) == 1:\n",
    "        return books[0]\n",
    "\n",
    "    merge_count += 1\n",
    "\n",
    "    # è¨˜éŒ„åˆä½µè³‡è¨Š\n",
    "    print(f\"åˆä½µ #{merge_count}: {len(books)} æœ¬æ›¸\")\n",
    "    for i, book in enumerate(books):\n",
    "        print(f\"  [{i}] TAICCA_ID: {book.get('TAICCA_ID', 'N/A')}, Title: {book.get('title', 'N/A')[:50]}\")\n",
    "\n",
    "    # ä»¥ç¬¬ä¸€æœ¬æ›¸ç‚ºåŸºç¤ï¼Œé€ä¸€åˆä½µå…¶ä»–æ›¸\n",
    "    result = books[0]\n",
    "    for i in range(1, len(books)):\n",
    "        result = merge_two_books(result, books[i])\n",
    "\n",
    "    if HAS_ID:\n",
    "        print(f\"  åˆä½µå¾Œ TAICCA_ID: {result.get('TAICCA_ID', 'N/A')}\")\n",
    "        \n",
    "    print(f\"  åˆä½µå¾Œ ISBN: {result.get('isbn', 'N/A')}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "def normalize_single_book(book):\n",
    "    \"\"\"æ¨™æº–åŒ–å–®æœ¬æ›¸çš„è³‡æ–™\"\"\"\n",
    "    # è™•ç† priceï¼šå¾å„å€‹ä¾†æºæ‰¾æœ€å¤§å€¼\n",
    "    if not book.get('price') or pd.isna(book.get('price')) or str(book.get('price')).strip() == '':\n",
    "        prices = []\n",
    "        for col in ['bookscom_original_price', 'readmoo_original_price', 'kobo_original_price']:\n",
    "            if pd.notna(book.get(col)):\n",
    "                try:\n",
    "                    price = float(book[col])\n",
    "                    prices.append(price)\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        if prices:\n",
    "            book['price'] = max(prices)\n",
    "        else:\n",
    "            book['price'] = ''\n",
    "    \n",
    "    # è™•ç† dates\n",
    "    dates = []\n",
    "    for col in ['bookscom_publish_date', 'readmoo_publish_date', 'kobo_publish_date']:\n",
    "        if pd.notna(book.get(col)) and str(book.get(col)).strip():\n",
    "            try:\n",
    "                date_str = str(book[col]).strip()\n",
    "                date_obj = None\n",
    "                for fmt in ['%Y/%m/%d', '%Y-%m-%d', '%m/%d/%y', '%m/%d/%Y', '%d/%m/%y', '%d/%m/%Y']:\n",
    "                    try:\n",
    "                        date_obj = datetime.datetime.strptime(date_str, fmt)\n",
    "                        break\n",
    "                    except:\n",
    "                        continue\n",
    "                if date_obj:\n",
    "                    dates.append(date_obj)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    if dates:\n",
    "        book['min_publish_date'] = min(dates).strftime('%Y-%m-%d')\n",
    "        book['max_publish_date'] = max(dates).strftime('%Y-%m-%d')\n",
    "    else:\n",
    "        book['min_publish_date'] = ''\n",
    "        book['max_publish_date'] = ''\n",
    "    \n",
    "    return book\n",
    "\n",
    "import torch\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def stage2_bert_deduplication():\n",
    "    \"\"\"\n",
    "    ç¬¬äºŒéšæ®µï¼šè®€å–åˆ†ç¾¤æª”æ¡ˆã€ä½¿ç”¨ BERT åˆ¤æ–·ä¸¦åˆä½µ (å·²å•Ÿç”¨ CUDA)\n",
    "    \"\"\"\n",
    "    global bert_model\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ğŸ¤– ç¬¬äºŒéšæ®µï¼šBERT å»é‡åˆä½µ\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # è¼‰å…¥ BERT æ¨¡å‹\n",
    "    if not BERT_AVAILABLE:\n",
    "        print(\"âŒ sentence-transformers æœªå®‰è£ï¼Œç„¡æ³•ä½¿ç”¨ BERT\", \"error\")\n",
    "        print(\"è«‹åŸ·è¡Œ: pip install sentence-transformers\", \"error\")\n",
    "        return None\n",
    "\n",
    "    # è¨­å®šè£ç½®\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"\\nä½¿ç”¨è£ç½®: {device}\")\n",
    "    if device == \"cuda\":\n",
    "        print(f\"  - GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "    print(f\"è¼‰å…¥ BERT æ¨¡å‹: {BERT_MODEL}\")\n",
    "    try:\n",
    "        bert_model = SentenceTransformer(BERT_MODEL, device=device)\n",
    "        print(f\"âœ… BERT æ¨¡å‹è¼‰å…¥å®Œæˆ\")\n",
    "        print(f\"ç›¸ä¼¼åº¦é–¾å€¼: {SIMILARITY_THRESHOLD}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ BERT æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}\", \"error\")\n",
    "        return None\n",
    "\n",
    "    # è®€å–æ‰€æœ‰åˆ†ç¾¤æª”æ¡ˆ\n",
    "    cluster_files = glob.glob(os.path.join(CLUSTERED_DATA_DIR, \"cluster_*.csv\"))\n",
    "    cluster_files = [f for f in cluster_files if 'full_data' not in f]\n",
    "\n",
    "    print(f\"\\næ‰¾åˆ° {len(cluster_files)} å€‹åˆ†ç¾¤æª”æ¡ˆ:\")\n",
    "    for f in cluster_files:\n",
    "        print(f\"  - {os.path.basename(f)}\")\n",
    "\n",
    "    if not cluster_files:\n",
    "        print(\"\\nâš ï¸ æ²’æœ‰æ‰¾åˆ°ä»»ä½•åˆ†ç¾¤æª”æ¡ˆ\", \"warning\")\n",
    "        return None\n",
    "\n",
    "    if HAS_ID:\n",
    "        final_output_columns = [\n",
    "            'TAICCA_ID', 'bookscom_production_id', 'readmoo_production_id', 'kobo_production_id',\n",
    "            'isbn', 'bookscom_isbn', 'readmoo_isbn', 'kobo_isbn',\n",
    "            'title', 'bookscom_title', 'readmoo_title', 'kobo_title',\n",
    "            'processed_title', 'bookscom_processed_title', 'readmoo_processed_title', 'kobo_processed_title',\n",
    "            'original_title', 'bookscom_original_title', 'readmoo_original_title', 'kobo_original_title',\n",
    "            'author', 'bookscom_author', 'readmoo_author', 'kobo_author',\n",
    "            'translator', 'bookscom_translator', 'readmoo_translator', 'kobo_translator',\n",
    "            'publisher', 'bookscom_publisher', 'readmoo_publisher', 'kobo_publisher',\n",
    "            'min_publish_date', 'max_publish_date', 'bookscom_publish_date', 'readmoo_publish_date', 'kobo_publish_date',\n",
    "            'price', 'bookscom_original_price', 'readmoo_original_price', 'kobo_original_price',\n",
    "            'bookscom_category', 'readmoo_category', 'kobo_category',\n",
    "            'bookscom_url', 'readmoo_url', 'kobo_url'\n",
    "        ]\n",
    "    else:\n",
    "        final_output_columns = [\n",
    "            'bookscom_production_id', 'readmoo_production_id', 'kobo_production_id',\n",
    "            'isbn', 'bookscom_isbn', 'readmoo_isbn', 'kobo_isbn',\n",
    "            'title', 'bookscom_title', 'readmoo_title', 'kobo_title',\n",
    "            'processed_title', 'bookscom_processed_title', 'readmoo_processed_title', 'kobo_processed_title',\n",
    "            'original_title', 'bookscom_original_title', 'readmoo_original_title', 'kobo_original_title',\n",
    "            'author', 'bookscom_author', 'readmoo_author', 'kobo_author',\n",
    "            'translator', 'bookscom_translator', 'readmoo_translator', 'kobo_translator',\n",
    "            'publisher', 'bookscom_publisher', 'readmoo_publisher', 'kobo_publisher',\n",
    "            'min_publish_date', 'max_publish_date', 'bookscom_publish_date', 'readmoo_publish_date', 'kobo_publish_date',\n",
    "            'price', 'bookscom_original_price', 'readmoo_original_price', 'kobo_original_price',\n",
    "            'bookscom_category', 'readmoo_category', 'kobo_category',\n",
    "            'bookscom_url', 'readmoo_url', 'kobo_url'\n",
    "        ]\n",
    "\n",
    "    total_original = 0\n",
    "    total_output = 0\n",
    "\n",
    "    # è™•ç†æ¯å€‹åˆ†ç¾¤æª”æ¡ˆä¸¦å³æ™‚å¯«å…¥\n",
    "    for idx, cluster_file in enumerate(cluster_files):\n",
    "        # è®€å–ä¸¦æ’é™¤ä¸éœ€è¦çš„æ¬„ä½\n",
    "        temp_df = pd.read_csv(cluster_file)\n",
    "        columns_to_drop = ['processed_title_prefix', 'embedding_parsed']\n",
    "        temp_df = temp_df.drop(columns=[col for col in columns_to_drop if col in temp_df.columns], errors='ignore')\n",
    "        original_count = len(temp_df)\n",
    "        total_original += original_count\n",
    "\n",
    "        is_noise_file = 'noise' in os.path.basename(cluster_file).lower()\n",
    "\n",
    "        if is_noise_file:\n",
    "            # å™ªéŸ³æª”æ¡ˆç›´æ¥å¯«å…¥\n",
    "            filename = os.path.basename(cluster_file)\n",
    "            print(f\"\\nğŸ“‚ è™•ç†æª”æ¡ˆ: {filename}\")\n",
    "            print(f\"é–‹å§‹è™•ç†å™ªéŸ³æª”æ¡ˆ: {cluster_file}\")\n",
    "\n",
    "            df = temp_df\n",
    "            print(f\"  - è®€å– {len(df)} ç­†è³‡æ–™\")\n",
    "            print(f\"  âš¡ å™ªéŸ³æª”æ¡ˆï¼Œæ¨™æº–åŒ–å¾Œå¯«å…¥ï¼ˆè·³éæ¯”è¼ƒï¼‰\")\n",
    "\n",
    "            if len(df) > 0:\n",
    "                # æ¨™æº–åŒ–æ¯ä¸€è¡Œçš„ price å’Œ publish_date æ¬„ä½\n",
    "                books = df.to_dict('records')\n",
    "                normalized_books = [normalize_single_book(book) for book in books]\n",
    "                df = pd.DataFrame(normalized_books)\n",
    "                \n",
    "                df = df[[col for col in final_output_columns if col in df.columns]]\n",
    "\n",
    "                if idx == 0:\n",
    "                    df.to_csv(FINAL_OUTPUT_FILE, index=False, encoding='utf-8-sig', mode='w')\n",
    "                    print(f\"  ğŸ’¾ å·²å¯«å…¥ {len(df)} ç­†è³‡æ–™åˆ° {FINAL_OUTPUT_FILE} (æ–°å»ºæª”æ¡ˆ)\")\n",
    "                else:\n",
    "                    df.to_csv(FINAL_OUTPUT_FILE, index=False, encoding='utf-8-sig', mode='a', header=False)\n",
    "                    print(f\"  ğŸ’¾ å·²è¿½åŠ  {len(df)} ç­†è³‡æ–™åˆ° {FINAL_OUTPUT_FILE}\")\n",
    "\n",
    "                print(f\"{filename}: ç›´æ¥å¯«å…¥ {len(df)} ç­†è³‡æ–™ï¼ˆå™ªéŸ³æª”æ¡ˆï¼‰\")\n",
    "                total_output += len(df)\n",
    "        else:\n",
    "            # ä¸€èˆ¬åˆ†ç¾¤æª”æ¡ˆï¼šé€²è¡Œæ¯”è¼ƒ\n",
    "            results = process_cluster_file(cluster_file)\n",
    "\n",
    "            if results:\n",
    "                result_df = pd.DataFrame(results)\n",
    "                result_df = result_df[[col for col in final_output_columns if col in result_df.columns]]\n",
    "\n",
    "                if idx == 0:\n",
    "                    result_df.to_csv(FINAL_OUTPUT_FILE, index=False, encoding='utf-8-sig', mode='w')\n",
    "                    print(f\"  ğŸ’¾ å·²å¯«å…¥ {len(result_df)} ç­†è³‡æ–™åˆ° {FINAL_OUTPUT_FILE} (æ–°å»ºæª”æ¡ˆ)\")\n",
    "                else:\n",
    "                    result_df.to_csv(FINAL_OUTPUT_FILE, index=False, encoding='utf-8-sig', mode='a', header=False)\n",
    "                    print(f\"  ğŸ’¾ å·²è¿½åŠ  {len(result_df)} ç­†è³‡æ–™åˆ° {FINAL_OUTPUT_FILE}\")\n",
    "\n",
    "                print(f\"å·²å¯«å…¥ {len(result_df)} ç­†è³‡æ–™åˆ° {FINAL_OUTPUT_FILE}\")\n",
    "                total_output += len(result_df)\n",
    "\n",
    "    print(\"\\nâœ… ç¬¬äºŒéšæ®µå®Œæˆï¼å»é‡åˆä½µå·²å®Œæˆã€‚\")\n",
    "\n",
    "    return {\n",
    "        'total_original': total_original,\n",
    "        'total_output': total_output,\n",
    "        'merges': merge_count\n",
    "    }\n",
    "\n",
    "def process_cluster_file(csv_file):\n",
    "    \"\"\"è™•ç†å–®å€‹åˆ†ç¾¤æª”æ¡ˆï¼ˆæ”¯æ´å¤šæœ¬æ›¸åˆä½µï¼‰\"\"\"\n",
    "    filename = os.path.basename(csv_file)\n",
    "    print(f\"\\nğŸ“‚ è™•ç†æª”æ¡ˆ: {filename}\")\n",
    "    print(f\"é–‹å§‹è™•ç†: {csv_file}\")\n",
    "\n",
    "    # è®€å– CSVï¼Œæ’é™¤ä¸éœ€è¦çš„æ¬„ä½\n",
    "    df = pd.read_csv(csv_file)\n",
    "    columns_to_drop = ['processed_title_prefix', 'embedding_parsed']\n",
    "    df = df.drop(columns=[col for col in columns_to_drop if col in df.columns], errors='ignore')\n",
    "    print(f\"  - è®€å– {len(df)} ç­†è³‡æ–™\")\n",
    "\n",
    "    if len(df) == 0:\n",
    "        print(f\"{filename} æ²’æœ‰è³‡æ–™\")\n",
    "        return []\n",
    "\n",
    "    # å»ºç«‹ processed_title_stripped æ¬„ä½\n",
    "    print(\"  - å»ºç«‹ processed_title_stripped æ¬„ä½ (å°å¯« + å»ç©ºç™½)...\")\n",
    "\n",
    "    def get_stripped_title(row):\n",
    "        # å„ªå…ˆä½¿ç”¨ processed_titleï¼Œå¦‚æœæ²’æœ‰å‰‡ä½¿ç”¨ title\n",
    "        src_title = row.get('processed_title', '')\n",
    "        if pd.isna(src_title) or not str(src_title).strip():\n",
    "            src_title = row.get('title', '')\n",
    "\n",
    "        # 1. å…ˆé€²è¡ŒåŸæœ¬çš„ clean_ebook_text æ¸…ç†\n",
    "        cleaned = clean_title_for_bert(src_title)\n",
    "\n",
    "        # 2. è½‰ç‚ºå°å¯«\n",
    "        cleaned = cleaned.lower()\n",
    "\n",
    "        # 3. å»é™¤æ‰€æœ‰ç©ºæ ¼\n",
    "        cleaned = cleaned.replace(\" \", \"\")\n",
    "\n",
    "        return cleaned\n",
    "\n",
    "    df['processed_title_stripped'] = df.apply(get_stripped_title, axis=1)\n",
    "\n",
    "    books = df.to_dict('records')\n",
    "    n = len(books)\n",
    "\n",
    "    # ä½¿ç”¨ä¸¦æŸ¥é›†ç®¡ç†æ›¸ç±åˆ†çµ„\n",
    "    uf = UnionFind(n)\n",
    "\n",
    "    print(f\"  - é–‹å§‹å…©å…©æ¯”è¼ƒ...\")\n",
    "    comparison_count = 0\n",
    "    total_comparisons = n * (n - 1) // 2\n",
    "\n",
    "    # å…©å…©æ¯”è¼ƒæ‰€æœ‰æ›¸ç±\n",
    "    for i in tqdm(range(n), desc=\"  æ¯”è¼ƒæ›¸ç±\"):\n",
    "        title1 = str(books[i].get('processed_title_stripped', '')).strip()\n",
    "        if not title1:\n",
    "            continue\n",
    "\n",
    "        for j in range(i + 1, n):\n",
    "            title2 = str(books[j].get('processed_title_stripped', '')).strip()\n",
    "            if not title2:\n",
    "                continue\n",
    "\n",
    "            comparison_count += 1\n",
    "\n",
    "            # ä½¿ç”¨ BERT åˆ¤æ–·æ˜¯å¦ç‚ºåŒä¸€æœ¬æ›¸\n",
    "            is_same = check_same_book_with_bert(title1, title2)\n",
    "\n",
    "            if is_same:\n",
    "                print(f\"    âœ… æ‰¾åˆ°ç›¸åŒæ›¸ç±:\")\n",
    "                print(f\"       [{i}] {title1[:60]}\")\n",
    "                print(f\"       [{j}] {title2[:60]}\")\n",
    "\n",
    "                # å°‡å…©æœ¬æ›¸åŠ å…¥åŒä¸€çµ„\n",
    "                uf.union(i, j)\n",
    "\n",
    "    print(f\"  - å®Œæˆ {comparison_count} æ¬¡æ¯”è¼ƒ\")\n",
    "\n",
    "    # å–å¾—æ‰€æœ‰åˆ†çµ„\n",
    "    groups = uf.get_groups()\n",
    "    print(f\"  - è­˜åˆ¥å‡º {len(groups)} å€‹ç¨ç«‹æ›¸ç±ï¼ˆçµ„ï¼‰\")\n",
    "\n",
    "    # å°æ¯ä¸€çµ„é€²è¡Œåˆä½µ\n",
    "    result_books = []\n",
    "    multi_book_groups = 0\n",
    "\n",
    "    for group_indices in groups:\n",
    "        group_books = [books[i] for i in group_indices]\n",
    "\n",
    "        if len(group_books) > 1:\n",
    "            multi_book_groups += 1\n",
    "            print(f\"    ğŸ“š åˆä½µ {len(group_books)} æœ¬ç›¸åŒçš„æ›¸:\")\n",
    "            for idx in group_indices:\n",
    "                book_title = str(books[idx].get('title', ''))[:60]\n",
    "                print(f\"       - {book_title}\")\n",
    "\n",
    "            # åˆä½µå¤šæœ¬æ›¸\n",
    "            merged_book = merge_multiple_books(group_books)\n",
    "\n",
    "            # ç¢ºä¿åˆä½µå¾Œçš„æ›¸ä¹Ÿæœ‰ processed_title_stripped æ¬„ä½\n",
    "            if 'processed_title_stripped' not in merged_book:\n",
    "                merged_book['processed_title_stripped'] = group_books[0].get('processed_title_stripped', '')\n",
    "\n",
    "            result_books.append(merged_book)\n",
    "        else:\n",
    "            # å–®ç¨çš„æ›¸ï¼šæ¨™æº–åŒ–å¾ŒåŠ å…¥\n",
    "            normalized_book = normalize_single_book(group_books[0])\n",
    "            result_books.append(normalized_book)\n",
    "\n",
    "    print(f\"  âœ… è™•ç†å®Œæˆ: {len(df)} ç­† â†’ {len(result_books)} ç­†\")\n",
    "    if multi_book_groups > 0:\n",
    "        print(f\"  ğŸ“Š å…¶ä¸­ {multi_book_groups} çµ„åŒ…å«å¤šæœ¬é‡è¤‡æ›¸ç±\")\n",
    "\n",
    "    print(f\"{filename} è™•ç†çµæœ: {len(df)} ç­† â†’ {len(result_books)} ç­†, å¤šæ›¸çµ„: {multi_book_groups}\")\n",
    "\n",
    "    return result_books\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "343de53a",
   "metadata": {
    "id": "343de53a"
   },
   "outputs": [],
   "source": [
    "def process_cluster_file(csv_file):\n",
    "    \"\"\"è™•ç†å–®å€‹åˆ†ç¾¤æª”æ¡ˆï¼ˆæ”¯æ´å¤šæœ¬æ›¸åˆä½µï¼‰\"\"\"\n",
    "    filename = os.path.basename(csv_file)\n",
    "    print(f\"\\nğŸ“‚ è™•ç†æª”æ¡ˆ: {filename}\")\n",
    "    print(f\"é–‹å§‹è™•ç†: {csv_file}\")\n",
    "\n",
    "    # è®€å– CSVï¼Œæ’é™¤ä¸éœ€è¦çš„æ¬„ä½\n",
    "    df = pd.read_csv(csv_file)\n",
    "    columns_to_drop = ['processed_title_prefix', 'embedding_parsed']\n",
    "    df = df.drop(columns=[col for col in columns_to_drop if col in df.columns], errors='ignore')\n",
    "    print(f\"  - è®€å– {len(df)} ç­†è³‡æ–™\")\n",
    "\n",
    "    if len(df) == 0:\n",
    "        print(f\"{filename} æ²’æœ‰è³‡æ–™\")\n",
    "        return []\n",
    "\n",
    "    # å»ºç«‹ processed_title_stripped æ¬„ä½\n",
    "    print(\"  - å»ºç«‹ processed_title_stripped æ¬„ä½ (å°å¯« + å»ç©ºç™½)...\")\n",
    "\n",
    "    def get_stripped_title(row):\n",
    "        # å„ªå…ˆä½¿ç”¨ processed_titleï¼Œå¦‚æœæ²’æœ‰å‰‡ä½¿ç”¨ title\n",
    "        src_title = row.get('processed_title', '')\n",
    "        if pd.isna(src_title) or not str(src_title).strip():\n",
    "            src_title = row.get('title', '')\n",
    "\n",
    "        # 1. å…ˆé€²è¡ŒåŸæœ¬çš„ clean_ebook_text æ¸…ç†\n",
    "        cleaned = clean_title_for_bert(src_title)\n",
    "\n",
    "        # 2. è½‰ç‚ºå°å¯«\n",
    "        cleaned = cleaned.lower()\n",
    "\n",
    "        # 3. å»é™¤æ‰€æœ‰ç©ºæ ¼\n",
    "        cleaned = cleaned.replace(\" \", \"\")\n",
    "\n",
    "        return cleaned\n",
    "\n",
    "    df['processed_title_stripped'] = df.apply(get_stripped_title, axis=1)\n",
    "\n",
    "    books = df.to_dict('records')\n",
    "    n = len(books)\n",
    "\n",
    "    # ä½¿ç”¨ä¸¦æŸ¥é›†ç®¡ç†æ›¸ç±åˆ†çµ„\n",
    "    uf = UnionFind(n)\n",
    "\n",
    "    print(f\"  - é–‹å§‹å…©å…©æ¯”è¼ƒ...\")\n",
    "    comparison_count = 0\n",
    "    total_comparisons = n * (n - 1) // 2\n",
    "\n",
    "    # å…©å…©æ¯”è¼ƒæ‰€æœ‰æ›¸ç±\n",
    "    for i in tqdm(range(n), desc=\"  æ¯”è¼ƒæ›¸ç±\"):\n",
    "        title1 = str(books[i].get('processed_title_stripped', '')).strip()\n",
    "        if not title1:\n",
    "            continue\n",
    "\n",
    "        for j in range(i + 1, n):\n",
    "            title2 = str(books[j].get('processed_title_stripped', '')).strip()\n",
    "            if not title2:\n",
    "                continue\n",
    "\n",
    "            comparison_count += 1\n",
    "\n",
    "            # ä½¿ç”¨ BERT åˆ¤æ–·æ˜¯å¦ç‚ºåŒä¸€æœ¬æ›¸\n",
    "            is_same = check_same_book_with_bert(title1, title2)\n",
    "\n",
    "            if is_same:\n",
    "                print(f\"    âœ… æ‰¾åˆ°ç›¸åŒæ›¸ç±:\")\n",
    "                print(f\"       [{i}] {title1[:60]}\")\n",
    "                print(f\"       [{j}] {title2[:60]}\")\n",
    "\n",
    "                # å°‡å…©æœ¬æ›¸åŠ å…¥åŒä¸€çµ„\n",
    "                uf.union(i, j)\n",
    "\n",
    "    print(f\"  - å®Œæˆ {comparison_count} æ¬¡æ¯”è¼ƒ\")\n",
    "\n",
    "    # å–å¾—æ‰€æœ‰åˆ†çµ„\n",
    "    groups = uf.get_groups()\n",
    "    print(f\"  - è­˜åˆ¥å‡º {len(groups)} å€‹ç¨ç«‹æ›¸ç±ï¼ˆçµ„ï¼‰\")\n",
    "\n",
    "    # å°æ¯ä¸€çµ„é€²è¡Œåˆä½µ\n",
    "    result_books = []\n",
    "    multi_book_groups = 0\n",
    "\n",
    "    for group_indices in groups:\n",
    "        group_books = [books[i] for i in group_indices]\n",
    "\n",
    "        if len(group_books) > 1:\n",
    "            multi_book_groups += 1\n",
    "            print(f\"    ğŸ“š åˆä½µ {len(group_books)} æœ¬ç›¸åŒçš„æ›¸:\")\n",
    "            for idx in group_indices:\n",
    "                book_title = str(books[idx].get('title', ''))[:60]\n",
    "                print(f\"       - {book_title}\")\n",
    "\n",
    "            # åˆä½µå¤šæœ¬æ›¸\n",
    "            merged_book = merge_multiple_books(group_books)\n",
    "\n",
    "            # ç¢ºä¿åˆä½µå¾Œçš„æ›¸ä¹Ÿæœ‰ processed_title_stripped æ¬„ä½\n",
    "            if 'processed_title_stripped' not in merged_book:\n",
    "                merged_book['processed_title_stripped'] = group_books[0].get('processed_title_stripped', '')\n",
    "\n",
    "            result_books.append(merged_book)\n",
    "        else:\n",
    "            # å–®ç¨çš„æ›¸ï¼šæ¨™æº–åŒ–å¾ŒåŠ å…¥\n",
    "            normalized_book = normalize_single_book(group_books[0])\n",
    "            result_books.append(normalized_book)\n",
    "\n",
    "    print(f\"  âœ… è™•ç†å®Œæˆ: {len(df)} ç­† â†’ {len(result_books)} ç­†\")\n",
    "    if multi_book_groups > 0:\n",
    "        print(f\"  ğŸ“Š å…¶ä¸­ {multi_book_groups} çµ„åŒ…å«å¤šæœ¬é‡è¤‡æ›¸ç±\")\n",
    "\n",
    "    print(f\"{filename} è™•ç†çµæœ: {len(df)} ç­† â†’ {len(result_books)} ç­†, å¤šæ›¸çµ„: {multi_book_groups}\")\n",
    "\n",
    "    return result_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1KPACd6X9_ez",
   "metadata": {
    "id": "1KPACd6X9_ez"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def stage2_bert_deduplication():\n",
    "    \"\"\"\n",
    "    ç¬¬äºŒéšæ®µï¼šè®€å–åˆ†ç¾¤æª”æ¡ˆã€ä½¿ç”¨ BERT åˆ¤æ–·ä¸¦åˆä½µ (å·²å•Ÿç”¨ CUDA)\n",
    "    \"\"\"\n",
    "    global bert_model\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ğŸ¤– ç¬¬äºŒéšæ®µï¼šBERT å»é‡åˆä½µ\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # è¼‰å…¥ BERT æ¨¡å‹\n",
    "    if not BERT_AVAILABLE:\n",
    "        print(\"âŒ sentence-transformers æœªå®‰è£ï¼Œç„¡æ³•ä½¿ç”¨ BERT\", 'error')\n",
    "        print(\"è«‹åŸ·è¡Œ: pip install sentence-transformers\", 'error')\n",
    "        return None\n",
    "\n",
    "    # è¨­å®šè£ç½®\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"\\nä½¿ç”¨è£ç½®: {device}\")\n",
    "    if device == \"cuda\":\n",
    "        print(f\"  - GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "    print(f\"è¼‰å…¥ BERT æ¨¡å‹: {BERT_MODEL}\")\n",
    "    try:\n",
    "        bert_model = SentenceTransformer(BERT_MODEL, device=device)\n",
    "        print(f\"âœ… BERT æ¨¡å‹è¼‰å…¥å®Œæˆ\")\n",
    "        print(f\"ç›¸ä¼¼åº¦é–¾å€¼: {SIMILARITY_THRESHOLD}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ BERT æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}\", 'error')\n",
    "        return None\n",
    "\n",
    "    # è®€å–æ‰€æœ‰åˆ†ç¾¤æª”æ¡ˆ\n",
    "    cluster_files = glob.glob(os.path.join(CLUSTERED_DATA_DIR, \"cluster_*.csv\"))\n",
    "    cluster_files = [f for f in cluster_files if 'full_data' not in f]\n",
    "\n",
    "    print(f\"\\næ‰¾åˆ° {len(cluster_files)} å€‹åˆ†ç¾¤æª”æ¡ˆ:\")\n",
    "    for f in cluster_files:\n",
    "        print(f\"  - {os.path.basename(f)}\")\n",
    "\n",
    "    if not cluster_files:\n",
    "        print(\"\\nâš ï¸ æ²’æœ‰æ‰¾åˆ°ä»»ä½•åˆ†ç¾¤æª”æ¡ˆ\", 'warning')\n",
    "        return None\n",
    "\n",
    "    if HAS_ID:\n",
    "        # å®šç¾©æœ€çµ‚è¼¸å‡ºçš„æ¬„ä½ï¼ˆæ’é™¤ processed_title_prefix, embedding_parsed, processed_title_strippedï¼‰\n",
    "        final_output_columns = [\n",
    "            'TAICCA_ID', 'bookscom_production_id', 'readmoo_production_id', 'kobo_production_id',\n",
    "            'isbn', 'bookscom_isbn', 'readmoo_isbn', 'kobo_isbn',\n",
    "            'title', 'bookscom_title', 'readmoo_title', 'kobo_title',\n",
    "            'processed_title', 'bookscom_processed_title', 'readmoo_processed_title', 'kobo_processed_title',\n",
    "            'original_title', 'bookscom_original_title', 'readmoo_original_title', 'kobo_original_title',\n",
    "            'author', 'bookscom_author', 'readmoo_author', 'kobo_author',\n",
    "            'translator', 'bookscom_translator', 'readmoo_translator', 'kobo_translator',\n",
    "            'publisher', 'bookscom_publisher', 'readmoo_publisher', 'kobo_publisher',\n",
    "            'min_publish_date', 'max_publish_date', 'bookscom_publish_date', 'readmoo_publish_date', 'kobo_publish_date',\n",
    "            'price', 'bookscom_original_price', 'readmoo_original_price', 'kobo_original_price',\n",
    "            'bookscom_category', 'readmoo_category', 'kobo_category',\n",
    "            'bookscom_url', 'readmoo_url', 'kobo_url'\n",
    "        ]\n",
    "    else:\n",
    "        final_output_columns = [\n",
    "            'bookscom_production_id', 'readmoo_production_id', 'kobo_production_id',\n",
    "            'isbn', 'bookscom_isbn', 'readmoo_isbn', 'kobo_isbn',\n",
    "            'title', 'bookscom_title', 'readmoo_title', 'kobo_title',\n",
    "            'processed_title', 'bookscom_processed_title', 'readmoo_processed_title', 'kobo_processed_title',\n",
    "            'original_title', 'bookscom_original_title', 'readmoo_original_title', 'kobo_original_title',\n",
    "            'author', 'bookscom_author', 'readmoo_author', 'kobo_author',\n",
    "            'translator', 'bookscom_translator', 'readmoo_translator', 'kobo_translator',\n",
    "            'publisher', 'bookscom_publisher', 'readmoo_publisher', 'kobo_publisher',\n",
    "            'min_publish_date', 'max_publish_date', 'bookscom_publish_date', 'readmoo_publish_date', 'kobo_publish_date',\n",
    "            'price', 'bookscom_original_price', 'readmoo_original_price', 'kobo_original_price',\n",
    "            'bookscom_category', 'readmoo_category', 'kobo_category',\n",
    "            'bookscom_url', 'readmoo_url', 'kobo_url'\n",
    "        ]\n",
    "\n",
    "    total_original = 0\n",
    "    total_output = 0\n",
    "\n",
    "    # è™•ç†æ¯å€‹åˆ†ç¾¤æª”æ¡ˆä¸¦å³æ™‚å¯«å…¥\n",
    "    for idx, cluster_file in enumerate(cluster_files):\n",
    "        # è®€å–ä¸¦æ’é™¤ä¸éœ€è¦çš„æ¬„ä½\n",
    "        temp_df = pd.read_csv(cluster_file)\n",
    "        columns_to_drop = ['processed_title_new', 'embedding_parsed']\n",
    "        temp_df = temp_df.drop(columns=[col for col in columns_to_drop if col in temp_df.columns], errors='ignore')\n",
    "        original_count = len(temp_df)\n",
    "        total_original += original_count\n",
    "\n",
    "        is_noise_file = 'noise' in os.path.basename(cluster_file).lower()\n",
    "\n",
    "        if is_noise_file:\n",
    "            # å™ªéŸ³æª”æ¡ˆç›´æ¥å¯«å…¥\n",
    "            filename = os.path.basename(cluster_file)\n",
    "            print(f\"\\nğŸ“‚ è™•ç†æª”æ¡ˆ: {filename}\")\n",
    "            print(f\"é–‹å§‹è™•ç†å™ªéŸ³æª”æ¡ˆ: {cluster_file}\")\n",
    "\n",
    "            df = temp_df\n",
    "            print(f\"  - è®€å– {len(df)} ç­†è³‡æ–™\")\n",
    "            print(f\"  âš¡ å™ªéŸ³æª”æ¡ˆï¼Œæ¨™æº–åŒ–å¾Œå¯«å…¥ï¼ˆè·³éæ¯”è¼ƒï¼‰\")\n",
    "\n",
    "            if len(df) > 0:\n",
    "                # æ¨™æº–åŒ–æ¯ä¸€è¡Œçš„ price å’Œ publish_date æ¬„ä½\n",
    "                books = df.to_dict('records')\n",
    "                normalized_books = [normalize_single_book(book) for book in books]\n",
    "                df = pd.DataFrame(normalized_books)\n",
    "                \n",
    "                df = df[[col for col in final_output_columns if col in df.columns]]\n",
    "\n",
    "                if idx == 0:\n",
    "                    df.to_csv(FINAL_OUTPUT_FILE, index=False, encoding='utf-8-sig', mode='w')\n",
    "                    print(f\"  ğŸ’¾ å·²å¯«å…¥ {len(df)} ç­†è³‡æ–™åˆ° {FINAL_OUTPUT_FILE} (æ–°å»ºæª”æ¡ˆ)\")\n",
    "                else:\n",
    "                    df.to_csv(FINAL_OUTPUT_FILE, index=False, encoding='utf-8-sig', mode='a', header=False)\n",
    "                    print(f\"  ğŸ’¾ å·²è¿½åŠ  {len(df)} ç­†è³‡æ–™åˆ° {FINAL_OUTPUT_FILE}\")\n",
    "\n",
    "                print(f\"{filename}: ç›´æ¥å¯«å…¥ {len(df)} ç­†è³‡æ–™ï¼ˆå™ªéŸ³æª”æ¡ˆï¼‰\")\n",
    "                total_output += len(df)\n",
    "        else:\n",
    "            # ä¸€èˆ¬åˆ†ç¾¤æª”æ¡ˆï¼šé€²è¡Œæ¯”è¼ƒ\n",
    "            results = process_cluster_file(cluster_file)\n",
    "\n",
    "            if results:\n",
    "                result_df = pd.DataFrame(results)\n",
    "                result_df = result_df[[col for col in final_output_columns if col in result_df.columns]]\n",
    "\n",
    "                if idx == 0:\n",
    "                    result_df.to_csv(FINAL_OUTPUT_FILE, index=False, encoding='utf-8-sig', mode='w')\n",
    "                    print(f\"  ğŸ’¾ å·²å¯«å…¥ {len(result_df)} ç­†è³‡æ–™åˆ° {FINAL_OUTPUT_FILE} (æ–°å»ºæª”æ¡ˆ)\")\n",
    "                else:\n",
    "                    result_df.to_csv(FINAL_OUTPUT_FILE, index=False, encoding='utf-8-sig', mode='a', header=False)\n",
    "                    print(f\"  ğŸ’¾ å·²è¿½åŠ  {len(result_df)} ç­†è³‡æ–™åˆ° {FINAL_OUTPUT_FILE}\")\n",
    "\n",
    "                print(f\"å·²å¯«å…¥ {len(result_df)} ç­†è³‡æ–™åˆ° {FINAL_OUTPUT_FILE}\")\n",
    "                total_output += len(result_df)\n",
    "\n",
    "    print(\"\\nâœ… ç¬¬äºŒéšæ®µå®Œæˆï¼å»é‡åˆä½µå·²å®Œæˆã€‚\")\n",
    "\n",
    "    return {\n",
    "        'total_original': total_original,\n",
    "        'total_output': total_output,\n",
    "        'merges': merge_count\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1e4e09dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ¤– ç¬¬äºŒéšæ®µï¼šBERT å»é‡åˆä½µ\n",
      "================================================================================\n",
      "\n",
      "ä½¿ç”¨è£ç½®: cpu\n",
      "è¼‰å…¥ BERT æ¨¡å‹: paraphrase-multilingual-MiniLM-L12-v2\n",
      "âœ… BERT æ¨¡å‹è¼‰å…¥å®Œæˆ\n",
      "ç›¸ä¼¼åº¦é–¾å€¼: 0.99\n",
      "\n",
      "æ‰¾åˆ° 3 å€‹åˆ†ç¾¤æª”æ¡ˆ:\n",
      "  - cluster_2.csv\n",
      "  - cluster_0.csv\n",
      "  - cluster_1.csv\n",
      "\n",
      "ğŸ“‚ è™•ç†æª”æ¡ˆ: cluster_2.csv\n",
      "é–‹å§‹è™•ç†: /Users/alioth1225/Documents/College/merge/clustering/ebook_clustered_data/cluster_2.csv\n",
      "  - è®€å– 6 ç­†è³‡æ–™\n",
      "  - å»ºç«‹ processed_title_stripped æ¬„ä½ (å°å¯« + å»ç©ºç™½)...\n",
      "  - é–‹å§‹å…©å…©æ¯”è¼ƒ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  æ¯”è¼ƒæ›¸ç±:   0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT æ¯”è¼ƒ: 'ç†Ÿå¥³è¨˜49...' vs 'ç†Ÿå¥³è¨˜49...' â†’ ç›¸ä¼¼åº¦: 1.0000 â†’ ç›¸åŒ\n",
      "    âœ… æ‰¾åˆ°ç›¸åŒæ›¸ç±:\n",
      "       [0] ç†Ÿå¥³è¨˜49\n",
      "       [2] ç†Ÿå¥³è¨˜49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  æ¯”è¼ƒæ›¸ç±:  17%|â–ˆâ–‹        | 1/6 [00:00<00:01,  2.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT æ¯”è¼ƒ: 'ç†Ÿå¥³è¨˜49...' vs 'ç†Ÿå¥³è¨˜49...' â†’ ç›¸ä¼¼åº¦: 1.0000 â†’ ç›¸åŒ\n",
      "    âœ… æ‰¾åˆ°ç›¸åŒæ›¸ç±:\n",
      "       [0] ç†Ÿå¥³è¨˜49\n",
      "       [4] ç†Ÿå¥³è¨˜49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  æ¯”è¼ƒæ›¸ç±:  33%|â–ˆâ–ˆâ–ˆâ–      | 2/6 [00:00<00:01,  3.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT æ¯”è¼ƒ: 'ç†Ÿå¥³è¨˜50...' vs 'ç†Ÿå¥³è¨˜50...' â†’ ç›¸ä¼¼åº¦: 1.0000 â†’ ç›¸åŒ\n",
      "    âœ… æ‰¾åˆ°ç›¸åŒæ›¸ç±:\n",
      "       [1] ç†Ÿå¥³è¨˜50\n",
      "       [3] ç†Ÿå¥³è¨˜50\n",
      "BERT æ¯”è¼ƒ: 'ç†Ÿå¥³è¨˜50...' vs 'ç†Ÿå¥³è¨˜50...' â†’ ç›¸ä¼¼åº¦: 1.0000 â†’ ç›¸åŒ\n",
      "    âœ… æ‰¾åˆ°ç›¸åŒæ›¸ç±:\n",
      "       [1] ç†Ÿå¥³è¨˜50\n",
      "       [5] ç†Ÿå¥³è¨˜50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  æ¯”è¼ƒæ›¸ç±:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:00<00:00,  3.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT æ¯”è¼ƒ: 'ç†Ÿå¥³è¨˜49...' vs 'ç†Ÿå¥³è¨˜49...' â†’ ç›¸ä¼¼åº¦: 1.0000 â†’ ç›¸åŒ\n",
      "    âœ… æ‰¾åˆ°ç›¸åŒæ›¸ç±:\n",
      "       [2] ç†Ÿå¥³è¨˜49\n",
      "       [4] ç†Ÿå¥³è¨˜49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  æ¯”è¼ƒæ›¸ç±: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:01<00:00,  5.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT æ¯”è¼ƒ: 'ç†Ÿå¥³è¨˜50...' vs 'ç†Ÿå¥³è¨˜50...' â†’ ç›¸ä¼¼åº¦: 1.0000 â†’ ç›¸åŒ\n",
      "    âœ… æ‰¾åˆ°ç›¸åŒæ›¸ç±:\n",
      "       [3] ç†Ÿå¥³è¨˜50\n",
      "       [5] ç†Ÿå¥³è¨˜50\n",
      "  - å®Œæˆ 15 æ¬¡æ¯”è¼ƒ\n",
      "  - è­˜åˆ¥å‡º 2 å€‹ç¨ç«‹æ›¸ç±ï¼ˆçµ„ï¼‰\n",
      "    ğŸ“š åˆä½µ 3 æœ¬ç›¸åŒçš„æ›¸:\n",
      "       - ç†Ÿå¥³è¨˜(49)\n",
      "       - ç†Ÿå¥³è¨˜(49)\n",
      "       - ç†Ÿå¥³è¨˜49(é™)\n",
      "åˆä½µ #5: 3 æœ¬æ›¸\n",
      "  [0] TAICCA_ID: E253-12552, Title: ç†Ÿå¥³è¨˜(49)\n",
      "  [1] TAICCA_ID: E253-20325, Title: ç†Ÿå¥³è¨˜(49)\n",
      "  [2] TAICCA_ID: E253-12561, Title: ç†Ÿå¥³è¨˜49(é™)\n",
      "  åˆä½µå¾Œ TAICCA_ID: E253-12552 / E253-20325 / E253-12561\n",
      "  åˆä½µå¾Œ ISBN: ï¼ˆç©ºç™½ï¼‰/ï¼ˆç©ºç™½ï¼‰ / ï¼ˆç©ºç™½ï¼‰\n",
      "    ğŸ“š åˆä½µ 3 æœ¬ç›¸åŒçš„æ›¸:\n",
      "       - ç†Ÿå¥³è¨˜(50)\n",
      "       - ç†Ÿå¥³è¨˜(50)\n",
      "       - ç†Ÿå¥³è¨˜50(é™)\n",
      "åˆä½µ #6: 3 æœ¬æ›¸\n",
      "  [0] TAICCA_ID: E253-12553, Title: ç†Ÿå¥³è¨˜(50)\n",
      "  [1] TAICCA_ID: E253-20323, Title: ç†Ÿå¥³è¨˜(50)\n",
      "  [2] TAICCA_ID: E253-12562, Title: ç†Ÿå¥³è¨˜50(é™)\n",
      "  åˆä½µå¾Œ TAICCA_ID: E253-12553 / E253-20323 / E253-12562\n",
      "  åˆä½µå¾Œ ISBN: ï¼ˆç©ºç™½ï¼‰/ï¼ˆç©ºç™½ï¼‰ / ï¼ˆç©ºç™½ï¼‰\n",
      "  âœ… è™•ç†å®Œæˆ: 6 ç­† â†’ 2 ç­†\n",
      "  ğŸ“Š å…¶ä¸­ 2 çµ„åŒ…å«å¤šæœ¬é‡è¤‡æ›¸ç±\n",
      "cluster_2.csv è™•ç†çµæœ: 6 ç­† â†’ 2 ç­†, å¤šæ›¸çµ„: 2\n",
      "  ğŸ’¾ å·²å¯«å…¥ 2 ç­†è³‡æ–™åˆ° /Users/alioth1225/Documents/College/merge/clustering/output_data/ebook_output.csv (æ–°å»ºæª”æ¡ˆ)\n",
      "å·²å¯«å…¥ 2 ç­†è³‡æ–™åˆ° /Users/alioth1225/Documents/College/merge/clustering/output_data/ebook_output.csv\n",
      "\n",
      "ğŸ“‚ è™•ç†æª”æ¡ˆ: cluster_0.csv\n",
      "é–‹å§‹è™•ç†: /Users/alioth1225/Documents/College/merge/clustering/ebook_clustered_data/cluster_0.csv\n",
      "  - è®€å– 2 ç­†è³‡æ–™\n",
      "  - å»ºç«‹ processed_title_stripped æ¬„ä½ (å°å¯« + å»ç©ºç™½)...\n",
      "  - é–‹å§‹å…©å…©æ¯”è¼ƒ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  æ¯”è¼ƒæ›¸ç±: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 27.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - å®Œæˆ 1 æ¬¡æ¯”è¼ƒ\n",
      "  - è­˜åˆ¥å‡º 2 å€‹ç¨ç«‹æ›¸ç±ï¼ˆçµ„ï¼‰\n",
      "  âœ… è™•ç†å®Œæˆ: 2 ç­† â†’ 2 ç­†\n",
      "cluster_0.csv è™•ç†çµæœ: 2 ç­† â†’ 2 ç­†, å¤šæ›¸çµ„: 0\n",
      "  ğŸ’¾ å·²è¿½åŠ  2 ç­†è³‡æ–™åˆ° /Users/alioth1225/Documents/College/merge/clustering/output_data/ebook_output.csv\n",
      "å·²å¯«å…¥ 2 ç­†è³‡æ–™åˆ° /Users/alioth1225/Documents/College/merge/clustering/output_data/ebook_output.csv\n",
      "\n",
      "ğŸ“‚ è™•ç†æª”æ¡ˆ: cluster_1.csv\n",
      "é–‹å§‹è™•ç†: /Users/alioth1225/Documents/College/merge/clustering/ebook_clustered_data/cluster_1.csv\n",
      "  - è®€å– 2 ç­†è³‡æ–™\n",
      "  - å»ºç«‹ processed_title_stripped æ¬„ä½ (å°å¯« + å»ç©ºç™½)...\n",
      "  - é–‹å§‹å…©å…©æ¯”è¼ƒ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  æ¯”è¼ƒæ›¸ç±: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 33.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - å®Œæˆ 1 æ¬¡æ¯”è¼ƒ\n",
      "  - è­˜åˆ¥å‡º 2 å€‹ç¨ç«‹æ›¸ç±ï¼ˆçµ„ï¼‰\n",
      "  âœ… è™•ç†å®Œæˆ: 2 ç­† â†’ 2 ç­†\n",
      "cluster_1.csv è™•ç†çµæœ: 2 ç­† â†’ 2 ç­†, å¤šæ›¸çµ„: 0\n",
      "  ğŸ’¾ å·²è¿½åŠ  2 ç­†è³‡æ–™åˆ° /Users/alioth1225/Documents/College/merge/clustering/output_data/ebook_output.csv\n",
      "å·²å¯«å…¥ 2 ç­†è³‡æ–™åˆ° /Users/alioth1225/Documents/College/merge/clustering/output_data/ebook_output.csv\n",
      "\n",
      "âœ… ç¬¬äºŒéšæ®µå®Œæˆï¼å»é‡åˆä½µå·²å®Œæˆã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_original': 10, 'total_output': 6, 'merges': 6}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stage2_bert_deduplication()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "condaEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
