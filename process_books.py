#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ›¸ç±è™•ç†å®Œæ•´æµç¨‹
1. Embedding åˆ†ç¾¤
2. LLM å»é‡åˆä½µ
"""

import pandas as pd
import numpy as np
import os
import glob
import json
import time
import logging
import argparse
from datetime import datetime
from tqdm import tqdm
from openai import OpenAI
from sklearn.cluster import DBSCAN

# ==================== å…¨åŸŸè¨­å®š ====================
OPENAI_API_KEY = "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"
client = OpenAI(api_key=OPENAI_API_KEY)

# Embedding è¨­å®š
EMBEDDING_MODEL = "text-embedding-3-small"
EPS = 0.5           # DBSCAN é„°åŸŸåŠå¾‘
MIN_SAMPLES = 2     # DBSCAN æœ€å°æ¨£æœ¬æ•¸

# è¼¸å‡ºè¨­å®š
CLUSTERED_DATA_DIR = "clustered_data"
FINAL_OUTPUT_FILE = "final_merged_output.csv"

# Log æª”æ¡ˆï¼ˆä½¿ç”¨æ™‚é–“æˆ³è¨˜ï¼‰
LOG_FILE = f"process_books_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"

# å…¨åŸŸè¨ˆæ•¸å™¨
api_call_count = 0
merge_count = 0

# ==================== è¼”åŠ©å‡½æ•¸ ====================

def log_and_print(message, level='info'):
    """åŒæ™‚è¼¸å‡ºåˆ°çµ‚ç«¯å’Œ log æª”æ¡ˆ"""
    print(message)
    if level == 'info':
        logging.info(message)
    elif level == 'warning':
        logging.warning(message)
    elif level == 'error':
        logging.error(message)

# ==================== ç¬¬ä¸€éšæ®µï¼šEmbedding åˆ†ç¾¤ ====================

def get_embedding(text, model=EMBEDDING_MODEL):
    """å–å¾—æ–‡å­—çš„ embedding å‘é‡"""
    if pd.isna(text) or not str(text).strip():
        return None
    
    try:
        text = str(text).replace("\n", " ")
        response = client.embeddings.create(
            input=text,
            model=model
        )
        return response.data[0].embedding
    except Exception as e:
        logging.error(f"Embedding éŒ¯èª¤: {e}")
        return None

def stage1_embedding_clustering(input_file):
    """
    ç¬¬ä¸€éšæ®µï¼šè®€å–è³‡æ–™ã€è¨ˆç®— embeddingã€é€²è¡Œ DBSCAN åˆ†ç¾¤ã€æ‹†åˆ†å„²å­˜
    """
    log_and_print("\n" + "=" * 80)
    log_and_print("ğŸ“Š ç¬¬ä¸€éšæ®µï¼šEmbedding åˆ†ç¾¤")
    log_and_print("=" * 80)
    
    # æ­¥é©Ÿ 1: è®€å–è³‡æ–™
    log_and_print(f"\nğŸ“‚ è®€å–è³‡æ–™: {input_file}")
    df = pd.read_csv(input_file)
    log_and_print(f"âœ… è®€å–å®Œæˆï¼ç¸½å…± {len(df)} ç­†è³‡æ–™")
    logging.info(f"è®€å–æª”æ¡ˆ: {input_file}, ç­†æ•¸: {len(df)}")
    
    # æ­¥é©Ÿ 2: è¨ˆç®— Embedding
    log_and_print(f"\nğŸ”„ é–‹å§‹è¨ˆç®— embeddings...")
    log_and_print(f"ç¸½å…±éœ€è¦è™•ç† {len(df)} ç­†è³‡æ–™")
    
    embeddings = []
    for idx, row in tqdm(df.iterrows(), total=len(df), desc="ç”Ÿæˆ Embeddings"):
        embedding = get_embedding(row['title'])
        embeddings.append(embedding)
    
    df['embedding'] = embeddings
    
    # çµ±è¨ˆçµæœ
    valid_embeddings = df['embedding'].notna().sum()
    invalid_embeddings = df['embedding'].isna().sum()
    
    log_and_print(f"\nâœ… Embedding ç”Ÿæˆå®Œæˆï¼")
    log_and_print(f"  - æˆåŠŸ: {valid_embeddings} ç­†")
    log_and_print(f"  - å¤±æ•—: {invalid_embeddings} ç­†")
    logging.info(f"Embedding çµ±è¨ˆ: æˆåŠŸ {valid_embeddings} ç­†, å¤±æ•— {invalid_embeddings} ç­†")
    
    # å„²å­˜åŒ…å« embedding çš„è³‡æ–™
    embedding_file = 'data_with_embeddings.csv'
    df.to_csv(embedding_file, index=False, encoding='utf-8-sig')
    log_and_print(f"ğŸ’¾ å·²å„²å­˜åŒ…å« embeddings çš„è³‡æ–™è‡³: {embedding_file}")
    
    # æ­¥é©Ÿ 3: æº–å‚™åˆ†ç¾¤è³‡æ–™
    log_and_print(f"\nğŸ“Š æº–å‚™åˆ†ç¾¤è³‡æ–™...")
    df_valid = df[df['embedding'].notna()].copy()
    log_and_print(f"  - æœ‰æ•ˆè³‡æ–™: {len(df_valid)} ç­†")
    
    embeddings_array = np.array(df_valid['embedding'].tolist())
    log_and_print(f"  - Embedding çŸ©é™£å½¢ç‹€: {embeddings_array.shape}")
    
    # æ­¥é©Ÿ 4: åŸ·è¡Œ DBSCAN åˆ†ç¾¤
    log_and_print(f"\nğŸ¯ åŸ·è¡Œ DBSCAN åˆ†ç¾¤...")
    log_and_print(f"  - eps (é„°åŸŸåŠå¾‘): {EPS}")
    log_and_print(f"  - min_samples (æœ€å°æ¨£æœ¬æ•¸): {MIN_SAMPLES}")
    logging.info(f"DBSCAN åƒæ•¸: eps={EPS}, min_samples={MIN_SAMPLES}")
    
    dbscan = DBSCAN(eps=EPS, min_samples=MIN_SAMPLES, metric='cosine')
    cluster_labels = dbscan.fit_predict(embeddings_array)
    
    df_valid['cluster'] = cluster_labels
    
    # çµ±è¨ˆåˆ†ç¾¤çµæœ
    n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)
    n_noise = list(cluster_labels).count(-1)
    
    log_and_print(f"\nâœ… åˆ†ç¾¤å®Œæˆï¼")
    log_and_print(f"  - è­˜åˆ¥å‡ºçš„ç¾¤æ•¸: {n_clusters}")
    log_and_print(f"  - å™ªéŸ³é»: {n_noise} ç­†")
    logging.info(f"åˆ†ç¾¤çµæœ: {n_clusters} å€‹ç¾¤, {n_noise} å€‹å™ªéŸ³é»")
    
    # é¡¯ç¤ºå„ç¾¤çµ±è¨ˆ
    if n_clusters > 0:
        log_and_print(f"\nğŸ“Š å„ç¾¤çš„è³‡æ–™ç­†æ•¸ï¼š")
        for cluster_id in sorted(df_valid['cluster'].unique()):
            if cluster_id == -1:
                continue
            count = (df_valid['cluster'] == cluster_id).sum()
            percentage = (count / len(df_valid)) * 100
            log_and_print(f"  ç¾¤ {cluster_id}: {count:>5} ç­† ({percentage:>5.1f}%)")
        
        if n_noise > 0:
            percentage = (n_noise / len(df_valid)) * 100
            log_and_print(f"  å™ªéŸ³: {n_noise:>5} ç­† ({percentage:>5.1f}%)")
    
    # æ­¥é©Ÿ 5: æ‹†åˆ†ä¸¦å„²å­˜ CSV
    log_and_print(f"\nğŸ’¾ é–‹å§‹æ‹†åˆ†ä¸¦å„²å­˜ CSV æª”æ¡ˆ...")
    log_and_print(f"  - è¼¸å‡ºè³‡æ–™å¤¾: {CLUSTERED_DATA_DIR}")
    
    os.makedirs(CLUSTERED_DATA_DIR, exist_ok=True)
    
    df_to_save = df_valid.drop(columns=['embedding'])
    saved_files = []
    
    for cluster_id in sorted(df_to_save['cluster'].unique()):
        cluster_data = df_to_save[df_to_save['cluster'] == cluster_id]
        cluster_data_original = cluster_data.drop(columns=['cluster'])
        
        if cluster_id == -1:
            output_file = os.path.join(CLUSTERED_DATA_DIR, "cluster_noise.csv")
            label = "å™ªéŸ³é»"
        else:
            output_file = os.path.join(CLUSTERED_DATA_DIR, f"cluster_{cluster_id}.csv")
            label = f"ç¾¤ {cluster_id}"
        
        cluster_data_original.to_csv(output_file, index=False, encoding='utf-8-sig')
        saved_files.append(output_file)
        log_and_print(f"  âœ… {label}: {len(cluster_data)} ç­† â†’ {output_file}")
        logging.info(f"å„²å­˜åˆ†ç¾¤æª”æ¡ˆ: {output_file}, ç­†æ•¸: {len(cluster_data)}")
    
    # å„²å­˜å®Œæ•´è³‡æ–™
    full_output_file = os.path.join(CLUSTERED_DATA_DIR, "full_data_with_clusters.csv")
    df_to_save.to_csv(full_output_file, index=False, encoding='utf-8-sig')
    log_and_print(f"\n  ğŸ“Š å®Œæ•´è³‡æ–™ï¼ˆå«åˆ†ç¾¤æ¨™ç±¤ï¼‰: {full_output_file}")
    
    log_and_print("\nâœ… ç¬¬ä¸€éšæ®µå®Œæˆï¼åˆ†ç¾¤æª”æ¡ˆå·²å„²å­˜ã€‚")
    
    return {
        'cluster_files': saved_files,
        'total_records': len(df),
        'valid_records': len(df_valid),
        'n_clusters': n_clusters,
        'n_noise': n_noise
    }

# ==================== ç¬¬äºŒéšæ®µï¼šLLM å»é‡åˆä½µ ====================

def check_same_book_with_llm(title1, title2):
    """ä½¿ç”¨ OpenAI LLM åˆ¤æ–·å…©æœ¬æ›¸æ˜¯å¦ç›¸åŒ"""
    global api_call_count
    
    prompt = f"""è«‹åˆ¤æ–·ä»¥ä¸‹å…©æœ¬æ›¸çš„æ¨™é¡Œæ˜¯å¦æŒ‡å‘åŒä¸€æœ¬æ›¸ã€‚
è«‹åªå›ç­” "YES" æˆ– "NO"ï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—ã€‚

æ›¸ç±1: {title1}
æ›¸ç±2: {title2}

åˆ¤æ–·æ¨™æº–ï¼š
- æ¨™é¡Œå®Œå…¨ç›¸åŒæˆ–åªæœ‰ç´°å¾®å·®ç•°ï¼ˆå¦‚æ¨™é»ç¬¦è™Ÿã€ç©ºæ ¼ï¼‰â†’ YES
- åŒä¸€ç³»åˆ—ä½†ä¸åŒé›†æ•¸ â†’ NO
- å®Œå…¨ä¸åŒçš„æ›¸ â†’ NO

å›ç­” (YES/NO):"""

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€å€‹åœ–æ›¸é¤¨ç®¡ç†å°ˆå®¶ï¼Œå°ˆé–€åˆ¤æ–·æ›¸ç±æ˜¯å¦ç›¸åŒã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0,
            max_tokens=10
        )
        
        api_call_count += 1
        answer = response.choices[0].message.content.strip().upper()
        result = "YES" in answer
        
        logging.info(f"API å‘¼å« #{api_call_count}: æ¯”è¼ƒ '{title1[:50]}...' vs '{title2[:50]}...' â†’ {result}")
        
        return result
        
    except Exception as e:
        error_msg = f"  âš ï¸ LLM åˆ¤æ–·éŒ¯èª¤: {e}"
        log_and_print(error_msg, 'error')
        return False

def merge_two_books(book1, book2):
    """åˆä½µå…©æœ¬æ›¸çš„è³‡æ–™"""
    global merge_count
    merge_count += 1
    
    logging.info(f"åˆä½µ #{merge_count}:")
    logging.info(f"  è¢«åˆä½µè€… TAICCA_ID: {book1.get('NEW_TAICCA_ID', 'N/A')}, Title: {book1.get('title', 'N/A')[:50]}")
    logging.info(f"  åˆä½µè€… TAICCA_ID: {book2.get('NEW_TAICCA_ID', 'N/A')}, Title: {book2.get('title', 'N/A')[:50]}")
    
    merged = {}
    
    # TAICCA_ID ç³»åˆ—ï¼šä»¥æ–œç·šåˆ†éš”
    for col in ['NEW_TAICCA_ID', '1106äº¦å¼ID', 'TAICCA_ID']:
        val1 = str(book1.get(col, '')).strip()
        val2 = str(book2.get(col, '')).strip()
        if pd.notna(book1.get(col)) and pd.notna(book2.get(col)):
            if val1 and val2 and val1 != val2:
                merged[col] = f"{val1} / {val2}"
            elif val1:
                merged[col] = val1
            elif val2:
                merged[col] = val2
        elif pd.notna(book1.get(col)):
            merged[col] = val1
        elif pd.notna(book2.get(col)):
            merged[col] = val2
        else:
            merged[col] = ''
    
    # isbn ç³»åˆ—ï¼šç‰¹æ®Šè™•ç†
    for col in ['isbn', 'eisbn']:
        val1 = str(book1.get(col, '')).strip() if pd.notna(book1.get(col)) else ''
        val2 = str(book2.get(col, '')).strip() if pd.notna(book2.get(col)) else ''
        
        if val1 and val2 and val1 != val2:
            merged[col] = f"{val1} / {val2}"
        elif val1 and not val2:
            merged[col] = f"{val1} / ï¼ˆç©ºç™½ï¼‰"
        elif not val1 and val2:
            merged[col] = f"ï¼ˆç©ºç™½ï¼‰/ {val2}"
        elif val1:
            merged[col] = val1
        else:
            merged[col] = ''
    
    # ç›´æ¥å¡«è£œçš„æ¬„ä½
    fill_cols = [
        'bookscom_isbn', 'kobo_isbn', 'readmoo_isbn', 'bookscom_eisbn', 'kobo_eisbn', 'readmoo_eisbn',
        'production_id', 'bookscom_production_id', 'kobo_production_id', 'readmoo_production_id',
        'bookscom_title', 'kobo_title', 'readmoo_title',
        'bookscom_processed_title', 'kobo_processed_title', 'readmoo_processed_title',
        'bookscom_original_title', 'kobo_original_title', 'readmoo_original_title',
        'bookscom_author', 'kobo_author', 'readmoo_author',
        'bookscom_translator', 'kobo_translator', 'readmoo_translator',
        'bookscom_publisher', 'kobo_publisher', 'readmoo_publisher',
        'bookscom_publish_date', 'kobo_publish_date', 'readmoo_publish_date',
        'bookscom_original_price', 'kobo_original_price', 'readmoo_original_price',
        'bookscom_category', 'kobo_category', 'readmoo_category',
        'kobo_type_ebook', 'readmoo_type_ebook',
        'bookscom_url', 'kobo_url', 'readmoo_url'
    ]
    
    for col in fill_cols:
        if pd.notna(book1.get(col)) and str(book1.get(col)).strip():
            merged[col] = book1[col]
        elif pd.notna(book2.get(col)) and str(book2.get(col)).strip():
            merged[col] = book2[col]
        else:
            merged[col] = ''
    
    # ä¿ç•™è¢«åˆä½µè€…çš„å…§å®¹
    keep_from_book1 = [
        'title', 'å‚™è¨»', 'processed_title', 'original_title',
        'author', 'translator', 'publisher'
    ]
    
    for col in keep_from_book1:
        merged[col] = book1.get(col, '')
    
    merged['Clean_publisher'] = book1.get('Clean_publisher', '')
    merged['æœªç´å…¥æ›¸ç›®FIND'] = book1.get('æœªç´å…¥æ›¸ç›®FIND', '')
    
    # min_publish_dateï¼šæœ€æ—©æ—¥æœŸ
    dates = []
    for col in ['min_publish_date', 'bookscom_publish_date', 'kobo_publish_date', 'readmoo_publish_date']:
        for book in [book1, book2]:
            if pd.notna(book.get(col)) and str(book.get(col)).strip():
                try:
                    date_str = str(book[col]).strip()
                    if '/' in date_str:
                        date_obj = datetime.strptime(date_str, '%Y/%m/%d')
                    elif '-' in date_str:
                        date_obj = datetime.strptime(date_str, '%Y-%m-%d')
                    else:
                        continue
                    dates.append(date_obj)
                except:
                    pass
    
    if dates:
        merged['min_publish_date'] = min(dates).strftime('%Y-%m-%d')
        merged['max_publish_date'] = max(dates).strftime('%Y-%m-%d')
    else:
        merged['min_publish_date'] = book1.get('min_publish_date', '')
        merged['max_publish_date'] = book1.get('max_publish_date', '')
    
    # priceï¼šæœ€å¤§å€¼
    prices = []
    for col in ['price', 'bookscom_original_price', 'kobo_original_price', 'readmoo_original_price']:
        for book in [book1, book2]:
            if pd.notna(book.get(col)):
                try:
                    price = float(book[col])
                    prices.append(price)
                except:
                    pass
    
    if prices:
        merged['price'] = max(prices)
    else:
        merged['price'] = book1.get('price', '')
    
    logging.info(f"  åˆä½µå¾Œ TAICCA_ID: {merged.get('NEW_TAICCA_ID', 'N/A')}")
    logging.info(f"  åˆä½µå¾Œ ISBN: {merged.get('isbn', 'N/A')}")
    
    return merged

def process_cluster_file(csv_file):
    """è™•ç†å–®å€‹åˆ†ç¾¤æª”æ¡ˆ"""
    filename = os.path.basename(csv_file)
    log_and_print(f"\nğŸ“‚ è™•ç†æª”æ¡ˆ: {filename}")
    logging.info(f"é–‹å§‹è™•ç†: {csv_file}")
    
    df = pd.read_csv(csv_file)
    log_and_print(f"  - è®€å– {len(df)} ç­†è³‡æ–™")
    
    if len(df) == 0:
        logging.warning(f"{filename} æ²’æœ‰è³‡æ–™")
        return []
    
    books = df.to_dict('records')
    merged_indices = set()
    result_books = []
    
    for i in tqdm(range(len(books)), desc="  æ¯”è¼ƒæ›¸ç±"):
        if i in merged_indices:
            continue
        
        current_book = books[i]
        found_match = False
        
        for j in range(i + 1, len(books)):
            if j in merged_indices:
                continue
            
            compare_book = books[j]
            
            title1 = str(current_book.get('title', '')).strip()
            title2 = str(compare_book.get('title', '')).strip()
            
            if not title1 or not title2:
                continue
            
            is_same = check_same_book_with_llm(title1, title2)
            
            if is_same:
                log_and_print(f"    âœ… æ‰¾åˆ°ç›¸åŒæ›¸ç±:")
                log_and_print(f"       [{i}] {title1}")
                log_and_print(f"       [{j}] {title2}")
                
                merged_book = merge_two_books(compare_book, current_book)
                result_books.append(merged_book)
                
                merged_indices.add(i)
                merged_indices.add(j)
                found_match = True
                break
        
        if not found_match:
            result_books.append(current_book)
    
    log_and_print(f"  âœ… è™•ç†å®Œæˆ: {len(result_books)} ç­†è³‡æ–™")
    logging.info(f"{filename} è™•ç†çµæœ: {len(df)} ç­† â†’ {len(result_books)} ç­†")
    
    return result_books

def stage2_llm_deduplication():
    """
    ç¬¬äºŒéšæ®µï¼šè®€å–åˆ†ç¾¤æª”æ¡ˆã€ä½¿ç”¨ LLM åˆ¤æ–·ä¸¦åˆä½µ
    """
    log_and_print("\n" + "=" * 80)
    log_and_print("ğŸ¤– ç¬¬äºŒéšæ®µï¼šLLM å»é‡åˆä½µ")
    log_and_print("=" * 80)
    
    # è®€å–æ‰€æœ‰åˆ†ç¾¤æª”æ¡ˆ
    cluster_files = glob.glob(os.path.join(CLUSTERED_DATA_DIR, "cluster_*.csv"))
    cluster_files = [f for f in cluster_files if 'full_data' not in f]
    
    log_and_print(f"\næ‰¾åˆ° {len(cluster_files)} å€‹åˆ†ç¾¤æª”æ¡ˆ:")
    for f in cluster_files:
        log_and_print(f"  - {os.path.basename(f)}")
    
    if not cluster_files:
        log_and_print("\nâš ï¸ æ²’æœ‰æ‰¾åˆ°ä»»ä½•åˆ†ç¾¤æª”æ¡ˆ", 'warning')
        return None
    
    # å–å¾—åŸå§‹æ¬„ä½é †åº
    original_columns = pd.read_csv(cluster_files[0]).columns.tolist()
    
    total_original = 0
    total_output = 0
    
    # è™•ç†æ¯å€‹åˆ†ç¾¤æª”æ¡ˆä¸¦å³æ™‚å¯«å…¥
    for idx, cluster_file in enumerate(cluster_files):
        original_count = len(pd.read_csv(cluster_file))
        total_original += original_count
        
        is_noise_file = 'noise' in os.path.basename(cluster_file).lower()
        
        if is_noise_file:
            # å™ªéŸ³æª”æ¡ˆç›´æ¥å¯«å…¥
            filename = os.path.basename(cluster_file)
            log_and_print(f"\nğŸ“‚ è™•ç†æª”æ¡ˆ: {filename}")
            logging.info(f"é–‹å§‹è™•ç†å™ªéŸ³æª”æ¡ˆ: {cluster_file}")
            
            df = pd.read_csv(cluster_file)
            log_and_print(f"  - è®€å– {len(df)} ç­†è³‡æ–™")
            log_and_print(f"  âš¡ å™ªéŸ³æª”æ¡ˆï¼Œç›´æ¥å¯«å…¥ï¼ˆè·³éæ¯”è¼ƒï¼‰")
            
            if len(df) > 0:
                df = df[[col for col in original_columns if col in df.columns]]
                
                if idx == 0:
                    df.to_csv(FINAL_OUTPUT_FILE, index=False, encoding='utf-8-sig', mode='w')
                    log_and_print(f"  ğŸ’¾ å·²å¯«å…¥ {len(df)} ç­†è³‡æ–™åˆ° {FINAL_OUTPUT_FILE} (æ–°å»ºæª”æ¡ˆ)")
                else:
                    df.to_csv(FINAL_OUTPUT_FILE, index=False, encoding='utf-8-sig', mode='a', header=False)
                    log_and_print(f"  ğŸ’¾ å·²è¿½åŠ  {len(df)} ç­†è³‡æ–™åˆ° {FINAL_OUTPUT_FILE}")
                
                logging.info(f"{filename}: ç›´æ¥å¯«å…¥ {len(df)} ç­†è³‡æ–™ï¼ˆå™ªéŸ³æª”æ¡ˆï¼‰")
                total_output += len(df)
        else:
            # ä¸€èˆ¬åˆ†ç¾¤æª”æ¡ˆï¼šé€²è¡Œæ¯”è¼ƒ
            results = process_cluster_file(cluster_file)
            
            if results:
                result_df = pd.DataFrame(results)
                result_df = result_df[[col for col in original_columns if col in result_df.columns]]
                
                if idx == 0:
                    result_df.to_csv(FINAL_OUTPUT_FILE, index=False, encoding='utf-8-sig', mode='w')
                    log_and_print(f"  ğŸ’¾ å·²å¯«å…¥ {len(result_df)} ç­†è³‡æ–™åˆ° {FINAL_OUTPUT_FILE} (æ–°å»ºæª”æ¡ˆ)")
                else:
                    result_df.to_csv(FINAL_OUTPUT_FILE, index=False, encoding='utf-8-sig', mode='a', header=False)
                    log_and_print(f"  ğŸ’¾ å·²è¿½åŠ  {len(result_df)} ç­†è³‡æ–™åˆ° {FINAL_OUTPUT_FILE}")
                
                logging.info(f"å·²å¯«å…¥ {len(result_df)} ç­†è³‡æ–™åˆ° {FINAL_OUTPUT_FILE}")
                total_output += len(result_df)
    
    log_and_print("\nâœ… ç¬¬äºŒéšæ®µå®Œæˆï¼å»é‡åˆä½µå·²å®Œæˆã€‚")
    
    return {
        'total_original': total_original,
        'total_output': total_output,
        'api_calls': api_call_count,
        'merges': merge_count
    }

# ==================== ä¸»ç¨‹å¼ ====================

def main():
    global api_call_count, merge_count
    
    # è§£æå‘½ä»¤è¡Œåƒæ•¸
    parser = argparse.ArgumentParser(description='æ›¸ç±è™•ç†å®Œæ•´æµç¨‹ï¼šEmbedding åˆ†ç¾¤ + LLM å»é‡åˆä½µ')
    parser.add_argument('input_file', type=str, help='è¼¸å…¥çš„ CSV æª”æ¡ˆè·¯å¾‘')
    parser.add_argument('--eps', type=float, default=0.5, help='DBSCAN eps åƒæ•¸ (é è¨­: 0.5)')
    parser.add_argument('--min-samples', type=int, default=2, help='DBSCAN min_samples åƒæ•¸ (é è¨­: 2)')
    parser.add_argument('--skip-embedding', action='store_true', help='è·³é embedding éšæ®µï¼Œç›´æ¥é€²è¡Œ LLM å»é‡')
    
    args = parser.parse_args()
    
    # æ›´æ–°å…¨åŸŸåƒæ•¸
    global EPS, MIN_SAMPLES
    EPS = args.eps
    MIN_SAMPLES = args.min_samples
    
    # åˆå§‹åŒ– logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S',
        handlers=[
            logging.FileHandler(LOG_FILE, encoding='utf-8'),
        ]
    )
    
    # è¨˜éŒ„é–‹å§‹æ™‚é–“
    start_time = time.time()
    start_datetime = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    
    # è¼¸å‡ºæ¨™é¡Œ
    title = "=" * 80 + "\nğŸ“š æ›¸ç±è™•ç†å®Œæ•´æµç¨‹ç³»çµ±\n" + "=" * 80
    log_and_print(title)
    logging.info(f"é–‹å§‹æ™‚é–“: {start_datetime}")
    logging.info(f"Log æª”æ¡ˆ: {LOG_FILE}")
    logging.info(f"è¼¸å…¥æª”æ¡ˆ: {args.input_file}")
    logging.info(f"æœ€çµ‚è¼¸å‡ºæª”æ¡ˆ: {FINAL_OUTPUT_FILE}")
    logging.info(f"DBSCAN åƒæ•¸: eps={EPS}, min_samples={MIN_SAMPLES}")
    
    log_and_print(f"\nğŸ“‹ åŸ·è¡Œè¨­å®š:")
    log_and_print(f"  - è¼¸å…¥æª”æ¡ˆ: {args.input_file}")
    log_and_print(f"  - DBSCAN åƒæ•¸: eps={EPS}, min_samples={MIN_SAMPLES}")
    log_and_print(f"  - æœ€çµ‚è¼¸å‡º: {FINAL_OUTPUT_FILE}")
    log_and_print(f"  - Log æª”æ¡ˆ: {LOG_FILE}")
    
    # æª¢æŸ¥è¼¸å…¥æª”æ¡ˆ
    if not os.path.exists(args.input_file):
        log_and_print(f"\nâŒ éŒ¯èª¤: æ‰¾ä¸åˆ°è¼¸å…¥æª”æ¡ˆ '{args.input_file}'", 'error')
        return
    
    try:
        # ç¬¬ä¸€éšæ®µï¼šEmbedding åˆ†ç¾¤
        if not args.skip_embedding:
            stage1_result = stage1_embedding_clustering(args.input_file)
            log_and_print(f"\nğŸ“Š ç¬¬ä¸€éšæ®µçµ±è¨ˆ:")
            log_and_print(f"  - ç¸½è³‡æ–™ç­†æ•¸: {stage1_result['total_records']}")
            log_and_print(f"  - æœ‰æ•ˆè³‡æ–™ç­†æ•¸: {stage1_result['valid_records']}")
            log_and_print(f"  - è­˜åˆ¥å‡ºçš„ç¾¤æ•¸: {stage1_result['n_clusters']}")
            log_and_print(f"  - å™ªéŸ³é»: {stage1_result['n_noise']}")
            log_and_print(f"  - ç”Ÿæˆæª”æ¡ˆæ•¸: {len(stage1_result['cluster_files'])}")
        else:
            log_and_print("\nâš ï¸ è·³é embedding éšæ®µï¼Œä½¿ç”¨ç¾æœ‰åˆ†ç¾¤æª”æ¡ˆ")
        
        # ç¬¬äºŒéšæ®µï¼šLLM å»é‡åˆä½µ
        stage2_result = stage2_llm_deduplication()
        
        if stage2_result:
            log_and_print(f"\nğŸ“Š ç¬¬äºŒéšæ®µçµ±è¨ˆ:")
            log_and_print(f"  - åŸå§‹ç¸½ç­†æ•¸: {stage2_result['total_original']}")
            log_and_print(f"  - è¼¸å‡ºè³‡æ–™ç­†æ•¸: {stage2_result['total_output']}")
            log_and_print(f"  - åˆä½µæ¸›å°‘: {stage2_result['total_original'] - stage2_result['total_output']} ç­†")
            log_and_print(f"  - LLM API å‘¼å«æ¬¡æ•¸: {stage2_result['api_calls']}")
            log_and_print(f"  - å¯¦éš›åˆä½µæ¬¡æ•¸: {stage2_result['merges']}")
        
        # è¨ˆç®—ç¸½åŸ·è¡Œæ™‚é–“
        end_time = time.time()
        end_datetime = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        elapsed_time = end_time - start_time
        
        # æœ€çµ‚ç¸½çµ
        log_and_print("\n" + "=" * 80)
        log_and_print("ğŸ‰ å®Œæ•´æµç¨‹åŸ·è¡Œå®Œç•¢ï¼")
        log_and_print("=" * 80)
        log_and_print(f"  - ç¸½è™•ç†æ™‚é–“: {elapsed_time:.2f} ç§’ ({elapsed_time/60:.2f} åˆ†é˜)")
        log_and_print(f"  - æœ€çµ‚è¼¸å‡ºæª”æ¡ˆ: {FINAL_OUTPUT_FILE}")
        log_and_print(f"  - Log æª”æ¡ˆ: {LOG_FILE}")
        log_and_print(f"  - åˆ†ç¾¤æª”æ¡ˆè³‡æ–™å¤¾: {CLUSTERED_DATA_DIR}/")
        
        logging.info("=" * 80)
        logging.info(f"çµæŸæ™‚é–“: {end_datetime}")
        logging.info(f"ç¸½åŸ·è¡Œæ™‚é–“: {elapsed_time:.2f} ç§’")
        logging.info("=" * 80)
        
    except Exception as e:
        log_and_print(f"\nâŒ åŸ·è¡Œéç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}", 'error')
        logging.exception("åŸ·è¡ŒéŒ¯èª¤")
        raise

if __name__ == "__main__":
    main()

