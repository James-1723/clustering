#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ›¸ç±è™•ç†å®Œæ•´æµç¨‹
1. OpenAI Embedding åˆ†ç¾¤ï¼ˆDBSCANï¼‰
2. BERT å»é‡åˆä½µï¼ˆæ”¯æ´å¤šæœ¬æ›¸åˆä½µï¼‰
"""

import pandas as pd
import numpy as np
import os
import glob
import json
import time
import logging
import argparse
import re
from datetime import datetime
from tqdm import tqdm
from sklearn.cluster import DBSCAN
try:
    from sentence_transformers import SentenceTransformer, util
    BERT_AVAILABLE = True
except ImportError:
    print("è­¦å‘Š: sentence-transformers æœªå®‰è£ï¼Œå°‡ç„¡æ³•ä½¿ç”¨ BERT")
    BERT_AVAILABLE = False

try:
    import cn2an
except ImportError:
    print("è­¦å‘Š: cn2an æœªå®‰è£ï¼Œå°‡ç„¡æ³•é€²è¡Œæ•¸å­—æ¨™æº–åŒ–æ¯”è¼ƒ")
    cn2an = None

# ==================== å…¨åŸŸè¨­å®š ====================

# OpenAI è¨­å®š
OPENAI_API_KEY = "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"

# OpenAI Embedding è¨­å®šï¼ˆåƒ…ç”¨æ–¼ç¬¬ä¸€éšæ®µï¼‰
EMBEDDING_MODEL = "text-embedding-3-small"
EPS = 0.15           # DBSCAN é„°åŸŸåŠå¾‘
MIN_SAMPLES = 2     # DBSCAN æœ€å°æ¨£æœ¬æ•¸

# BERT è¨­å®š
BERT_MODEL = "paraphrase-multilingual-MiniLM-L12-v2"
SIMILARITY_THRESHOLD = 0.99  # BERT ç›¸ä¼¼åº¦é–¾å€¼

# è¼¸å‡ºè¨­å®š
CLUSTERED_DATA_DIR = "clustered_data"
FINAL_OUTPUT_FILE = "final_merged_output.csv"

# Log æª”æ¡ˆï¼ˆä½¿ç”¨æ™‚é–“æˆ³è¨˜ï¼‰
LOG_FILE = f"process_books_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"

# å…¨åŸŸè¨ˆæ•¸å™¨
merge_count = 0
bert_model = None  # BERT æ¨¡å‹å…¨åŸŸè®Šæ•¸
openai_client = None  # OpenAI å®¢æˆ¶ç«¯å…¨åŸŸè®Šæ•¸

# ==================== è¼”åŠ©å‡½æ•¸ ====================

def log_and_print(message, level='info'):
    """åŒæ™‚è¼¸å‡ºåˆ°çµ‚ç«¯å’Œ log æª”æ¡ˆ"""
    print(message)
    if level == 'info':
        logging.info(message)
    elif level == 'warning':
        logging.warning(message)
    elif level == 'error':
        logging.error(message)

# ==================== ç¬¬ä¸€éšæ®µï¼šEmbedding åˆ†ç¾¤ ====================

def get_embedding(text, model=EMBEDDING_MODEL):
    """å–å¾—æ–‡å­—çš„ embedding å‘é‡ï¼ˆä½¿ç”¨ OpenAI APIï¼‰"""
    global openai_client
    
    if pd.isna(text) or not str(text).strip():
        return None
    
    try:
        # åˆå§‹åŒ– OpenAI å®¢æˆ¶ç«¯ï¼ˆåªåˆå§‹åŒ–ä¸€æ¬¡ï¼‰
        if openai_client is None:
            from openai import OpenAI
            openai_client = OpenAI(api_key=OPENAI_API_KEY)
        
        text = str(text).replace("\n", " ")
        response = openai_client.embeddings.create(
            input=text,
            model=model
        )
        return response.data[0].embedding
    except Exception as e:
        logging.error(f"Embedding éŒ¯èª¤: {e}")
        return None

def stage1_embedding_clustering(input_file):
    """
    ç¬¬ä¸€éšæ®µï¼šè®€å–è³‡æ–™ã€è¨ˆç®— embeddingã€é€²è¡Œ DBSCAN åˆ†ç¾¤ã€æ‹†åˆ†å„²å­˜
    """
    log_and_print("\n" + "=" * 80)
    log_and_print("ğŸ“Š ç¬¬ä¸€éšæ®µï¼šEmbedding åˆ†ç¾¤")
    log_and_print("=" * 80)
    
    # æ­¥é©Ÿ 1: è®€å–è³‡æ–™
    log_and_print(f"\nğŸ“‚ è®€å–è³‡æ–™: {input_file}")
    df = pd.read_csv(input_file)
    log_and_print(f"âœ… è®€å–å®Œæˆï¼ç¸½å…± {len(df)} ç­†è³‡æ–™")
    logging.info(f"è®€å–æª”æ¡ˆ: {input_file}, ç­†æ•¸: {len(df)}")
    
    # æ­¥é©Ÿ 2: æ¸…ç† processed_titleï¼ˆç§»é™¤ã€Œé›»å­æ›¸ã€å­—æ¨£ï¼‰
    log_and_print(f"\nğŸ§¹ æ¸…ç†æ¨™é¡Œ...")
    def clean_ebook_text(title):
        """ç§»é™¤æ¨™é¡Œä¸­çš„ã€Œé›»å­æ›¸ã€ç›¸é—œå­—æ¨£"""
        if pd.isna(title) or not str(title).strip():
            return title
        
        title = str(title)
        patterns_to_remove = [
            r'\(é›»å­æ›¸\)',
            r'ï¼ˆé›»å­æ›¸ï¼‰',
            r'\[é›»å­æ›¸\]',
            r'ã€é›»å­æ›¸ã€‘',
            r'é›»å­æ›¸',
            r'\(ebook\)',
            r'ï¼ˆebookï¼‰',
            r'ebook',
            r'e-book',
            r'é™',
            r'é™åˆ¶ç´š',
            r'\(é™\)',
            r'ï¼ˆé™ï¼‰',
        ]
        
        for pattern in patterns_to_remove:
            title = re.sub(pattern, '', title, flags=re.IGNORECASE)
        
        # æ¸…ç†å¤šé¤˜ç©ºæ ¼
        title = ' '.join(title.split())
        return title.strip()
    
    df['processed_title_clean'] = df['processed_title'].apply(clean_ebook_text)
    log_and_print(f"âœ… æ¨™é¡Œæ¸…ç†å®Œæˆ")
    
    # æ­¥é©Ÿ 3: è¨ˆç®— Embedding
    log_and_print(f"\nğŸ”„ é–‹å§‹è¨ˆç®— embeddings...")
    log_and_print(f"ç¸½å…±éœ€è¦è™•ç† {len(df)} ç­†è³‡æ–™")
    
    embeddings = []
    for idx, row in tqdm(df.iterrows(), total=len(df), desc="ç”Ÿæˆ Embeddings"):
        # ä½¿ç”¨æ¸…ç†å¾Œçš„æ¨™é¡Œè¨ˆç®— embedding
        embedding = get_embedding(row['processed_title_clean'])
        embeddings.append(embedding)
    
    df['embedding'] = embeddings
    
    # çµ±è¨ˆçµæœ
    valid_embeddings = df['embedding'].notna().sum()
    invalid_embeddings = df['embedding'].isna().sum()
    
    log_and_print(f"\nâœ… Embedding ç”Ÿæˆå®Œæˆï¼")
    log_and_print(f"  - æˆåŠŸ: {valid_embeddings} ç­†")
    log_and_print(f"  - å¤±æ•—: {invalid_embeddings} ç­†")
    logging.info(f"Embedding çµ±è¨ˆ: æˆåŠŸ {valid_embeddings} ç­†, å¤±æ•— {invalid_embeddings} ç­†")
    
    # ç§»é™¤è‡¨æ™‚çš„æ¸…ç†æ¬„ä½
    df = df.drop(columns=['processed_title_clean'])
    
    # å„²å­˜åŒ…å« embedding çš„è³‡æ–™
    embedding_file = 'data_with_embeddings.csv'
    df.to_csv(embedding_file, index=False, encoding='utf-8-sig')
    log_and_print(f"ğŸ’¾ å·²å„²å­˜åŒ…å« embeddings çš„è³‡æ–™è‡³: {embedding_file}")
    
    # æ­¥é©Ÿ 4: æº–å‚™åˆ†ç¾¤è³‡æ–™
    log_and_print(f"\nğŸ“Š æº–å‚™åˆ†ç¾¤è³‡æ–™...")
    df_valid = df[df['embedding'].notna()].copy()
    log_and_print(f"  - æœ‰æ•ˆè³‡æ–™: {len(df_valid)} ç­†")
    
    embeddings_array = np.array(df_valid['embedding'].tolist())
    log_and_print(f"  - Embedding çŸ©é™£å½¢ç‹€: {embeddings_array.shape}")
    
    # æ­¥é©Ÿ 5: åŸ·è¡Œ DBSCAN åˆ†ç¾¤ï¼ˆä½¿ç”¨ recluster_only.py çš„é‚è¼¯ï¼‰
    log_and_print(f"\nğŸ¯ åŸ·è¡Œ DBSCAN åˆ†ç¾¤...")
    log_and_print(f"  - eps (é„°åŸŸåŠå¾‘): {EPS}")
    log_and_print(f"  - min_samples (æœ€å°æ¨£æœ¬æ•¸): {MIN_SAMPLES}")
    log_and_print(f"  - ä½¿ç”¨ cosine è·é›¢")
    logging.info(f"DBSCAN åƒæ•¸: eps={EPS}, min_samples={MIN_SAMPLES}, metric=cosine")
    
    try:
        dbscan = DBSCAN(eps=EPS, min_samples=MIN_SAMPLES, metric='cosine', n_jobs=-1)
        cluster_labels = dbscan.fit_predict(embeddings_array)
        df_valid['cluster'] = cluster_labels
        log_and_print(f"  âœ… DBSCAN åˆ†ç¾¤å®Œæˆ")
    except Exception as e:
        log_and_print(f"  âŒ DBSCAN åˆ†ç¾¤å¤±æ•—: {e}", 'error')
        raise
    
    # çµ±è¨ˆåˆ†ç¾¤çµæœ
    n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)
    n_noise = list(cluster_labels).count(-1)
    
    log_and_print(f"\nâœ… åˆ†ç¾¤å®Œæˆï¼")
    log_and_print(f"  - è­˜åˆ¥å‡ºçš„ç¾¤æ•¸: {n_clusters}")
    log_and_print(f"  - å™ªéŸ³é»: {n_noise} ç­†")
    logging.info(f"åˆ†ç¾¤çµæœ: {n_clusters} å€‹ç¾¤, {n_noise} å€‹å™ªéŸ³é»")
    
    # é¡¯ç¤ºå„ç¾¤çµ±è¨ˆ
    if n_clusters > 0:
        log_and_print(f"\nğŸ“Š å„ç¾¤çš„è³‡æ–™ç­†æ•¸ï¼š")
        for cluster_id in sorted(df_valid['cluster'].unique()):
            if cluster_id == -1:
                continue
            count = (df_valid['cluster'] == cluster_id).sum()
            percentage = (count / len(df_valid)) * 100
            log_and_print(f"  ç¾¤ {cluster_id}: {count:>5} ç­† ({percentage:>5.1f}%)")
        
        if n_noise > 0:
            percentage = (n_noise / len(df_valid)) * 100
            log_and_print(f"  å™ªéŸ³: {n_noise:>5} ç­† ({percentage:>5.1f}%)")
    
    # æ­¥é©Ÿ 6: æ‹†åˆ†ä¸¦å„²å­˜ CSV
    log_and_print(f"\nğŸ’¾ é–‹å§‹æ‹†åˆ†ä¸¦å„²å­˜ CSV æª”æ¡ˆ...")
    log_and_print(f"  - è¼¸å‡ºè³‡æ–™å¤¾: {CLUSTERED_DATA_DIR}")
    
    os.makedirs(CLUSTERED_DATA_DIR, exist_ok=True)
    
    df_to_save = df_valid.drop(columns=['embedding'])
    saved_files = []
    
    for cluster_id in sorted(df_to_save['cluster'].unique()):
        cluster_data = df_to_save[df_to_save['cluster'] == cluster_id]
        cluster_data_original = cluster_data.drop(columns=['cluster'])
        
        if cluster_id == -1:
            output_file = os.path.join(CLUSTERED_DATA_DIR, "cluster_noise.csv")
            label = "å™ªéŸ³é»"
        else:
            output_file = os.path.join(CLUSTERED_DATA_DIR, f"cluster_{cluster_id}.csv")
            label = f"ç¾¤ {cluster_id}"
        
        cluster_data_original.to_csv(output_file, index=False, encoding='utf-8-sig')
        saved_files.append(output_file)
        log_and_print(f"  âœ… {label}: {len(cluster_data)} ç­† â†’ {output_file}")
        logging.info(f"å„²å­˜åˆ†ç¾¤æª”æ¡ˆ: {output_file}, ç­†æ•¸: {len(cluster_data)}")
    
    # å„²å­˜å®Œæ•´è³‡æ–™
    full_output_file = os.path.join(CLUSTERED_DATA_DIR, "full_data_with_clusters.csv")
    df_to_save.to_csv(full_output_file, index=False, encoding='utf-8-sig')
    log_and_print(f"\n  ğŸ“Š å®Œæ•´è³‡æ–™ï¼ˆå«åˆ†ç¾¤æ¨™ç±¤ï¼‰: {full_output_file}")
    
    log_and_print("\nâœ… ç¬¬ä¸€éšæ®µå®Œæˆï¼åˆ†ç¾¤æª”æ¡ˆå·²å„²å­˜ã€‚")
    
    return {
        'cluster_files': saved_files,
        'total_records': len(df),
        'valid_records': len(df_valid),
        'n_clusters': n_clusters,
        'n_noise': n_noise
    }

# ==================== ç¬¬äºŒéšæ®µï¼šBERT å»é‡åˆä½µ ====================

class UnionFind:
    """ä¸¦æŸ¥é›†ï¼Œç”¨æ–¼ç®¡ç†æ›¸ç±åˆ†çµ„"""
    def __init__(self, n):
        self.parent = list(range(n))
        self.rank = [0] * n
    
    def find(self, x):
        if self.parent[x] != x:
            self.parent[x] = self.find(self.parent[x])
        return self.parent[x]
    
    def union(self, x, y):
        px, py = self.find(x), self.find(y)
        if px == py:
            return
        if self.rank[px] < self.rank[py]:
            px, py = py, px
        self.parent[py] = px
        if self.rank[px] == self.rank[py]:
            self.rank[px] += 1
    
    def get_groups(self):
        """å–å¾—æ‰€æœ‰åˆ†çµ„"""
        groups = {}
        for i in range(len(self.parent)):
            root = self.find(i)
            if root not in groups:
                groups[root] = []
            groups[root].append(i)
        return list(groups.values())

def normalize_numbers_in_title(title):
    """
    å°‡æ¨™é¡Œä¸­çš„æ•¸å­—çµ±ä¸€è½‰æ›ç‚ºé˜¿æ‹‰ä¼¯æ•¸å­—æ ¼å¼ï¼ˆç”¨æ–¼æ¯”è¼ƒï¼‰
    è™•ç†ï¼šä¸­æ–‡æ•¸å­—ï¼ˆä¸€äºŒä¸‰ï¼‰ã€é˜¿æ‹‰ä¼¯æ•¸å­—ï¼ˆ1 2 3ï¼‰ã€å…¨å½¢æ•¸å­—ï¼ˆï¼‘ï¼’ï¼“ï¼‰
    """
    if not cn2an or not title:
        return title
    
    normalized = title
    
    try:
        # 1. è½‰æ›å…¨å½¢æ•¸å­—ç‚ºåŠå½¢
        full_to_half = str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™', '0123456789')
        normalized = normalized.translate(full_to_half)
        
        # 2. æ‰¾å‡ºæ‰€æœ‰ä¸­æ–‡æ•¸å­—æ¨¡å¼ä¸¦è½‰æ›
        # åŒ¹é…ï¼šç¬¬ä¸€é›†ã€ç¬¬äºŒåä¸‰ç« ã€å·ä¸‰ã€Vol.äº”ã€ç­‰ç­‰
        chinese_num_pattern = r'[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒè¬é›¶å£¹è²³åƒè‚†ä¼é™¸æŸ’æŒç–æ‹¾ä½°ä»Ÿ]+'
        
        def replace_chinese_num(match):
            chinese_num = match.group(0)
            try:
                # ä½¿ç”¨ cn2an è½‰æ›ä¸­æ–‡æ•¸å­—ç‚ºé˜¿æ‹‰ä¼¯æ•¸å­—
                arabic_num = cn2an.cn2an(chinese_num, "smart")
                return str(arabic_num)
            except:
                return chinese_num  # è½‰æ›å¤±æ•—å‰‡ä¿æŒåŸæ¨£
        
        normalized = re.sub(chinese_num_pattern, replace_chinese_num, normalized)
        
    except Exception as e:
        logging.warning(f"æ•¸å­—æ¨™æº–åŒ–å¤±æ•—: {e}, æ¨™é¡Œ: {title[:50]}")
        return title
    
    return normalized

def clean_title_for_bert(title):
    """æ¸…ç†æ¨™é¡Œç”¨æ–¼ BERT æ¯”è¼ƒï¼ˆç§»é™¤é›»å­æ›¸ã€é™åˆ¶ç´šç­‰æ¨™è¨˜ï¼‰"""
    if pd.isna(title) or not str(title).strip():
        return ""
    
    title = str(title).strip()
    
    # ç§»é™¤å¸¸è¦‹çš„æ¨™è¨˜å’Œå¹²æ“¾å­—æ¨£
    patterns_to_remove = [
        # é›»å­æ›¸ç›¸é—œ
        r'\(é›»å­æ›¸\)',
        r'ï¼ˆé›»å­æ›¸ï¼‰',
        r'\[é›»å­æ›¸\]',
        r'ã€é›»å­æ›¸ã€‘',
        r'é›»å­æ›¸',
        r'\(ebook\)',
        r'ï¼ˆebookï¼‰',
        r'ebook',
        r'e-book',
        # é™åˆ¶ç´šç›¸é—œ
        r'\(é™\)',
        r'ï¼ˆé™ï¼‰',
        r'\[é™\]',
        r'ã€é™ã€‘',
        r'é™$',  # çµå°¾çš„ã€Œé™ã€
        r'é™åˆ¶ç´š',
        r'18\+',
        r'18ç¦',
        # å…¶ä»–å¸¸è¦‹å¹²æ“¾å­—æ¨£
        r'\(å®Œ\)',
        r'ï¼ˆå®Œï¼‰',
        r'\(æ–°ç‰ˆ\)',
        r'ï¼ˆæ–°ç‰ˆï¼‰',
        r'\(ä¿®è¨‚ç‰ˆ\)',
        r'ï¼ˆä¿®è¨‚ç‰ˆï¼‰',
        r'\(å…¨\)',
        r'ï¼ˆå…¨ï¼‰',
    ]
    
    for pattern in patterns_to_remove:
        title = re.sub(pattern, ' ', title, flags=re.IGNORECASE)
    
    # æ¸…ç†å¤šé¤˜ç©ºæ ¼
    title = ' '.join(title.split())
    return title.strip()

def check_same_book_with_bert(title1, title2):
    """ä½¿ç”¨ BERT åˆ¤æ–·å…©æœ¬æ›¸æ˜¯å¦ç›¸åŒï¼ˆæ¯”è¼ƒå‰å…ˆæ¨™æº–åŒ–æ•¸å­—ï¼‰"""
    global bert_model
    
    if not BERT_AVAILABLE or bert_model is None:
        logging.error("BERT æ¨¡å‹æœªè¼‰å…¥")
        return False
    
    if not title1 or not title2:
        return False
    
    # æ¸…ç†æ¨™é¡Œ
    cleaned_title1 = clean_title_for_bert(title1)
    cleaned_title2 = clean_title_for_bert(title2)
    
    if not cleaned_title1 or not cleaned_title2:
        return False
    
    # æ¨™æº–åŒ–æ•¸å­—å¾Œå†æ¯”è¼ƒ
    normalized_title1 = normalize_numbers_in_title(cleaned_title1)
    normalized_title2 = normalize_numbers_in_title(cleaned_title2)
    
    try:
        # è¨ˆç®— embeddings
        embedding1 = bert_model.encode(normalized_title1, convert_to_tensor=True)
        embedding2 = bert_model.encode(normalized_title2, convert_to_tensor=True)
        
        # è¨ˆç®— cosine ç›¸ä¼¼åº¦
        similarity = util.cos_sim(embedding1, embedding2).item()
        
        # åˆ¤æ–·æ˜¯å¦ç‚ºåŒä¸€æœ¬æ›¸
        is_same = similarity >= SIMILARITY_THRESHOLD
        
        if is_same:
            logging.info(f"BERT æ¯”è¼ƒ: '{title1[:50]}...' vs '{title2[:50]}...' â†’ ç›¸ä¼¼åº¦: {similarity:.4f} â†’ ç›¸åŒ")
            if normalized_title1 != cleaned_title1 or normalized_title2 != cleaned_title2:
                logging.info(f"  æ¨™æº–åŒ–å¾Œ: '{normalized_title1[:50]}...' vs '{normalized_title2[:50]}...'")
        
        return is_same
        
    except Exception as e:
        error_msg = f"  âš ï¸ BERT åˆ¤æ–·éŒ¯èª¤: {e}"
        logging.error(error_msg)
        return False

def merge_two_books(book1, book2):
    """åˆä½µå…©æœ¬æ›¸çš„è³‡æ–™ï¼ˆå…§éƒ¨ä½¿ç”¨ï¼Œä¸å¢åŠ è¨ˆæ•¸å™¨ï¼‰"""
    merged = {}
    
    # TAICCA_ID ç³»åˆ—ï¼šä»¥æ–œç·šåˆ†éš”
    for col in ['NEW_TAICCA_ID', '1106äº¦å¼ID', 'TAICCA_ID']:
        val1 = str(book1.get(col, '')).strip()
        val2 = str(book2.get(col, '')).strip()
        if pd.notna(book1.get(col)) and pd.notna(book2.get(col)):
            if val1 and val2 and val1 != val2:
                merged[col] = f"{val1} / {val2}"
            elif val1:
                merged[col] = val1
            elif val2:
                merged[col] = val2
        elif pd.notna(book1.get(col)):
            merged[col] = val1
        elif pd.notna(book2.get(col)):
            merged[col] = val2
        else:
            merged[col] = ''
    
    # isbn ç³»åˆ—ï¼šç‰¹æ®Šè™•ç†
    for col in ['isbn', 'eisbn', 'æœªç´å…¥æ›¸ç›®FIND']:
        val1 = str(book1.get(col, '')).strip() if pd.notna(book1.get(col)) else ''
        val2 = str(book2.get(col, '')).strip() if pd.notna(book2.get(col)) else ''
        
        if val1 and val2 and val1 != val2:
            merged[col] = f"{val1} / {val2}"
        elif val1 and not val2:
            merged[col] = f"{val1} / ï¼ˆç©ºç™½ï¼‰"
        elif not val1 and val2:
            merged[col] = f"ï¼ˆç©ºç™½ï¼‰/ {val2}"
        elif val1:
            merged[col] = val1
        else:
            merged[col] = ''
    
    # ç›´æ¥å¡«è£œçš„æ¬„ä½
    fill_cols = [
        'bookscom_isbn', 'kobo_isbn', 'readmoo_isbn', 'bookscom_eisbn', 'kobo_eisbn', 'readmoo_eisbn',
        'production_id', 'bookscom_production_id', 'kobo_production_id', 'readmoo_production_id',
        'bookscom_title', 'kobo_title', 'readmoo_title',
        'bookscom_processed_title', 'kobo_processed_title', 'readmoo_processed_title',
        'bookscom_original_title', 'kobo_original_title', 'readmoo_original_title',
        'bookscom_author', 'kobo_author', 'readmoo_author',
        'bookscom_translator', 'kobo_translator', 'readmoo_translator',
        'bookscom_publisher', 'kobo_publisher', 'readmoo_publisher',
        'bookscom_publish_date', 'kobo_publish_date', 'readmoo_publish_date',
        'bookscom_original_price', 'kobo_original_price', 'readmoo_original_price',
        'bookscom_category', 'kobo_category', 'readmoo_category',
        'kobo_type_ebook', 'readmoo_type_ebook',
        'bookscom_url', 'kobo_url', 'readmoo_url'
    ]
    
    for col in fill_cols:
        if pd.notna(book1.get(col)) and str(book1.get(col)).strip():
            merged[col] = book1[col]
        elif pd.notna(book2.get(col)) and str(book2.get(col)).strip():
            merged[col] = book2[col]
        else:
            merged[col] = ''
    
    # ä¿ç•™è¢«åˆä½µè€…çš„å…§å®¹
    keep_from_book1 = [
        'title', 'å‚™è¨»', 'processed_title', 'original_title',
        'author', 'translator', 'publisher'
    ]
    
    for col in keep_from_book1:
        merged[col] = book1.get(col, '')
    
    merged['Clean_publisher'] = book1.get('Clean_publisher', '')
    merged['æœªç´å…¥æ›¸ç›®FIND'] = book1.get('æœªç´å…¥æ›¸ç›®FIND', '')
    
    # min_publish_dateï¼šæœ€æ—©æ—¥æœŸ
    dates = []
    for col in ['min_publish_date', 'bookscom_publish_date', 'kobo_publish_date', 'readmoo_publish_date']:
        for book in [book1, book2]:
            if pd.notna(book.get(col)) and str(book.get(col)).strip():
                try:
                    date_str = str(book[col]).strip()
                    if '/' in date_str:
                        date_obj = datetime.strptime(date_str, '%Y/%m/%d')
                    elif '-' in date_str:
                        date_obj = datetime.strptime(date_str, '%Y-%m-%d')
                    else:
                        continue
                    dates.append(date_obj)
                except:
                    pass
    
    if dates:
        merged['min_publish_date'] = min(dates).strftime('%Y-%m-%d')
        merged['max_publish_date'] = max(dates).strftime('%Y-%m-%d')
    else:
        merged['min_publish_date'] = book1.get('min_publish_date', '')
        merged['max_publish_date'] = book1.get('max_publish_date', '')
    
    # priceï¼šæœ€å¤§å€¼
    prices = []
    for col in ['price', 'bookscom_original_price', 'kobo_original_price', 'readmoo_original_price']:
        for book in [book1, book2]:
            if pd.notna(book.get(col)):
                try:
                    price = float(book[col])
                    prices.append(price)
                except:
                    pass
    
    if prices:
        merged['price'] = max(prices)
    else:
        merged['price'] = book1.get('price', '')
    
    return merged

def merge_multiple_books(books):
    """åˆä½µå¤šæœ¬æ›¸çš„è³‡æ–™"""
    global merge_count
    
    if len(books) == 0:
        return None
    if len(books) == 1:
        return books[0]
    
    merge_count += 1
    
    # è¨˜éŒ„åˆä½µè³‡è¨Š
    logging.info(f"åˆä½µ #{merge_count}: {len(books)} æœ¬æ›¸")
    for i, book in enumerate(books):
        logging.info(f"  [{i}] TAICCA_ID: {book.get('NEW_TAICCA_ID', 'N/A')}, Title: {book.get('title', 'N/A')[:50]}")
    
    # ä»¥ç¬¬ä¸€æœ¬æ›¸ç‚ºåŸºç¤ï¼Œé€ä¸€åˆä½µå…¶ä»–æ›¸
    result = books[0]
    for i in range(1, len(books)):
        result = merge_two_books(result, books[i])
    
    logging.info(f"  åˆä½µå¾Œ TAICCA_ID: {result.get('NEW_TAICCA_ID', 'N/A')}")
    logging.info(f"  åˆä½µå¾Œ ISBN: {result.get('isbn', 'N/A')}")
    
    return result

def process_cluster_file(csv_file):
    """è™•ç†å–®å€‹åˆ†ç¾¤æª”æ¡ˆï¼ˆæ”¯æ´å¤šæœ¬æ›¸åˆä½µï¼‰"""
    filename = os.path.basename(csv_file)
    log_and_print(f"\nğŸ“‚ è™•ç†æª”æ¡ˆ: {filename}")
    logging.info(f"é–‹å§‹è™•ç†: {csv_file}")
    
    df = pd.read_csv(csv_file)
    log_and_print(f"  - è®€å– {len(df)} ç­†è³‡æ–™")
    
    if len(df) == 0:
        logging.warning(f"{filename} æ²’æœ‰è³‡æ–™")
        return []
    
    books = df.to_dict('records')
    n = len(books)
    
    # ä½¿ç”¨ä¸¦æŸ¥é›†ç®¡ç†æ›¸ç±åˆ†çµ„
    uf = UnionFind(n)
    
    log_and_print(f"  - é–‹å§‹å…©å…©æ¯”è¼ƒ...")
    comparison_count = 0
    total_comparisons = n * (n - 1) // 2
    
    # å…©å…©æ¯”è¼ƒæ‰€æœ‰æ›¸ç±
    for i in tqdm(range(n), desc="  æ¯”è¼ƒæ›¸ç±"):
        title1 = str(books[i].get('processed_title', '') or books[i].get('title', '')).strip()
        if not title1:
            continue
        
        for j in range(i + 1, n):
            title2 = str(books[j].get('processed_title', '') or books[j].get('title', '')).strip()
            if not title2:
                continue
            
            comparison_count += 1
            
            # ä½¿ç”¨ BERT åˆ¤æ–·æ˜¯å¦ç‚ºåŒä¸€æœ¬æ›¸
            is_same = check_same_book_with_bert(title1, title2)
            
            if is_same:
                log_and_print(f"    âœ… æ‰¾åˆ°ç›¸åŒæ›¸ç±:")
                log_and_print(f"       [{i}] {title1[:60]}")
                log_and_print(f"       [{j}] {title2[:60]}")
                
                # å°‡å…©æœ¬æ›¸åŠ å…¥åŒä¸€çµ„
                uf.union(i, j)
    
    log_and_print(f"  - å®Œæˆ {comparison_count} æ¬¡æ¯”è¼ƒ")
    
    # å–å¾—æ‰€æœ‰åˆ†çµ„
    groups = uf.get_groups()
    log_and_print(f"  - è­˜åˆ¥å‡º {len(groups)} å€‹ç¨ç«‹æ›¸ç±ï¼ˆçµ„ï¼‰")
    
    # å°æ¯ä¸€çµ„é€²è¡Œåˆä½µ
    result_books = []
    multi_book_groups = 0
    
    for group_indices in groups:
        group_books = [books[i] for i in group_indices]
        
        if len(group_books) > 1:
            multi_book_groups += 1
            log_and_print(f"    ğŸ“š åˆä½µ {len(group_books)} æœ¬ç›¸åŒçš„æ›¸:")
            for idx in group_indices:
                book_title = str(books[idx].get('title', ''))[:60]
                log_and_print(f"       - {book_title}")
            
            # åˆä½µå¤šæœ¬æ›¸
            merged_book = merge_multiple_books(group_books)
            result_books.append(merged_book)
        else:
            # å–®ç¨çš„æ›¸ç›´æ¥åŠ å…¥
            result_books.append(group_books[0])
    
    log_and_print(f"  âœ… è™•ç†å®Œæˆ: {len(df)} ç­† â†’ {len(result_books)} ç­†")
    if multi_book_groups > 0:
        log_and_print(f"  ğŸ“Š å…¶ä¸­ {multi_book_groups} çµ„åŒ…å«å¤šæœ¬é‡è¤‡æ›¸ç±")
    
    logging.info(f"{filename} è™•ç†çµæœ: {len(df)} ç­† â†’ {len(result_books)} ç­†, å¤šæ›¸çµ„: {multi_book_groups}")
    
    return result_books

def stage2_bert_deduplication():
    """
    ç¬¬äºŒéšæ®µï¼šè®€å–åˆ†ç¾¤æª”æ¡ˆã€ä½¿ç”¨ BERT åˆ¤æ–·ä¸¦åˆä½µ
    """
    global bert_model
    
    log_and_print("\n" + "=" * 80)
    log_and_print("ğŸ¤– ç¬¬äºŒéšæ®µï¼šBERT å»é‡åˆä½µ")
    log_and_print("=" * 80)
    
    # è¼‰å…¥ BERT æ¨¡å‹
    if not BERT_AVAILABLE:
        log_and_print("âŒ sentence-transformers æœªå®‰è£ï¼Œç„¡æ³•ä½¿ç”¨ BERT", 'error')
        log_and_print("è«‹åŸ·è¡Œ: pip install sentence-transformers", 'error')
        return None
    
    log_and_print(f"\nè¼‰å…¥ BERT æ¨¡å‹: {BERT_MODEL}")
    try:
        bert_model = SentenceTransformer(BERT_MODEL)
        log_and_print(f"âœ… BERT æ¨¡å‹è¼‰å…¥å®Œæˆ")
        log_and_print(f"ç›¸ä¼¼åº¦é–¾å€¼: {SIMILARITY_THRESHOLD}")
    except Exception as e:
        log_and_print(f"âŒ BERT æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}", 'error')
        return None
    
    # è®€å–æ‰€æœ‰åˆ†ç¾¤æª”æ¡ˆ
    cluster_files = glob.glob(os.path.join(CLUSTERED_DATA_DIR, "cluster_*.csv"))
    cluster_files = [f for f in cluster_files if 'full_data' not in f]
    
    log_and_print(f"\næ‰¾åˆ° {len(cluster_files)} å€‹åˆ†ç¾¤æª”æ¡ˆ:")
    for f in cluster_files:
        log_and_print(f"  - {os.path.basename(f)}")
    
    if not cluster_files:
        log_and_print("\nâš ï¸ æ²’æœ‰æ‰¾åˆ°ä»»ä½•åˆ†ç¾¤æª”æ¡ˆ", 'warning')
        return None
    
    # å–å¾—åŸå§‹æ¬„ä½é †åº
    original_columns = pd.read_csv(cluster_files[0]).columns.tolist()
    
    total_original = 0
    total_output = 0
    
    # è™•ç†æ¯å€‹åˆ†ç¾¤æª”æ¡ˆä¸¦å³æ™‚å¯«å…¥
    for idx, cluster_file in enumerate(cluster_files):
        original_count = len(pd.read_csv(cluster_file))
        total_original += original_count
        
        is_noise_file = 'noise' in os.path.basename(cluster_file).lower()
        
        if is_noise_file:
            # å™ªéŸ³æª”æ¡ˆç›´æ¥å¯«å…¥
            filename = os.path.basename(cluster_file)
            log_and_print(f"\nğŸ“‚ è™•ç†æª”æ¡ˆ: {filename}")
            logging.info(f"é–‹å§‹è™•ç†å™ªéŸ³æª”æ¡ˆ: {cluster_file}")
            
            df = pd.read_csv(cluster_file)
            log_and_print(f"  - è®€å– {len(df)} ç­†è³‡æ–™")
            log_and_print(f"  âš¡ å™ªéŸ³æª”æ¡ˆï¼Œç›´æ¥å¯«å…¥ï¼ˆè·³éæ¯”è¼ƒï¼‰")
            
            if len(df) > 0:
                df = df[[col for col in original_columns if col in df.columns]]
                
                if idx == 0:
                    df.to_csv(FINAL_OUTPUT_FILE, index=False, encoding='utf-8-sig', mode='w')
                    log_and_print(f"  ğŸ’¾ å·²å¯«å…¥ {len(df)} ç­†è³‡æ–™åˆ° {FINAL_OUTPUT_FILE} (æ–°å»ºæª”æ¡ˆ)")
                else:
                    df.to_csv(FINAL_OUTPUT_FILE, index=False, encoding='utf-8-sig', mode='a', header=False)
                    log_and_print(f"  ğŸ’¾ å·²è¿½åŠ  {len(df)} ç­†è³‡æ–™åˆ° {FINAL_OUTPUT_FILE}")
                
                logging.info(f"{filename}: ç›´æ¥å¯«å…¥ {len(df)} ç­†è³‡æ–™ï¼ˆå™ªéŸ³æª”æ¡ˆï¼‰")
                total_output += len(df)
        else:
            # ä¸€èˆ¬åˆ†ç¾¤æª”æ¡ˆï¼šé€²è¡Œæ¯”è¼ƒ
            results = process_cluster_file(cluster_file)
            
            if results:
                result_df = pd.DataFrame(results)
                result_df = result_df[[col for col in original_columns if col in result_df.columns]]
                
                if idx == 0:
                    result_df.to_csv(FINAL_OUTPUT_FILE, index=False, encoding='utf-8-sig', mode='w')
                    log_and_print(f"  ğŸ’¾ å·²å¯«å…¥ {len(result_df)} ç­†è³‡æ–™åˆ° {FINAL_OUTPUT_FILE} (æ–°å»ºæª”æ¡ˆ)")
                else:
                    result_df.to_csv(FINAL_OUTPUT_FILE, index=False, encoding='utf-8-sig', mode='a', header=False)
                    log_and_print(f"  ğŸ’¾ å·²è¿½åŠ  {len(result_df)} ç­†è³‡æ–™åˆ° {FINAL_OUTPUT_FILE}")
                
                logging.info(f"å·²å¯«å…¥ {len(result_df)} ç­†è³‡æ–™åˆ° {FINAL_OUTPUT_FILE}")
                total_output += len(result_df)
    
    log_and_print("\nâœ… ç¬¬äºŒéšæ®µå®Œæˆï¼å»é‡åˆä½µå·²å®Œæˆã€‚")
    
    return {
        'total_original': total_original,
        'total_output': total_output,
        'merges': merge_count
    }

# ==================== ä¸»ç¨‹å¼ ====================

def main():
    global merge_count
    
    # è§£æå‘½ä»¤è¡Œåƒæ•¸
    parser = argparse.ArgumentParser(description='æ›¸ç±è™•ç†å®Œæ•´æµç¨‹ï¼šEmbedding åˆ†ç¾¤ + BERT å»é‡åˆä½µ')
    parser.add_argument('input_file', type=str, help='è¼¸å…¥çš„ CSV æª”æ¡ˆè·¯å¾‘')
    parser.add_argument('--eps', type=float, default=0.15, help='DBSCAN eps åƒæ•¸ (é è¨­: 0.15)')
    parser.add_argument('--min-samples', type=int, default=2, help='DBSCAN min_samples åƒæ•¸ (é è¨­: 2)')
    parser.add_argument('--similarity', type=float, default=0.99, help='BERT ç›¸ä¼¼åº¦é–¾å€¼ (é è¨­: 0.99)')
    parser.add_argument('--skip-embedding', action='store_true', help='è·³é embedding éšæ®µï¼Œç›´æ¥é€²è¡Œ BERT å»é‡')
    
    args = parser.parse_args()
    
    # æ›´æ–°å…¨åŸŸåƒæ•¸
    global EPS, MIN_SAMPLES, SIMILARITY_THRESHOLD
    EPS = args.eps
    MIN_SAMPLES = args.min_samples
    SIMILARITY_THRESHOLD = args.similarity
    
    # åˆå§‹åŒ– logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S',
        handlers=[
            logging.FileHandler(LOG_FILE, encoding='utf-8'),
        ]
    )
    
    # è¨˜éŒ„é–‹å§‹æ™‚é–“
    start_time = time.time()
    start_datetime = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    
    # è¼¸å‡ºæ¨™é¡Œ
    title = "=" * 80 + "\nğŸ“š æ›¸ç±è™•ç†å®Œæ•´æµç¨‹ç³»çµ±\n" + "=" * 80
    log_and_print(title)
    logging.info(f"é–‹å§‹æ™‚é–“: {start_datetime}")
    logging.info(f"Log æª”æ¡ˆ: {LOG_FILE}")
    logging.info(f"è¼¸å…¥æª”æ¡ˆ: {args.input_file}")
    logging.info(f"æœ€çµ‚è¼¸å‡ºæª”æ¡ˆ: {FINAL_OUTPUT_FILE}")
    logging.info(f"DBSCAN åƒæ•¸: eps={EPS}, min_samples={MIN_SAMPLES}")
    
    log_and_print(f"\nğŸ“‹ åŸ·è¡Œè¨­å®š:")
    log_and_print(f"  - è¼¸å…¥æª”æ¡ˆ: {args.input_file}")
    log_and_print(f"  - DBSCAN åƒæ•¸: eps={EPS}, min_samples={MIN_SAMPLES}")
    log_and_print(f"  - BERT ç›¸ä¼¼åº¦é–¾å€¼: {SIMILARITY_THRESHOLD}")
    log_and_print(f"  - æœ€çµ‚è¼¸å‡º: {FINAL_OUTPUT_FILE}")
    log_and_print(f"  - Log æª”æ¡ˆ: {LOG_FILE}")
    
    # æª¢æŸ¥è¼¸å…¥æª”æ¡ˆ
    if not os.path.exists(args.input_file):
        log_and_print(f"\nâŒ éŒ¯èª¤: æ‰¾ä¸åˆ°è¼¸å…¥æª”æ¡ˆ '{args.input_file}'", 'error')
        return
    
    try:
        # ç¬¬ä¸€éšæ®µï¼šEmbedding åˆ†ç¾¤
        if not args.skip_embedding:
            stage1_result = stage1_embedding_clustering(args.input_file)
            log_and_print(f"\nğŸ“Š ç¬¬ä¸€éšæ®µçµ±è¨ˆ:")
            log_and_print(f"  - ç¸½è³‡æ–™ç­†æ•¸: {stage1_result['total_records']}")
            log_and_print(f"  - æœ‰æ•ˆè³‡æ–™ç­†æ•¸: {stage1_result['valid_records']}")
            log_and_print(f"  - è­˜åˆ¥å‡ºçš„ç¾¤æ•¸: {stage1_result['n_clusters']}")
            log_and_print(f"  - å™ªéŸ³é»: {stage1_result['n_noise']}")
            log_and_print(f"  - ç”Ÿæˆæª”æ¡ˆæ•¸: {len(stage1_result['cluster_files'])}")
        else:
            log_and_print("\nâš ï¸ è·³é embedding éšæ®µï¼Œä½¿ç”¨ç¾æœ‰åˆ†ç¾¤æª”æ¡ˆ")
        
        # ç¬¬äºŒéšæ®µï¼šBERT å»é‡åˆä½µ
        stage2_result = stage2_bert_deduplication()
        
        if stage2_result:
            log_and_print(f"\nğŸ“Š ç¬¬äºŒéšæ®µçµ±è¨ˆ:")
            log_and_print(f"  - åŸå§‹ç¸½ç­†æ•¸: {stage2_result['total_original']}")
            log_and_print(f"  - è¼¸å‡ºè³‡æ–™ç­†æ•¸: {stage2_result['total_output']}")
            log_and_print(f"  - åˆä½µæ¸›å°‘: {stage2_result['total_original'] - stage2_result['total_output']} ç­†")
            log_and_print(f"  - å¯¦éš›åˆä½µæ¬¡æ•¸: {stage2_result['merges']}")
        
        # è¨ˆç®—ç¸½åŸ·è¡Œæ™‚é–“
        end_time = time.time()
        end_datetime = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        elapsed_time = end_time - start_time
        
        # æœ€çµ‚ç¸½çµ
        log_and_print("\n" + "=" * 80)
        log_and_print("ğŸ‰ å®Œæ•´æµç¨‹åŸ·è¡Œå®Œç•¢ï¼")
        log_and_print("=" * 80)
        log_and_print(f"  - ç¸½è™•ç†æ™‚é–“: {elapsed_time:.2f} ç§’ ({elapsed_time/60:.2f} åˆ†é˜)")
        log_and_print(f"  - æœ€çµ‚è¼¸å‡ºæª”æ¡ˆ: {FINAL_OUTPUT_FILE}")
        log_and_print(f"  - Log æª”æ¡ˆ: {LOG_FILE}")
        log_and_print(f"  - åˆ†ç¾¤æª”æ¡ˆè³‡æ–™å¤¾: {CLUSTERED_DATA_DIR}/")
        
        logging.info("=" * 80)
        logging.info(f"çµæŸæ™‚é–“: {end_datetime}")
        logging.info(f"ç¸½åŸ·è¡Œæ™‚é–“: {elapsed_time:.2f} ç§’")
        logging.info("=" * 80)
        
    except Exception as e:
        log_and_print(f"\nâŒ åŸ·è¡Œéç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}", 'error')
        logging.exception("åŸ·è¡ŒéŒ¯èª¤")
        raise

if __name__ == "__main__":
    main()

# python process_books.py input.csv