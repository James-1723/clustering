{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3866f6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e7324b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"input.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1e367cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ é–‹å§‹è™•ç†æ—¥æœŸ...\n",
      "\n",
      "âœ… æ—¥æœŸè™•ç†å®Œæˆï¼\n",
      "  - ç¸½ç­†æ•¸: 11269\n",
      "  - æœ‰æ—¥æœŸçš„ç­†æ•¸: 11267 (100.0%)\n",
      "  - å–®ä¸€æ—¥æœŸæº: 11053 ç­†\n",
      "  - å¤šå€‹æ—¥æœŸæº: 214 ç­†\n",
      "\n",
      "ğŸ“Š å‰ 10 ç­†ç¯„ä¾‹:\n",
      "  bookscom_publish_date    kobo_publish_date readmoo_publish_date  \\\n",
      "0                   NaN                  NaN  2025-09-20 00:00:00   \n",
      "1                   NaN                  NaN  2025-09-09 00:00:00   \n",
      "2                   NaN  2025-07-14 00:00:00  2025-07-15 00:00:00   \n",
      "3                   NaN  2025-07-14 00:00:00                  NaN   \n",
      "4                   NaN                  NaN  2025-09-19 00:00:00   \n",
      "5   2025-09-10 00:00:00                  NaN                  NaN   \n",
      "6                   NaN  2025-09-25 00:00:00                  NaN   \n",
      "7                   NaN  2025-07-15 00:00:00  2025-01-17 00:00:00   \n",
      "8                   NaN  2025-07-17 00:00:00                  NaN   \n",
      "9                   NaN  2025-09-03 00:00:00                  NaN   \n",
      "\n",
      "  min_publish_date max_publish_date  \n",
      "0       2025-09-20       2025-09-20  \n",
      "1       2025-09-09       2025-09-09  \n",
      "2       2025-07-14       2025-07-15  \n",
      "3       2025-07-14       2025-07-14  \n",
      "4       2025-09-19       2025-09-19  \n",
      "5       2025-09-10       2025-09-10  \n",
      "6       2025-09-25       2025-09-25  \n",
      "7       2025-01-17       2025-07-15  \n",
      "8       2025-07-17       2025-07-17  \n",
      "9       2025-09-03       2025-09-03  \n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def parse_date(date_str):\n",
    "    \"\"\"\n",
    "    è§£ææ—¥æœŸå­—ä¸²ï¼Œè¿”å›æ¨™æº–æ ¼å¼çš„æ—¥æœŸå­—ä¸²å’Œæ•¸å€¼\n",
    "    æ”¯æ´å„ç¨®æ—¥æœŸæ ¼å¼\n",
    "    \"\"\"\n",
    "    if pd.isna(date_str) or date_str == '' or str(date_str).strip() == '':\n",
    "        return None, None\n",
    "    \n",
    "    date_str = str(date_str).strip()\n",
    "    \n",
    "    # å˜—è©¦å„ç¨®æ—¥æœŸæ ¼å¼ï¼ˆåŒ…å«æ™‚é–“æˆ³è¨˜ï¼‰\n",
    "    date_formats = [\n",
    "        '%Y-%m-%d %H:%M:%S',  # 2025-09-20 00:00:00\n",
    "        '%Y/%m/%d %H:%M:%S',  # 2025/09/20 00:00:00\n",
    "        '%Y-%m-%d',           # 2025-09-20\n",
    "        '%Y/%m/%d',           # 2025/09/20\n",
    "        '%Y-%m',              # 2025-09\n",
    "        '%Y/%m',              # 2025/09\n",
    "        '%Y',                 # 2025\n",
    "    ]\n",
    "    \n",
    "    for fmt in date_formats:\n",
    "        try:\n",
    "            dt = datetime.strptime(date_str, fmt)\n",
    "            # è¿”å›æ¨™æº–æ ¼å¼çš„æ—¥æœŸå­—ä¸²å’Œç”¨æ–¼æ¯”è¼ƒçš„æ•¸å€¼\n",
    "            date_num = int(dt.strftime('%Y%m%d'))\n",
    "            date_str_formatted = dt.strftime('%Y-%m-%d')\n",
    "            return date_str_formatted, date_num\n",
    "        except ValueError:\n",
    "            continue\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "def find_min_max_dates(row):\n",
    "    \"\"\"\n",
    "    æ‰¾å‡ºæœ€æ—©å’Œæœ€æ™šçš„å‡ºç‰ˆæ—¥æœŸ\n",
    "    - å¦‚æœåªæœ‰ä¸€å€‹æ›¸å•†æœ‰æ—¥æœŸ â†’ min = max = è©²æ—¥æœŸ\n",
    "    - å¦‚æœæœ‰å¤šå€‹æ›¸å•†æœ‰æ—¥æœŸ â†’ æ¯”è¼ƒæ‰¾å‡º min å’Œ max\n",
    "    \"\"\"\n",
    "    dates_info = []\n",
    "    \n",
    "    # æ”¶é›†æ‰€æœ‰æœ‰æ•ˆçš„æ—¥æœŸ\n",
    "    for col in ['bookscom_publish_date', 'kobo_publish_date', 'readmoo_publish_date']:\n",
    "        date_str, date_num = parse_date(row[col])\n",
    "        if date_str is not None:\n",
    "            dates_info.append((date_str, date_num))\n",
    "    \n",
    "    # å¦‚æœæ²’æœ‰ä»»ä½•æœ‰æ•ˆæ—¥æœŸ\n",
    "    if not dates_info:\n",
    "        return None, None\n",
    "    \n",
    "    # å¦‚æœåªæœ‰ä¸€å€‹æ—¥æœŸæº\n",
    "    if len(dates_info) == 1:\n",
    "        date_str = dates_info[0][0]\n",
    "        return date_str, date_str  # min = max = è©²æ—¥æœŸ\n",
    "    \n",
    "    # å¦‚æœæœ‰å¤šå€‹æ—¥æœŸæºï¼Œé€²è¡Œæ¯”è¼ƒ\n",
    "    dates_info.sort(key=lambda x: x[1])  # æŒ‰æ•¸å€¼æ’åº\n",
    "    min_date = dates_info[0][0]   # æœ€æ—©çš„æ—¥æœŸ\n",
    "    max_date = dates_info[-1][0]  # æœ€æ™šçš„æ—¥æœŸ\n",
    "    \n",
    "    return min_date, max_date\n",
    "\n",
    "# æ‡‰ç”¨å‡½æ•¸æ‰¾å‡º min å’Œ max æ—¥æœŸ\n",
    "print(\"ğŸ”„ é–‹å§‹è™•ç†æ—¥æœŸ...\")\n",
    "df[['min_publish_date', 'max_publish_date']] = df.apply(\n",
    "    lambda row: pd.Series(find_min_max_dates(row)), axis=1\n",
    ")\n",
    "\n",
    "# çµ±è¨ˆçµæœ\n",
    "total_rows = len(df)\n",
    "rows_with_dates = df['min_publish_date'].notna().sum()\n",
    "rows_with_single_date = (df['min_publish_date'] == df['max_publish_date']).sum()\n",
    "rows_with_multiple_dates = rows_with_dates - rows_with_single_date\n",
    "\n",
    "print(f\"\\nâœ… æ—¥æœŸè™•ç†å®Œæˆï¼\")\n",
    "print(f\"  - ç¸½ç­†æ•¸: {total_rows}\")\n",
    "print(f\"  - æœ‰æ—¥æœŸçš„ç­†æ•¸: {rows_with_dates} ({rows_with_dates/total_rows*100:.1f}%)\")\n",
    "print(f\"  - å–®ä¸€æ—¥æœŸæº: {rows_with_single_date} ç­†\")\n",
    "print(f\"  - å¤šå€‹æ—¥æœŸæº: {rows_with_multiple_dates} ç­†\")\n",
    "\n",
    "print(\"\\nğŸ“Š å‰ 10 ç­†ç¯„ä¾‹:\")\n",
    "print(df[['bookscom_publish_date', 'kobo_publish_date', 'readmoo_publish_date', \n",
    "          'min_publish_date', 'max_publish_date']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b549ccaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df = pd.read_csv('/Users/alioth1225/Documents/College/merge/clustering/final_merged_bert_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "617fb79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª æ¸¬è©¦æ‹†åˆ† TAICCA_ID:\n",
      "  è¼¸å…¥: E253-00004 / E253-00003\n",
      "  è¼¸å‡º: ['E253-00004', 'E253-00003']\n",
      "\n",
      "ğŸ§ª æ¸¬è©¦æŸ¥è©¢ç¬¬ä¸€ç­†è³‡æ–™:\n",
      "  NEW_TAICCA_ID: E253-00004 / E253-00003\n",
      "  æŸ¥è©¢çµæœ:\n",
      "    min_publish_date: 2025-07-14\n",
      "    max_publish_date: 2025-07-15\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "def split_taicca_ids(taicca_id_str):\n",
    "    \"\"\"\n",
    "    æ‹†åˆ† TAICCA_IDï¼Œæ”¯æ´ã€Œ/ã€åˆ†éš”\n",
    "    ä¾‹å¦‚: \"E253-00004 / E253-00003\" -> [\"E253-00004\", \"E253-00003\"]\n",
    "    \"\"\"\n",
    "    if pd.isna(taicca_id_str) or str(taicca_id_str).strip() == '':\n",
    "        return []\n",
    "    \n",
    "    # ç”¨ã€Œ/ã€åˆ†éš”ï¼Œä¸¦å»é™¤å‰å¾Œç©ºç™½\n",
    "    ids = [id.strip() for id in str(taicca_id_str).split('/')]\n",
    "    # éæ¿¾æ‰ç©ºå­—ä¸²\n",
    "    ids = [id for id in ids if id]\n",
    "    return ids\n",
    "\n",
    "def parse_date_to_datetime(date_str):\n",
    "    \"\"\"\n",
    "    å°‡æ—¥æœŸå­—ä¸²è½‰æ›æˆ datetime ç‰©ä»¶ï¼Œæ–¹ä¾¿æ¯”è¼ƒ\n",
    "    \"\"\"\n",
    "    if pd.isna(date_str) or str(date_str).strip() == '':\n",
    "        return None\n",
    "    \n",
    "    date_str = str(date_str).strip()\n",
    "    \n",
    "    # å˜—è©¦å„ç¨®æ—¥æœŸæ ¼å¼\n",
    "    date_formats = [\n",
    "        '%Y-%m-%d %H:%M:%S',\n",
    "        '%Y/%m/%d %H:%M:%S',\n",
    "        '%Y-%m-%d',\n",
    "        '%Y/%m/%d',\n",
    "        '%Y-%m',\n",
    "        '%Y/%m',\n",
    "        '%Y',\n",
    "    ]\n",
    "    \n",
    "    for fmt in date_formats:\n",
    "        try:\n",
    "            return datetime.strptime(date_str, fmt)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    \n",
    "    return None\n",
    "\n",
    "def find_dates_for_merge_row(row, source_df):\n",
    "    \"\"\"\n",
    "    æ ¹æ“š merge_df çš„æŸä¸€åˆ—ï¼ŒæŸ¥è©¢ source_df (df) ä¸­çš„æ—¥æœŸè³‡æ–™\n",
    "    \n",
    "    è™•ç†æµç¨‹ï¼š\n",
    "    1. æ‹†åˆ† NEW_TAICCA_IDï¼ˆå¦‚æœæœ‰å¤šå€‹ç”¨ã€Œ/ã€åˆ†éš”ï¼‰\n",
    "    2. å°æ¯å€‹ ID å» source_df æŸ¥è©¢\n",
    "    3. æ”¶é›†æ‰€æœ‰æŸ¥åˆ°çš„ min_publish_date å’Œ max_publish_date\n",
    "    4. è¿”å›æ‰€æœ‰æ—¥æœŸä¸­æœ€æ—©å’Œæœ€æ™šçš„\n",
    "    \"\"\"\n",
    "    # æ‹†åˆ† TAICCA_ID\n",
    "    taicca_ids = split_taicca_ids(row['NEW_TAICCA_ID'])\n",
    "    \n",
    "    if not taicca_ids:\n",
    "        return None, None\n",
    "    \n",
    "    all_min_dates = []\n",
    "    all_max_dates = []\n",
    "    \n",
    "    # å°æ¯å€‹ ID æŸ¥è©¢\n",
    "    for taicca_id in taicca_ids:\n",
    "        # åœ¨ source_df ä¸­æŸ¥è©¢è©² ID\n",
    "        matched_rows = source_df[source_df['NEW_TAICCA_ID'] == taicca_id]\n",
    "        \n",
    "        # æ”¶é›†è©² ID çš„æ‰€æœ‰æ—¥æœŸ\n",
    "        for _, matched_row in matched_rows.iterrows():\n",
    "            min_date = parse_date_to_datetime(matched_row.get('min_publish_date'))\n",
    "            max_date = parse_date_to_datetime(matched_row.get('max_publish_date'))\n",
    "            \n",
    "            if min_date is not None:\n",
    "                all_min_dates.append(min_date)\n",
    "            if max_date is not None:\n",
    "                all_max_dates.append(max_date)\n",
    "    \n",
    "    # å¦‚æœæ²’æœ‰æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆæ—¥æœŸ\n",
    "    if not all_min_dates and not all_max_dates:\n",
    "        return None, None\n",
    "    \n",
    "    # æ‰¾å‡ºæœ€æ—©å’Œæœ€æ™šçš„æ—¥æœŸ\n",
    "    final_min = min(all_min_dates).strftime('%Y-%m-%d') if all_min_dates else None\n",
    "    final_max = max(all_max_dates).strftime('%Y-%m-%d') if all_max_dates else None\n",
    "    \n",
    "    return final_min, final_max\n",
    "\n",
    "# æ¸¬è©¦å‡½æ•¸\n",
    "print(\"ğŸ§ª æ¸¬è©¦æ‹†åˆ† TAICCA_ID:\")\n",
    "test_id = \"E253-00004 / E253-00003\"\n",
    "print(f\"  è¼¸å…¥: {test_id}\")\n",
    "print(f\"  è¼¸å‡º: {split_taicca_ids(test_id)}\")\n",
    "\n",
    "print(\"\\nğŸ§ª æ¸¬è©¦æŸ¥è©¢ç¬¬ä¸€ç­†è³‡æ–™:\")\n",
    "if len(merge_df) > 0:\n",
    "    first_row = merge_df.iloc[0]\n",
    "    print(f\"  NEW_TAICCA_ID: {first_row['NEW_TAICCA_ID']}\")\n",
    "    min_date, max_date = find_dates_for_merge_row(first_row, df)\n",
    "    print(f\"  æŸ¥è©¢çµæœ:\")\n",
    "    print(f\"    min_publish_date: {min_date}\")\n",
    "    print(f\"    max_publish_date: {max_date}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "53c8745a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ é–‹å§‹ç‚º merge_df å¡«å…¥æ—¥æœŸè³‡æ–™...\n",
      "ç¸½å…±éœ€è¦è™•ç† 9276 ç­†è³‡æ–™\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "è™•ç†ä¸­: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9276/9276 [00:04<00:00, 2312.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… æ—¥æœŸå¡«å…¥å®Œæˆï¼\n",
      "  - ç¸½ç­†æ•¸: 9276\n",
      "  - æˆåŠŸå¡«å…¥ min_publish_date: 9275 (100.0%)\n",
      "  - æˆåŠŸå¡«å…¥ max_publish_date: 9275 (100.0%)\n",
      "  - ç„¡æ³•å¡«å…¥: 1 (0.0%)\n",
      "\n",
      "ğŸ“‹ æŸ¥çœ‹å‰ 10 ç­†çµæœ:\n",
      "             NEW_TAICCA_ID min_publish_date max_publish_date\n",
      "0  E253-00004 / E253-00003       2025-07-14       2025-07-15\n",
      "1               E253-00010       2025-09-03       2025-09-03\n",
      "2               E253-00011       2025-09-03       2025-09-03\n",
      "3               E253-00050       2025-08-08       2025-08-18\n",
      "4               E253-05218       2025-08-08       2025-08-18\n",
      "5               E253-05219       2025-08-08       2025-08-18\n",
      "6               E253-00493       2025-09-12       2025-09-12\n",
      "7               E253-00494       2025-09-12       2025-09-12\n",
      "8               E253-00495       2025-09-12       2025-09-12\n",
      "9               E253-00496       2025-09-12       2025-09-12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "print(\"ğŸ”„ é–‹å§‹ç‚º merge_df å¡«å…¥æ—¥æœŸè³‡æ–™...\")\n",
    "print(f\"ç¸½å…±éœ€è¦è™•ç† {len(merge_df)} ç­†è³‡æ–™\\n\")\n",
    "\n",
    "# ä½¿ç”¨ tqdm é¡¯ç¤ºé€²åº¦æ¢\n",
    "min_dates = []\n",
    "max_dates = []\n",
    "\n",
    "for idx, row in tqdm(merge_df.iterrows(), total=len(merge_df), desc=\"è™•ç†ä¸­\"):\n",
    "    min_date, max_date = find_dates_for_merge_row(row, df)\n",
    "    min_dates.append(min_date)\n",
    "    max_dates.append(max_date)\n",
    "\n",
    "# å°‡çµæœå¡«å…¥ merge_df\n",
    "merge_df['min_publish_date'] = min_dates\n",
    "merge_df['max_publish_date'] = max_dates\n",
    "\n",
    "# çµ±è¨ˆçµæœ\n",
    "total = len(merge_df)\n",
    "filled_min = merge_df['min_publish_date'].notna().sum()\n",
    "filled_max = merge_df['max_publish_date'].notna().sum()\n",
    "not_filled = total - filled_min\n",
    "\n",
    "print(f\"\\nâœ… æ—¥æœŸå¡«å…¥å®Œæˆï¼\")\n",
    "print(f\"  - ç¸½ç­†æ•¸: {total}\")\n",
    "print(f\"  - æˆåŠŸå¡«å…¥ min_publish_date: {filled_min} ({filled_min/total*100:.1f}%)\")\n",
    "print(f\"  - æˆåŠŸå¡«å…¥ max_publish_date: {filled_max} ({filled_max/total*100:.1f}%)\")\n",
    "print(f\"  - ç„¡æ³•å¡«å…¥: {not_filled} ({not_filled/total*100:.1f}%)\")\n",
    "\n",
    "# é¡¯ç¤ºä¸€äº›ç¯„ä¾‹\n",
    "print(\"\\nğŸ“‹ æŸ¥çœ‹å‰ 10 ç­†çµæœ:\")\n",
    "print(merge_df[['NEW_TAICCA_ID', 'min_publish_date', 'max_publish_date']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "87b127c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ“Š è©³ç´°ç¯„ä¾‹åˆ†æ\n",
      "================================================================================\n",
      "\n",
      "âœ… æ‰¾åˆ° 1993 ç­†åŒ…å«å¤šå€‹ ID çš„è³‡æ–™\n",
      "\n",
      "ğŸ“‹ ç¯„ä¾‹ 1: å¤šå€‹ ID çš„è³‡æ–™\n",
      "--------------------------------------------------------------------------------\n",
      "NEW_TAICCA_ID: E253-00004 / E253-00003\n",
      "æ‹†åˆ†å¾Œçš„ ID: ['E253-00004', 'E253-00003']\n",
      "\n",
      "å„ ID åœ¨ df ä¸­çš„æ—¥æœŸè³‡æ–™:\n",
      "  - E253-00004:\n",
      "      min_publish_date: 2025-07-14\n",
      "      max_publish_date: 2025-07-14\n",
      "  - E253-00003:\n",
      "      min_publish_date: 2025-07-14\n",
      "      max_publish_date: 2025-07-15\n",
      "\n",
      "åˆä½µå¾Œçš„æ—¥æœŸ:\n",
      "  - min_publish_date: 2025-07-14\n",
      "  - max_publish_date: 2025-07-15\n",
      "\n",
      "\n",
      "ğŸ“‹ ç¯„ä¾‹ 2: å–®ä¸€ ID çš„è³‡æ–™\n",
      "--------------------------------------------------------------------------------\n",
      "NEW_TAICCA_ID: E253-00010\n",
      "\n",
      "åœ¨ df ä¸­æ‰¾åˆ° 1 ç­†è³‡æ–™:\n",
      "  - min_publish_date: 2025-09-03\n",
      "  - max_publish_date: 2025-09-03\n",
      "\n",
      "merge_df ä¸­çš„æ—¥æœŸ:\n",
      "  - min_publish_date: 2025-09-03\n",
      "  - max_publish_date: 2025-09-03\n",
      "\n",
      "================================================================================\n",
      "\n",
      "âœ… å·²å„²å­˜è™•ç†å¾Œçš„ merge_df è‡³: final_merged_with_dates.csv\n",
      "   ç¸½ç­†æ•¸: 9276\n",
      "   æ¬„ä½æ•¸: 59\n",
      "\n",
      "ğŸ“Š æœ€çµ‚çµ±è¨ˆ:\n",
      "  - æœ‰ min_publish_date çš„ç­†æ•¸: 9275\n",
      "  - æœ‰ max_publish_date çš„ç­†æ•¸: 9275\n",
      "  - ç¼ºå°‘æ—¥æœŸçš„ç­†æ•¸: 1\n",
      "\n",
      "âœ… æ²’æœ‰ç™¼ç¾ç•°å¸¸è³‡æ–™ï¼ˆæ‰€æœ‰ min_date <= max_dateï¼‰\n"
     ]
    }
   ],
   "source": [
    "# é¡¯ç¤ºè©³ç´°ç¯„ä¾‹\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ“Š è©³ç´°ç¯„ä¾‹åˆ†æ\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# æ‰¾å‡ºæœ‰å¤šå€‹ ID çš„è³‡æ–™ï¼ˆåŒ…å«ã€Œ/ã€ï¼‰\n",
    "multi_id_mask = merge_df['NEW_TAICCA_ID'].astype(str).str.contains('/', na=False)\n",
    "multi_id_rows = merge_df[multi_id_mask]\n",
    "\n",
    "if len(multi_id_rows) > 0:\n",
    "    print(f\"\\nâœ… æ‰¾åˆ° {len(multi_id_rows)} ç­†åŒ…å«å¤šå€‹ ID çš„è³‡æ–™\")\n",
    "    print(\"\\nğŸ“‹ ç¯„ä¾‹ 1: å¤šå€‹ ID çš„è³‡æ–™\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # é¡¯ç¤ºç¬¬ä¸€ç­†å¤š ID çš„è©³ç´°è³‡è¨Š\n",
    "    example = multi_id_rows.iloc[0]\n",
    "    print(f\"NEW_TAICCA_ID: {example['NEW_TAICCA_ID']}\")\n",
    "    \n",
    "    # æ‹†åˆ†ä¸¦æŸ¥è©¢æ¯å€‹ ID\n",
    "    ids = split_taicca_ids(example['NEW_TAICCA_ID'])\n",
    "    print(f\"æ‹†åˆ†å¾Œçš„ ID: {ids}\")\n",
    "    \n",
    "    print(\"\\nå„ ID åœ¨ df ä¸­çš„æ—¥æœŸè³‡æ–™:\")\n",
    "    for taicca_id in ids:\n",
    "        matched = df[df['NEW_TAICCA_ID'] == taicca_id]\n",
    "        if len(matched) > 0:\n",
    "            for _, row in matched.iterrows():\n",
    "                print(f\"  - {taicca_id}:\")\n",
    "                print(f\"      min_publish_date: {row.get('min_publish_date')}\")\n",
    "                print(f\"      max_publish_date: {row.get('max_publish_date')}\")\n",
    "    \n",
    "    print(f\"\\nåˆä½µå¾Œçš„æ—¥æœŸ:\")\n",
    "    print(f\"  - min_publish_date: {example['min_publish_date']}\")\n",
    "    print(f\"  - max_publish_date: {example['max_publish_date']}\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  æ²’æœ‰æ‰¾åˆ°åŒ…å«å¤šå€‹ ID çš„è³‡æ–™\")\n",
    "\n",
    "# æ‰¾å‡ºå–®ä¸€ ID çš„ç¯„ä¾‹\n",
    "single_id_mask = ~multi_id_mask & merge_df['NEW_TAICCA_ID'].notna()\n",
    "single_id_rows = merge_df[single_id_mask]\n",
    "\n",
    "if len(single_id_rows) > 0:\n",
    "    print(f\"\\n\\nğŸ“‹ ç¯„ä¾‹ 2: å–®ä¸€ ID çš„è³‡æ–™\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    example = single_id_rows.iloc[0]\n",
    "    print(f\"NEW_TAICCA_ID: {example['NEW_TAICCA_ID']}\")\n",
    "    \n",
    "    # æŸ¥è©¢é€™å€‹ ID åœ¨ df ä¸­çš„è³‡æ–™\n",
    "    matched = df[df['NEW_TAICCA_ID'] == example['NEW_TAICCA_ID']]\n",
    "    if len(matched) > 0:\n",
    "        print(f\"\\nåœ¨ df ä¸­æ‰¾åˆ° {len(matched)} ç­†è³‡æ–™:\")\n",
    "        for idx, row in matched.iterrows():\n",
    "            print(f\"  - min_publish_date: {row.get('min_publish_date')}\")\n",
    "            print(f\"  - max_publish_date: {row.get('max_publish_date')}\")\n",
    "    \n",
    "    print(f\"\\nmerge_df ä¸­çš„æ—¥æœŸ:\")\n",
    "    print(f\"  - min_publish_date: {example['min_publish_date']}\")\n",
    "    print(f\"  - max_publish_date: {example['max_publish_date']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# å„²å­˜çµæœ\n",
    "output_file = \"final_merged_with_dates.csv\"\n",
    "merge_df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "print(f\"\\nâœ… å·²å„²å­˜è™•ç†å¾Œçš„ merge_df è‡³: {output_file}\")\n",
    "print(f\"   ç¸½ç­†æ•¸: {len(merge_df)}\")\n",
    "print(f\"   æ¬„ä½æ•¸: {len(merge_df.columns)}\")\n",
    "\n",
    "# é¡¯ç¤ºçµ±è¨ˆæ‘˜è¦\n",
    "print(\"\\nğŸ“Š æœ€çµ‚çµ±è¨ˆ:\")\n",
    "print(f\"  - æœ‰ min_publish_date çš„ç­†æ•¸: {merge_df['min_publish_date'].notna().sum()}\")\n",
    "print(f\"  - æœ‰ max_publish_date çš„ç­†æ•¸: {merge_df['max_publish_date'].notna().sum()}\")\n",
    "print(f\"  - ç¼ºå°‘æ—¥æœŸçš„ç­†æ•¸: {merge_df['min_publish_date'].isna().sum()}\")\n",
    "\n",
    "# æª¢æŸ¥æ˜¯å¦æœ‰ min > max çš„ç•°å¸¸æƒ…æ³\n",
    "if (merge_df['min_publish_date'].notna() & merge_df['max_publish_date'].notna()).any():\n",
    "    merge_df['min_date_dt'] = pd.to_datetime(merge_df['min_publish_date'], errors='coerce')\n",
    "    merge_df['max_date_dt'] = pd.to_datetime(merge_df['max_publish_date'], errors='coerce')\n",
    "    \n",
    "    anomaly = merge_df[merge_df['min_date_dt'] > merge_df['max_date_dt']]\n",
    "    if len(anomaly) > 0:\n",
    "        print(f\"\\nâš ï¸  ç™¼ç¾ {len(anomaly)} ç­†ç•°å¸¸è³‡æ–™ï¼ˆmin_date > max_dateï¼‰\")\n",
    "    else:\n",
    "        print(f\"\\nâœ… æ²’æœ‰ç™¼ç¾ç•°å¸¸è³‡æ–™ï¼ˆæ‰€æœ‰ min_date <= max_dateï¼‰\")\n",
    "    \n",
    "    # æ¸…ç†è‡¨æ™‚æ¬„ä½\n",
    "    merge_df = merge_df.drop(['min_date_dt', 'max_date_dt'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0eed6c54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min_publish_date</th>\n",
       "      <th>max_publish_date</th>\n",
       "      <th>bookscom_publish_date</th>\n",
       "      <th>kobo_publish_date</th>\n",
       "      <th>readmoo_publish_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-07-14</td>\n",
       "      <td>2025-07-15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-07-14 00:00:00</td>\n",
       "      <td>2025-07-15 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-09-03</td>\n",
       "      <td>2025-09-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-09-03 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-09-03</td>\n",
       "      <td>2025-09-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-09-03 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-08-08</td>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-08-18 00:00:00</td>\n",
       "      <td>2025-08-08 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-08-08</td>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-08-18 00:00:00</td>\n",
       "      <td>2025-08-08 00:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  min_publish_date max_publish_date bookscom_publish_date  \\\n",
       "0       2025-07-14       2025-07-15                   NaN   \n",
       "1       2025-09-03       2025-09-03                   NaN   \n",
       "2       2025-09-03       2025-09-03                   NaN   \n",
       "3       2025-08-08       2025-08-18                   NaN   \n",
       "4       2025-08-08       2025-08-18                   NaN   \n",
       "\n",
       "     kobo_publish_date readmoo_publish_date  \n",
       "0  2025-07-14 00:00:00  2025-07-15 00:00:00  \n",
       "1  2025-09-03 00:00:00                  NaN  \n",
       "2                  NaN  2025-09-03 00:00:00  \n",
       "3  2025-08-18 00:00:00  2025-08-08 00:00:00  \n",
       "4  2025-08-18 00:00:00  2025-08-08 00:00:00  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_df[['min_publish_date', 'max_publish_date',\n",
    "       'bookscom_publish_date', 'kobo_publish_date', 'readmoo_publish_date']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cc475687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª æ¸¬è©¦åƒ¹æ ¼æŸ¥è©¢:\n",
      "  æ¸¬è©¦ NEW_TAICCA_ID: E253-00010\n",
      "\n",
      "  åœ¨ df ä¸­æ‰¾åˆ°çš„åƒ¹æ ¼:\n",
      "    - bookscom_original_price: nan\n",
      "    - kobo_original_price: 520\n",
      "    - readmoo_original_price: nan\n",
      "\n",
      "  è¨ˆç®—å‡ºçš„æœ€å¤§åƒ¹æ ¼: 520.0\n"
     ]
    }
   ],
   "source": [
    "def find_max_price_for_row(row, source_df):\n",
    "    \"\"\"\n",
    "    æ ¹æ“š merge_df çš„æŸä¸€åˆ—ï¼ŒæŸ¥è©¢ source_df (df) ä¸­çš„åƒ¹æ ¼è³‡æ–™\n",
    "    \n",
    "    è™•ç†æµç¨‹ï¼š\n",
    "    1. æ‹†åˆ† NEW_TAICCA_IDï¼ˆå¦‚æœæœ‰å¤šå€‹ç”¨ã€Œ/ã€åˆ†éš”ï¼‰\n",
    "    2. å°æ¯å€‹ ID å» source_df æŸ¥è©¢\n",
    "    3. æ”¶é›†æ‰€æœ‰æŸ¥åˆ°çš„ bookscom_original_price, kobo_original_price, readmoo_original_price\n",
    "    4. è¿”å›æ‰€æœ‰åƒ¹æ ¼ä¸­çš„æœ€å¤§å€¼\n",
    "    \"\"\"\n",
    "    # æ‹†åˆ† TAICCA_ID\n",
    "    taicca_ids = split_taicca_ids(row['NEW_TAICCA_ID'])\n",
    "    \n",
    "    if not taicca_ids:\n",
    "        return None\n",
    "    \n",
    "    all_prices = []\n",
    "    \n",
    "    # å°æ¯å€‹ ID æŸ¥è©¢\n",
    "    for taicca_id in taicca_ids:\n",
    "        # åœ¨ source_df ä¸­æŸ¥è©¢è©² ID\n",
    "        matched_rows = source_df[source_df['NEW_TAICCA_ID'] == taicca_id]\n",
    "        \n",
    "        # æ”¶é›†è©² ID çš„æ‰€æœ‰åƒ¹æ ¼\n",
    "        for _, matched_row in matched_rows.iterrows():\n",
    "            for price_col in ['bookscom_original_price', 'kobo_original_price', 'readmoo_original_price']:\n",
    "                price_val = matched_row.get(price_col)\n",
    "                \n",
    "                # å˜—è©¦è½‰æ›æˆæ•¸å€¼\n",
    "                if pd.notna(price_val):\n",
    "                    try:\n",
    "                        price = float(price_val)\n",
    "                        if price > 0:  # åªæ”¶é›†æ­£æ•¸åƒ¹æ ¼\n",
    "                            all_prices.append(price)\n",
    "                    except (ValueError, TypeError):\n",
    "                        pass\n",
    "    \n",
    "    # å¦‚æœæ²’æœ‰æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆåƒ¹æ ¼\n",
    "    if not all_prices:\n",
    "        return None\n",
    "    \n",
    "    # è¿”å›æœ€å¤§å€¼\n",
    "    return max(all_prices)\n",
    "\n",
    "# æ¸¬è©¦å‡½æ•¸\n",
    "print(\"ğŸ§ª æ¸¬è©¦åƒ¹æ ¼æŸ¥è©¢:\")\n",
    "if len(merge_df) > 0:\n",
    "    # æ‰¾ä¸€ç­†å–®ä¸€ ID çš„è³‡æ–™æ¸¬è©¦\n",
    "    single_id_mask = ~merge_df['NEW_TAICCA_ID'].astype(str).str.contains('/', na=False)\n",
    "    single_id_rows = merge_df[single_id_mask & merge_df['NEW_TAICCA_ID'].notna()]\n",
    "    \n",
    "    if len(single_id_rows) > 0:\n",
    "        test_row = single_id_rows.iloc[0]\n",
    "        print(f\"  æ¸¬è©¦ NEW_TAICCA_ID: {test_row['NEW_TAICCA_ID']}\")\n",
    "        \n",
    "        # æŸ¥è©¢ df ä¸­çš„åƒ¹æ ¼\n",
    "        matched = df[df['NEW_TAICCA_ID'] == test_row['NEW_TAICCA_ID']]\n",
    "        if len(matched) > 0:\n",
    "            print(f\"\\n  åœ¨ df ä¸­æ‰¾åˆ°çš„åƒ¹æ ¼:\")\n",
    "            for _, row in matched.iterrows():\n",
    "                print(f\"    - bookscom_original_price: {row.get('bookscom_original_price')}\")\n",
    "                print(f\"    - kobo_original_price: {row.get('kobo_original_price')}\")\n",
    "                print(f\"    - readmoo_original_price: {row.get('readmoo_original_price')}\")\n",
    "        \n",
    "        max_price = find_max_price_for_row(test_row, df)\n",
    "        print(f\"\\n  è¨ˆç®—å‡ºçš„æœ€å¤§åƒ¹æ ¼: {max_price}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7948041e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ é–‹å§‹ç‚ºå–®ä¸€ ID çš„è³‡æ–™å¡«å…¥åƒ¹æ ¼...\n",
      "æ‰¾åˆ° 7283 ç­†å–®ä¸€ ID çš„è³‡æ–™\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "è™•ç†åƒ¹æ ¼: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9276/9276 [00:02<00:00, 3719.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… åƒ¹æ ¼å¡«å…¥å®Œæˆï¼\n",
      "  - ç¸½ç­†æ•¸: 9276\n",
      "  - å–®ä¸€ ID ç­†æ•¸: 7283\n",
      "  - æˆåŠŸå¡«å…¥åƒ¹æ ¼: 7201 (98.9%)\n",
      "  - ç„¡æ³•å¡«å…¥: 82 (1.1%)\n",
      "\n",
      "ğŸ“‹ å–®ä¸€ ID è³‡æ–™çš„åƒ¹æ ¼ç¯„ä¾‹ï¼ˆå‰ 10 ç­†ï¼‰:\n",
      "   NEW_TAICCA_ID  price\n",
      "1     E253-00010  520.0\n",
      "2     E253-00011  520.0\n",
      "3     E253-00050  630.0\n",
      "4     E253-05218  315.0\n",
      "5     E253-05219  315.0\n",
      "6     E253-00493  700.0\n",
      "7     E253-00494  700.0\n",
      "8     E253-00495  700.0\n",
      "9     E253-00496  700.0\n",
      "10    E253-00497  700.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ”„ é–‹å§‹ç‚ºå–®ä¸€ ID çš„è³‡æ–™å¡«å…¥åƒ¹æ ¼...\")\n",
    "\n",
    "# æ‰¾å‡ºå–®ä¸€ ID çš„è³‡æ–™ï¼ˆä¸åŒ…å«ã€Œ/ã€ï¼‰\n",
    "single_id_mask = ~merge_df['NEW_TAICCA_ID'].astype(str).str.contains('/', na=False)\n",
    "single_id_rows = merge_df[single_id_mask & merge_df['NEW_TAICCA_ID'].notna()]\n",
    "\n",
    "print(f\"æ‰¾åˆ° {len(single_id_rows)} ç­†å–®ä¸€ ID çš„è³‡æ–™\\n\")\n",
    "\n",
    "# æº–å‚™åƒ¹æ ¼åˆ—è¡¨\n",
    "prices_to_fill = []\n",
    "\n",
    "# ä½¿ç”¨ tqdm é¡¯ç¤ºé€²åº¦\n",
    "for idx, row in tqdm(merge_df.iterrows(), total=len(merge_df), desc=\"è™•ç†åƒ¹æ ¼\"):\n",
    "    # æª¢æŸ¥æ˜¯å¦ç‚ºå–®ä¸€ ID\n",
    "    taicca_id = str(row['NEW_TAICCA_ID'])\n",
    "    \n",
    "    if pd.notna(row['NEW_TAICCA_ID']) and '/' not in taicca_id:\n",
    "        # å–®ä¸€ ID - æŸ¥è©¢ä¸¦å¡«å…¥åƒ¹æ ¼\n",
    "        max_price = find_max_price_for_row(row, df)\n",
    "        prices_to_fill.append(max_price)\n",
    "    else:\n",
    "        # å¤šå€‹ ID æˆ–ç©ºå€¼ - ä¿ç•™åŸæœ¬çš„ price\n",
    "        prices_to_fill.append(row.get('price'))\n",
    "\n",
    "# æ›´æ–° merge_df çš„ price æ¬„ä½\n",
    "merge_df['price'] = prices_to_fill\n",
    "\n",
    "# çµ±è¨ˆçµæœ\n",
    "total = len(merge_df)\n",
    "single_id_count = len(single_id_rows)\n",
    "filled_count = merge_df[single_id_mask]['price'].notna().sum()\n",
    "not_filled_count = single_id_count - filled_count\n",
    "\n",
    "print(f\"\\nâœ… åƒ¹æ ¼å¡«å…¥å®Œæˆï¼\")\n",
    "print(f\"  - ç¸½ç­†æ•¸: {total}\")\n",
    "print(f\"  - å–®ä¸€ ID ç­†æ•¸: {single_id_count}\")\n",
    "print(f\"  - æˆåŠŸå¡«å…¥åƒ¹æ ¼: {filled_count} ({filled_count/single_id_count*100:.1f}%)\")\n",
    "print(f\"  - ç„¡æ³•å¡«å…¥: {not_filled_count} ({not_filled_count/single_id_count*100:.1f}%)\")\n",
    "\n",
    "# é¡¯ç¤ºç¯„ä¾‹\n",
    "print(\"\\nğŸ“‹ å–®ä¸€ ID è³‡æ–™çš„åƒ¹æ ¼ç¯„ä¾‹ï¼ˆå‰ 10 ç­†ï¼‰:\")\n",
    "print(merge_df[single_id_mask][['NEW_TAICCA_ID', 'price']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b5b607a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ“Š åƒ¹æ ¼å¡«è£œè©³ç´°ç¯„ä¾‹\n",
      "================================================================================\n",
      "\n",
      "ğŸ“‹ ç¯„ä¾‹ï¼šå–®ä¸€ ID çš„åƒ¹æ ¼å¡«è£œ\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "NEW_TAICCA_ID: E253-00010\n",
      "  åœ¨ df ä¸­æ‰¾åˆ°çš„åƒ¹æ ¼:\n",
      "    - bookscom_original_price: N/A\n",
      "    - kobo_original_price: 520\n",
      "    - readmoo_original_price: N/A\n",
      "    â†’ æ‰€æœ‰åƒ¹æ ¼: [520.0]\n",
      "    â†’ æœ€å¤§å€¼: 520.0\n",
      "\n",
      "  å¡«å…¥ merge_df çš„åƒ¹æ ¼: 520.0\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "NEW_TAICCA_ID: E253-00011\n",
      "  åœ¨ df ä¸­æ‰¾åˆ°çš„åƒ¹æ ¼:\n",
      "    - bookscom_original_price: N/A\n",
      "    - kobo_original_price: N/A\n",
      "    - readmoo_original_price: 520.0\n",
      "    â†’ æ‰€æœ‰åƒ¹æ ¼: [520.0]\n",
      "    â†’ æœ€å¤§å€¼: 520.0\n",
      "\n",
      "  å¡«å…¥ merge_df çš„åƒ¹æ ¼: 520.0\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "NEW_TAICCA_ID: E253-00050\n",
      "  åœ¨ df ä¸­æ‰¾åˆ°çš„åƒ¹æ ¼:\n",
      "    - bookscom_original_price: N/A\n",
      "    - kobo_original_price: 630\n",
      "    - readmoo_original_price: 630.0\n",
      "    â†’ æ‰€æœ‰åƒ¹æ ¼: [630.0, 630.0]\n",
      "    â†’ æœ€å¤§å€¼: 630.0\n",
      "\n",
      "  å¡«å…¥ merge_df çš„åƒ¹æ ¼: 630.0\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š å¤šå€‹ ID çš„è³‡æ–™çµ±è¨ˆ:\n",
      "  - ç¸½ç­†æ•¸: 1993\n",
      "  - é€™äº›è³‡æ–™ä¿ç•™åŸæœ¬çš„ price æ¬„ä½ï¼ˆä¸å—æ­¤æ­¥é©Ÿå½±éŸ¿ï¼‰\n",
      "\n",
      "ğŸ“ˆ åƒ¹æ ¼çµ±è¨ˆ:\n",
      "  - å–®ä¸€ ID æœ‰åƒ¹æ ¼: 7201 ç­†\n",
      "  - å–®ä¸€ ID ç„¡åƒ¹æ ¼: 82 ç­†\n",
      "  - å¤šå€‹ ID æœ‰åƒ¹æ ¼: 1993 ç­†\n",
      "  - å¤šå€‹ ID ç„¡åƒ¹æ ¼: 0 ç­†\n",
      "\n",
      "ğŸ’° å–®ä¸€ ID åƒ¹æ ¼ç¯„åœ:\n",
      "  - æœ€å°å€¼: 10.00\n",
      "  - æœ€å¤§å€¼: 5460.00\n",
      "  - å¹³å‡å€¼: 234.43\n",
      "  - ä¸­ä½æ•¸: 210.00\n",
      "\n",
      "================================================================================\n",
      "âœ… å·²å„²å­˜æœ€çµ‚çµæœè‡³: final_merged_with_dates_and_prices.csv\n",
      "   ç¸½ç­†æ•¸: 9276\n",
      "   æ¬„ä½æ•¸: 59\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"ğŸ“Š åƒ¹æ ¼å¡«è£œè©³ç´°ç¯„ä¾‹\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# æ‰¾å‡ºå–®ä¸€ ID çš„è³‡æ–™\n",
    "single_id_mask = ~merge_df['NEW_TAICCA_ID'].astype(str).str.contains('/', na=False)\n",
    "single_id_rows = merge_df[single_id_mask & merge_df['NEW_TAICCA_ID'].notna()]\n",
    "\n",
    "# æ‰¾å¹¾ç­†æœ‰åƒ¹æ ¼çš„ç¯„ä¾‹\n",
    "filled_examples = single_id_rows[single_id_rows['price'].notna()].head(3)\n",
    "\n",
    "print(\"\\nğŸ“‹ ç¯„ä¾‹ï¼šå–®ä¸€ ID çš„åƒ¹æ ¼å¡«è£œ\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for idx, example in filled_examples.iterrows():\n",
    "    print(f\"\\nNEW_TAICCA_ID: {example['NEW_TAICCA_ID']}\")\n",
    "    \n",
    "    # æŸ¥è©¢ df ä¸­çš„åƒ¹æ ¼è³‡æ–™\n",
    "    matched = df[df['NEW_TAICCA_ID'] == example['NEW_TAICCA_ID']]\n",
    "    \n",
    "    if len(matched) > 0:\n",
    "        print(\"  åœ¨ df ä¸­æ‰¾åˆ°çš„åƒ¹æ ¼:\")\n",
    "        for _, row in matched.iterrows():\n",
    "            bookscom_price = row.get('bookscom_original_price')\n",
    "            kobo_price = row.get('kobo_original_price')\n",
    "            readmoo_price = row.get('readmoo_original_price')\n",
    "            \n",
    "            print(f\"    - bookscom_original_price: {bookscom_price if pd.notna(bookscom_price) else 'N/A'}\")\n",
    "            print(f\"    - kobo_original_price: {kobo_price if pd.notna(kobo_price) else 'N/A'}\")\n",
    "            print(f\"    - readmoo_original_price: {readmoo_price if pd.notna(readmoo_price) else 'N/A'}\")\n",
    "            \n",
    "            # æ”¶é›†æ‰€æœ‰æœ‰æ•ˆåƒ¹æ ¼\n",
    "            prices = []\n",
    "            for p in [bookscom_price, kobo_price, readmoo_price]:\n",
    "                if pd.notna(p):\n",
    "                    try:\n",
    "                        prices.append(float(p))\n",
    "                    except:\n",
    "                        pass\n",
    "            \n",
    "            if prices:\n",
    "                print(f\"    â†’ æ‰€æœ‰åƒ¹æ ¼: {prices}\")\n",
    "                print(f\"    â†’ æœ€å¤§å€¼: {max(prices)}\")\n",
    "    \n",
    "    print(f\"\\n  å¡«å…¥ merge_df çš„åƒ¹æ ¼: {example['price']}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# æ¯”è¼ƒå¤šå€‹ ID çš„è³‡æ–™ï¼ˆé€™äº›æ‡‰è©²ä¿ç•™åŸæœ¬çš„ priceï¼‰\n",
    "multi_id_mask = merge_df['NEW_TAICCA_ID'].astype(str).str.contains('/', na=False)\n",
    "multi_id_rows = merge_df[multi_id_mask]\n",
    "\n",
    "print(f\"\\nğŸ“Š å¤šå€‹ ID çš„è³‡æ–™çµ±è¨ˆ:\")\n",
    "print(f\"  - ç¸½ç­†æ•¸: {len(multi_id_rows)}\")\n",
    "print(f\"  - é€™äº›è³‡æ–™ä¿ç•™åŸæœ¬çš„ price æ¬„ä½ï¼ˆä¸å—æ­¤æ­¥é©Ÿå½±éŸ¿ï¼‰\")\n",
    "\n",
    "# é¡¯ç¤ºåƒ¹æ ¼åˆ†å¸ƒ\n",
    "print(f\"\\nğŸ“ˆ åƒ¹æ ¼çµ±è¨ˆ:\")\n",
    "print(f\"  - å–®ä¸€ ID æœ‰åƒ¹æ ¼: {single_id_rows['price'].notna().sum()} ç­†\")\n",
    "print(f\"  - å–®ä¸€ ID ç„¡åƒ¹æ ¼: {single_id_rows['price'].isna().sum()} ç­†\")\n",
    "print(f\"  - å¤šå€‹ ID æœ‰åƒ¹æ ¼: {multi_id_rows['price'].notna().sum()} ç­†\")\n",
    "print(f\"  - å¤šå€‹ ID ç„¡åƒ¹æ ¼: {multi_id_rows['price'].isna().sum()} ç­†\")\n",
    "\n",
    "if single_id_rows['price'].notna().any():\n",
    "    print(f\"\\nğŸ’° å–®ä¸€ ID åƒ¹æ ¼ç¯„åœ:\")\n",
    "    print(f\"  - æœ€å°å€¼: {single_id_rows['price'].min():.2f}\")\n",
    "    print(f\"  - æœ€å¤§å€¼: {single_id_rows['price'].max():.2f}\")\n",
    "    print(f\"  - å¹³å‡å€¼: {single_id_rows['price'].mean():.2f}\")\n",
    "    print(f\"  - ä¸­ä½æ•¸: {single_id_rows['price'].median():.2f}\")\n",
    "\n",
    "# å„²å­˜æœ€çµ‚çµæœ\n",
    "output_file = \"final_merged_with_dates_and_prices.csv\"\n",
    "merge_df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(f\"âœ… å·²å„²å­˜æœ€çµ‚çµæœè‡³: {output_file}\")\n",
    "print(f\"   ç¸½ç­†æ•¸: {len(merge_df)}\")\n",
    "print(f\"   æ¬„ä½æ•¸: {len(merge_df.columns)}\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7bec8d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª æ¸¬è©¦ FIND æ¬„ä½æŸ¥è©¢:\n",
      "\n",
      "ç¯„ä¾‹ 1: å¤šå€‹ ID\n",
      "  NEW_TAICCA_ID: E253-00004 / E253-00003\n",
      "  æ‹†åˆ†å¾Œ: ['E253-00004', 'E253-00003']\n",
      "\n",
      "  å„ ID åœ¨ df ä¸­çš„ FIND æ¬„ä½:\n",
      "    - E253-00004: ç„¡ FIND (å€¼: nan)\n",
      "    - E253-00003: ç„¡ FIND (å€¼: nan)\n",
      "\n",
      "  ç”Ÿæˆçš„çµæœ: None\n",
      "\n",
      "ç¯„ä¾‹ 2: å–®ä¸€ ID\n",
      "  NEW_TAICCA_ID: E253-00010\n",
      "  åœ¨ df ä¸­çš„ FIND: ç„¡ (å€¼: nan)\n",
      "  ç”Ÿæˆçš„çµæœ: None\n"
     ]
    }
   ],
   "source": [
    "def find_find_values_for_row(row, source_df):\n",
    "    \"\"\"\n",
    "    æ ¹æ“š merge_df çš„æŸä¸€åˆ—ï¼ŒæŸ¥è©¢ source_df (df) ä¸­çš„ FIND æ¬„ä½è³‡æ–™\n",
    "    \n",
    "    è™•ç†æµç¨‹ï¼š\n",
    "    1. æ‹†åˆ† NEW_TAICCA_IDï¼ˆå¦‚æœæœ‰å¤šå€‹ç”¨ã€Œ/ã€åˆ†éš”ï¼‰\n",
    "    2. å°æ¯å€‹ ID å» source_df æŸ¥è©¢\n",
    "    3. æª¢æŸ¥è©² ID çš„ã€Œæœªç´å…¥æ›¸ç›®FINDã€æ¬„ä½\n",
    "    4. å¦‚æœæœ‰å€¼ â†’ è¨˜éŒ„ç‚º \"FIND\"\n",
    "    5. å¦‚æœæ²’å€¼ â†’ è¨˜éŒ„ç‚º \"ç©ºç™½\"\n",
    "    6. ç”¨ã€Œ/ã€é€£æ¥æ‰€æœ‰çµæœ\n",
    "    \n",
    "    å›å‚³ç¯„ä¾‹ï¼š\n",
    "    - å–®ä¸€ ID æœ‰ FIND â†’ \"FIND\"\n",
    "    - å–®ä¸€ ID æ²’æœ‰ FIND â†’ Noneï¼ˆç•™ç©ºï¼‰\n",
    "    - å…©å€‹ ID éƒ½æœ‰ FIND â†’ \"FIND / FIND\"\n",
    "    - ç¬¬ä¸€å€‹æœ‰ï¼Œç¬¬äºŒå€‹æ²’æœ‰ â†’ \"FIND / ç©ºç™½\"\n",
    "    - å…©å€‹ ID éƒ½æ²’æœ‰ FIND â†’ Noneï¼ˆç•™ç©ºï¼‰\n",
    "    \"\"\"\n",
    "    # æ‹†åˆ† TAICCA_ID\n",
    "    taicca_ids = split_taicca_ids(row['NEW_TAICCA_ID'])\n",
    "    \n",
    "    if not taicca_ids:\n",
    "        return None\n",
    "    \n",
    "    find_results = []\n",
    "    \n",
    "    # å°æ¯å€‹ ID æŸ¥è©¢\n",
    "    for taicca_id in taicca_ids:\n",
    "        # åœ¨ source_df ä¸­æŸ¥è©¢è©² ID\n",
    "        matched_rows = source_df[source_df['NEW_TAICCA_ID'] == taicca_id]\n",
    "        \n",
    "        if len(matched_rows) > 0:\n",
    "            # å–ç¬¬ä¸€ç­†åŒ¹é…çš„è³‡æ–™\n",
    "            matched_row = matched_rows.iloc[0]\n",
    "            find_value = matched_row.get('æœªç´å…¥æ›¸ç›®FIND')\n",
    "            \n",
    "            # æª¢æŸ¥æ˜¯å¦æœ‰å€¼\n",
    "            if pd.notna(find_value) and str(find_value).strip() != '':\n",
    "                find_results.append(\"FIND\")\n",
    "            else:\n",
    "                find_results.append(\"ç©ºç™½\")\n",
    "        else:\n",
    "            # æŸ¥ä¸åˆ°è©² ID\n",
    "            find_results.append(\"ç©ºç™½\")\n",
    "    \n",
    "    # å¦‚æœæ‰€æœ‰çµæœéƒ½æ˜¯ã€Œç©ºç™½ã€ï¼Œè¿”å› Noneï¼ˆç•™ç©ºï¼‰\n",
    "    if all(result == \"ç©ºç™½\" for result in find_results):\n",
    "        return None\n",
    "    \n",
    "    # ç”¨ã€Œ/ã€é€£æ¥çµæœ\n",
    "    return \" / \".join(find_results)\n",
    "\n",
    "# æ¸¬è©¦å‡½æ•¸\n",
    "print(\"ğŸ§ª æ¸¬è©¦ FIND æ¬„ä½æŸ¥è©¢:\")\n",
    "\n",
    "# æ¸¬è©¦å¤šå€‹ ID çš„æƒ…æ³\n",
    "if len(merge_df) > 0:\n",
    "    # æ‰¾ä¸€ç­†å¤šå€‹ ID çš„è³‡æ–™æ¸¬è©¦\n",
    "    multi_id_mask = merge_df['NEW_TAICCA_ID'].astype(str).str.contains('/', na=False)\n",
    "    multi_id_rows = merge_df[multi_id_mask]\n",
    "    \n",
    "    if len(multi_id_rows) > 0:\n",
    "        test_row = multi_id_rows.iloc[0]\n",
    "        print(f\"\\nç¯„ä¾‹ 1: å¤šå€‹ ID\")\n",
    "        print(f\"  NEW_TAICCA_ID: {test_row['NEW_TAICCA_ID']}\")\n",
    "        \n",
    "        # æ‹†åˆ†ä¸¦æŸ¥è©¢æ¯å€‹ ID\n",
    "        ids = split_taicca_ids(test_row['NEW_TAICCA_ID'])\n",
    "        print(f\"  æ‹†åˆ†å¾Œ: {ids}\")\n",
    "        \n",
    "        print(f\"\\n  å„ ID åœ¨ df ä¸­çš„ FIND æ¬„ä½:\")\n",
    "        for taicca_id in ids:\n",
    "            matched = df[df['NEW_TAICCA_ID'] == taicca_id]\n",
    "            if len(matched) > 0:\n",
    "                find_val = matched.iloc[0].get('æœªç´å…¥æ›¸ç›®FIND')\n",
    "                has_find = pd.notna(find_val) and str(find_val).strip() != ''\n",
    "                print(f\"    - {taicca_id}: {'æœ‰ FIND' if has_find else 'ç„¡ FIND'} (å€¼: {find_val})\")\n",
    "        \n",
    "        result = find_find_values_for_row(test_row, df)\n",
    "        print(f\"\\n  ç”Ÿæˆçš„çµæœ: {result}\")\n",
    "    \n",
    "    # æ¸¬è©¦å–®ä¸€ ID çš„æƒ…æ³\n",
    "    single_id_mask = ~merge_df['NEW_TAICCA_ID'].astype(str).str.contains('/', na=False)\n",
    "    single_id_rows = merge_df[single_id_mask & merge_df['NEW_TAICCA_ID'].notna()]\n",
    "    \n",
    "    if len(single_id_rows) > 0:\n",
    "        test_row = single_id_rows.iloc[0]\n",
    "        print(f\"\\nç¯„ä¾‹ 2: å–®ä¸€ ID\")\n",
    "        print(f\"  NEW_TAICCA_ID: {test_row['NEW_TAICCA_ID']}\")\n",
    "        \n",
    "        matched = df[df['NEW_TAICCA_ID'] == test_row['NEW_TAICCA_ID']]\n",
    "        if len(matched) > 0:\n",
    "            find_val = matched.iloc[0].get('æœªç´å…¥æ›¸ç›®FIND')\n",
    "            has_find = pd.notna(find_val) and str(find_val).strip() != ''\n",
    "            print(f\"  åœ¨ df ä¸­çš„ FIND: {'æœ‰' if has_find else 'ç„¡'} (å€¼: {find_val})\")\n",
    "        \n",
    "        result = find_find_values_for_row(test_row, df)\n",
    "        print(f\"  ç”Ÿæˆçš„çµæœ: {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5f870833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ é–‹å§‹ç‚º merge_df å¡«å…¥ FIND æ¬„ä½...\n",
      "ç¸½å…±éœ€è¦è™•ç† 9276 ç­†è³‡æ–™\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "è™•ç† FIND: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9276/9276 [00:03<00:00, 2579.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… FIND æ¬„ä½å¡«å…¥å®Œæˆï¼\n",
      "  - ç¸½ç­†æ•¸: 9276\n",
      "  - æˆåŠŸå¡«å…¥: 3500 (37.7%)\n",
      "  - ç•™ç©ºï¼ˆæŸ¥ç„¡ FINDï¼‰: 5776 (62.3%)\n",
      "\n",
      "ğŸ“Š FIND é¡å‹çµ±è¨ˆ:\n",
      "  - å–®ä¸€ 'FIND': 2783 ç­†\n",
      "  - åŒ…å«å¤šå€‹çµæœï¼ˆæœ‰ /ï¼‰: 717 ç­†\n",
      "  - åŒ…å«ã€Œç©ºç™½ã€: 515 ç­†\n",
      "\n",
      "ğŸ“‹ æŸ¥çœ‹å‰ 10 ç­†çµæœ:\n",
      "             NEW_TAICCA_ID æœªç´å…¥æ›¸ç›®FIND\n",
      "0  E253-00004 / E253-00003      None\n",
      "1               E253-00010      None\n",
      "2               E253-00011      None\n",
      "3               E253-00050      None\n",
      "4               E253-05218      None\n",
      "5               E253-05219      None\n",
      "6               E253-00493      None\n",
      "7               E253-00494      None\n",
      "8               E253-00495      None\n",
      "9               E253-00496      None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ”„ é–‹å§‹ç‚º merge_df å¡«å…¥ FIND æ¬„ä½...\")\n",
    "print(f\"ç¸½å…±éœ€è¦è™•ç† {len(merge_df)} ç­†è³‡æ–™\\n\")\n",
    "\n",
    "# ä½¿ç”¨ tqdm é¡¯ç¤ºé€²åº¦æ¢\n",
    "find_values = []\n",
    "\n",
    "for idx, row in tqdm(merge_df.iterrows(), total=len(merge_df), desc=\"è™•ç† FIND\"):\n",
    "    find_result = find_find_values_for_row(row, df)\n",
    "    find_values.append(find_result)\n",
    "\n",
    "# å°‡çµæœå¡«å…¥ merge_df\n",
    "merge_df['æœªç´å…¥æ›¸ç›®FIND'] = find_values\n",
    "\n",
    "# çµ±è¨ˆçµæœ\n",
    "total = len(merge_df)\n",
    "filled = merge_df['æœªç´å…¥æ›¸ç›®FIND'].notna().sum()\n",
    "not_filled = total - filled\n",
    "\n",
    "print(f\"\\nâœ… FIND æ¬„ä½å¡«å…¥å®Œæˆï¼\")\n",
    "print(f\"  - ç¸½ç­†æ•¸: {total}\")\n",
    "print(f\"  - æˆåŠŸå¡«å…¥: {filled} ({filled/total*100:.1f}%)\")\n",
    "print(f\"  - ç•™ç©ºï¼ˆæŸ¥ç„¡ FINDï¼‰: {not_filled} ({not_filled/total*100:.1f}%)\")\n",
    "\n",
    "# çµ±è¨ˆä¸åŒé¡å‹çš„çµæœ\n",
    "single_find = (merge_df['æœªç´å…¥æ›¸ç›®FIND'] == 'FIND').sum()\n",
    "multiple_find = merge_df['æœªç´å…¥æ›¸ç›®FIND'].astype(str).str.contains('/', na=False).sum()\n",
    "with_blank = merge_df['æœªç´å…¥æ›¸ç›®FIND'].astype(str).str.contains('ç©ºç™½', na=False).sum()\n",
    "\n",
    "print(f\"\\nğŸ“Š FIND é¡å‹çµ±è¨ˆ:\")\n",
    "print(f\"  - å–®ä¸€ 'FIND': {single_find} ç­†\")\n",
    "print(f\"  - åŒ…å«å¤šå€‹çµæœï¼ˆæœ‰ /ï¼‰: {multiple_find} ç­†\")\n",
    "print(f\"  - åŒ…å«ã€Œç©ºç™½ã€: {with_blank} ç­†\")\n",
    "\n",
    "# é¡¯ç¤ºä¸€äº›ç¯„ä¾‹\n",
    "print(\"\\nğŸ“‹ æŸ¥çœ‹å‰ 10 ç­†çµæœ:\")\n",
    "print(merge_df[['NEW_TAICCA_ID', 'æœªç´å…¥æ›¸ç›®FIND']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c23c936d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ“Š æœ€çµ‚å®Œæ•´çµ±è¨ˆï¼ˆä¿®æ­£ç‰ˆï¼‰\n",
      "================================================================================\n",
      "\n",
      "ğŸ“… æ—¥æœŸæ¬„ä½:\n",
      "  - æœ‰ min_publish_date: 9275 ç­†\n",
      "  - æœ‰ max_publish_date: 9275 ç­†\n",
      "\n",
      "ğŸ’° åƒ¹æ ¼æ¬„ä½:\n",
      "  - æœ‰ price è³‡æ–™: 9194 ç­†\n",
      "  - æœ‰æ•ˆæ•¸å€¼åƒ¹æ ¼: 9194 ç­†\n",
      "  - åƒ¹æ ¼ç¯„åœ: 0.00 ~ 5460.00\n",
      "  - å¹³å‡åƒ¹æ ¼: 240.14\n",
      "  - ä¸­ä½æ•¸åƒ¹æ ¼: 210.00\n",
      "\n",
      "ğŸ” FIND æ¬„ä½:\n",
      "  - æœ‰å¡«å…¥ FIND ç›¸é—œ: 3500 ç­†\n",
      "  - å–®ä¸€ 'FIND': 2783 ç­†\n",
      "  - åŒ…å«å¤šå€‹çµæœï¼ˆæœ‰ /ï¼‰: 717 ç­†\n",
      "  - åŒ…å«ã€Œç©ºç™½ã€: 515 ç­†\n",
      "  - ç•™ç©ºï¼ˆç„¡ FINDï¼‰: 5776 ç­†\n",
      "\n",
      "ğŸ“‹ FIND æ¬„ä½çš„å„ç¨®é¡å‹ç¯„ä¾‹:\n",
      "\n",
      "  å–®ä¸€ 'FIND' ç¯„ä¾‹ (å…± 2783 ç­†):\n",
      "    1. E253-11459: FIND\n",
      "    2. E253-11461: FIND\n",
      "    3. E253-11462: FIND\n",
      "\n",
      "  å¤šå€‹ FIND ç¯„ä¾‹ (å…± 202 ç­†):\n",
      "    1. E253-11465 / E253-11458: FIND / FIND\n",
      "    2. E253-11467 / E253-11460: FIND / FIND\n",
      "    3. E253-12450 / E253-12443: FIND / FIND\n",
      "\n",
      "  æ··åˆå‹ï¼ˆFIND / ç©ºç™½ï¼‰ç¯„ä¾‹ (å…± 515 ç­†):\n",
      "    1. E253-20387 / E253-11466: ç©ºç™½ / FIND\n",
      "    2. E253-20235 / E253-11468: ç©ºç™½ / FIND\n",
      "    3. E253-20168 / E253-11469: ç©ºç™½ / FIND\n",
      "\n",
      "================================================================================\n",
      "âœ… æ‰€æœ‰æ¬„ä½å·²æˆåŠŸè™•ç†ï¼\n",
      "âœ… æœ€çµ‚çµæœå·²å„²å­˜è‡³: final_complete_merged.csv\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ä¿®æ­£ç‰ˆæœ¬çš„æœ€çµ‚çµ±è¨ˆï¼ˆè™•ç† price æ¬„ä½çš„å‹åˆ¥å•é¡Œï¼‰\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ“Š æœ€çµ‚å®Œæ•´çµ±è¨ˆï¼ˆä¿®æ­£ç‰ˆï¼‰\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nğŸ“… æ—¥æœŸæ¬„ä½:\")\n",
    "print(f\"  - æœ‰ min_publish_date: {merge_df['min_publish_date'].notna().sum()} ç­†\")\n",
    "print(f\"  - æœ‰ max_publish_date: {merge_df['max_publish_date'].notna().sum()} ç­†\")\n",
    "\n",
    "print(\"\\nğŸ’° åƒ¹æ ¼æ¬„ä½:\")\n",
    "print(f\"  - æœ‰ price è³‡æ–™: {merge_df['price'].notna().sum()} ç­†\")\n",
    "\n",
    "# è½‰æ›æˆæ•¸å€¼å‹ï¼Œå¿½ç•¥ç„¡æ³•è½‰æ›çš„å€¼\n",
    "price_numeric = pd.to_numeric(merge_df['price'], errors='coerce')\n",
    "valid_prices = price_numeric.dropna()\n",
    "\n",
    "if len(valid_prices) > 0:\n",
    "    print(f\"  - æœ‰æ•ˆæ•¸å€¼åƒ¹æ ¼: {len(valid_prices)} ç­†\")\n",
    "    print(f\"  - åƒ¹æ ¼ç¯„åœ: {valid_prices.min():.2f} ~ {valid_prices.max():.2f}\")\n",
    "    print(f\"  - å¹³å‡åƒ¹æ ¼: {valid_prices.mean():.2f}\")\n",
    "    print(f\"  - ä¸­ä½æ•¸åƒ¹æ ¼: {valid_prices.median():.2f}\")\n",
    "\n",
    "print(\"\\nğŸ” FIND æ¬„ä½:\")\n",
    "print(f\"  - æœ‰å¡«å…¥ FIND ç›¸é—œ: {merge_df['æœªç´å…¥æ›¸ç›®FIND'].notna().sum()} ç­†\")\n",
    "print(f\"  - å–®ä¸€ 'FIND': {(merge_df['æœªç´å…¥æ›¸ç›®FIND'] == 'FIND').sum()} ç­†\")\n",
    "\n",
    "# çµ±è¨ˆåŒ…å«ã€Œ/ã€çš„è³‡æ–™\n",
    "has_slash = merge_df['æœªç´å…¥æ›¸ç›®FIND'].astype(str).str.contains('/', na=False).sum()\n",
    "has_blank = merge_df['æœªç´å…¥æ›¸ç›®FIND'].astype(str).str.contains('ç©ºç™½', na=False).sum()\n",
    "\n",
    "print(f\"  - åŒ…å«å¤šå€‹çµæœï¼ˆæœ‰ /ï¼‰: {has_slash} ç­†\")\n",
    "print(f\"  - åŒ…å«ã€Œç©ºç™½ã€: {has_blank} ç­†\")\n",
    "print(f\"  - ç•™ç©ºï¼ˆç„¡ FINDï¼‰: {merge_df['æœªç´å…¥æ›¸ç›®FIND'].isna().sum()} ç­†\")\n",
    "\n",
    "# é¡¯ç¤ºä¸€äº› FIND é¡å‹çš„ç¯„ä¾‹\n",
    "print(\"\\nğŸ“‹ FIND æ¬„ä½çš„å„ç¨®é¡å‹ç¯„ä¾‹:\")\n",
    "\n",
    "# å–®ä¸€ FIND\n",
    "single_find = merge_df[merge_df['æœªç´å…¥æ›¸ç›®FIND'] == 'FIND']\n",
    "if len(single_find) > 0:\n",
    "    print(f\"\\n  å–®ä¸€ 'FIND' ç¯„ä¾‹ (å…± {len(single_find)} ç­†):\")\n",
    "    for i, (idx, row) in enumerate(single_find.head(3).iterrows(), 1):\n",
    "        print(f\"    {i}. {row['NEW_TAICCA_ID']}: {row['æœªç´å…¥æ›¸ç›®FIND']}\")\n",
    "\n",
    "# FIND / FIND\n",
    "double_find = merge_df[\n",
    "    (merge_df['æœªç´å…¥æ›¸ç›®FIND'].astype(str).str.contains('/', na=False)) &\n",
    "    (~merge_df['æœªç´å…¥æ›¸ç›®FIND'].astype(str).str.contains('ç©ºç™½', na=False))\n",
    "]\n",
    "if len(double_find) > 0:\n",
    "    print(f\"\\n  å¤šå€‹ FIND ç¯„ä¾‹ (å…± {len(double_find)} ç­†):\")\n",
    "    for i, (idx, row) in enumerate(double_find.head(3).iterrows(), 1):\n",
    "        print(f\"    {i}. {row['NEW_TAICCA_ID']}: {row['æœªç´å…¥æ›¸ç›®FIND']}\")\n",
    "\n",
    "# FIND / ç©ºç™½\n",
    "mixed_find = merge_df[merge_df['æœªç´å…¥æ›¸ç›®FIND'].astype(str).str.contains('ç©ºç™½', na=False)]\n",
    "if len(mixed_find) > 0:\n",
    "    print(f\"\\n  æ··åˆå‹ï¼ˆFIND / ç©ºç™½ï¼‰ç¯„ä¾‹ (å…± {len(mixed_find)} ç­†):\")\n",
    "    for i, (idx, row) in enumerate(mixed_find.head(3).iterrows(), 1):\n",
    "        print(f\"    {i}. {row['NEW_TAICCA_ID']}: {row['æœªç´å…¥æ›¸ç›®FIND']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… æ‰€æœ‰æ¬„ä½å·²æˆåŠŸè™•ç†ï¼\")\n",
    "print(f\"âœ… æœ€çµ‚çµæœå·²å„²å­˜è‡³: final_complete_merged.csv\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4ac3a0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å·²å®šç¾©ä¿®æ­£ç‰ˆ find_find_values_for_row_v2 å‡½æ•¸\n",
      "   æ”¹å‹•ï¼šå¡«å…¥åŸå§‹çš„ FIND å€¼ï¼ˆå¦‚ FIND_3092ï¼‰è€Œéåªå¡« 'FIND'\n",
      "\n",
      "ğŸ§ª æ¸¬è©¦ä¿®æ­£ç‰ˆå‡½æ•¸:\n",
      "  æ¸¬è©¦ ID: E253-10000\n",
      "  df ä¸­çš„ FIND å€¼: FIND_01\n",
      "  ä¿®æ­£ç‰ˆå‡½æ•¸è¿”å›: FIND_01\n",
      "  âœ… æˆåŠŸï¼è¿”å›åŸå§‹å€¼è€Œé 'FIND'\n"
     ]
    }
   ],
   "source": [
    "# ä¿®æ­£ç‰ˆï¼šå¡«å…¥åŸå§‹çš„ FIND å€¼ï¼ˆå¦‚ FIND_3092ï¼‰è€Œéåªå¡« \"FIND\"\n",
    "\n",
    "def find_find_values_for_row_v2(row, source_df):\n",
    "    \"\"\"\n",
    "    æ ¹æ“š merge_df çš„æŸä¸€åˆ—ï¼ŒæŸ¥è©¢ source_df (df) ä¸­çš„ FIND æ¬„ä½è³‡æ–™\n",
    "    \n",
    "    **ä¿®æ­£ç‰ˆ**ï¼šå¡«å…¥åŸå§‹çš„å€¼ï¼ˆå¦‚ FIND_3092ï¼‰\n",
    "    \n",
    "    å›å‚³ç¯„ä¾‹ï¼š\n",
    "    - å–®ä¸€ ID æœ‰ FIND â†’ \"FIND_3092\"\n",
    "    - å–®ä¸€ ID æ²’æœ‰ FIND â†’ Noneï¼ˆç•™ç©ºï¼‰\n",
    "    - å…©å€‹ ID éƒ½æœ‰ FIND â†’ \"FIND_3092 / FIND_3093\"\n",
    "    - ç¬¬ä¸€å€‹æœ‰ï¼Œç¬¬äºŒå€‹æ²’æœ‰ â†’ \"FIND_3092 / ç©ºç™½\"\n",
    "    - å…©å€‹ ID éƒ½æ²’æœ‰ FIND â†’ Noneï¼ˆç•™ç©ºï¼‰\n",
    "    \"\"\"\n",
    "    # æ‹†åˆ† TAICCA_ID\n",
    "    taicca_ids = split_taicca_ids(row['NEW_TAICCA_ID'])\n",
    "    \n",
    "    if not taicca_ids:\n",
    "        return None\n",
    "    \n",
    "    find_results = []\n",
    "    \n",
    "    # å°æ¯å€‹ ID æŸ¥è©¢\n",
    "    for taicca_id in taicca_ids:\n",
    "        # åœ¨ source_df ä¸­æŸ¥è©¢è©² ID\n",
    "        matched_rows = source_df[source_df['NEW_TAICCA_ID'] == taicca_id]\n",
    "        \n",
    "        if len(matched_rows) > 0:\n",
    "            # å–ç¬¬ä¸€ç­†åŒ¹é…çš„è³‡æ–™\n",
    "            matched_row = matched_rows.iloc[0]\n",
    "            find_value = matched_row.get('æœªç´å…¥æ›¸ç›®FIND')\n",
    "            \n",
    "            # æª¢æŸ¥æ˜¯å¦æœ‰å€¼\n",
    "            if pd.notna(find_value) and str(find_value).strip() != '':\n",
    "                # å¡«å…¥åŸå§‹çš„å€¼ï¼ˆå¦‚ FIND_3092ï¼‰\n",
    "                find_results.append(str(find_value).strip())\n",
    "            else:\n",
    "                find_results.append(\"ç©ºç™½\")\n",
    "        else:\n",
    "            # æŸ¥ä¸åˆ°è©² ID\n",
    "            find_results.append(\"ç©ºç™½\")\n",
    "    \n",
    "    # å¦‚æœæ‰€æœ‰çµæœéƒ½æ˜¯ã€Œç©ºç™½ã€ï¼Œè¿”å› Noneï¼ˆç•™ç©ºï¼‰\n",
    "    if all(result == \"ç©ºç™½\" for result in find_results):\n",
    "        return None\n",
    "    \n",
    "    # ç”¨ã€Œ / ã€é€£æ¥çµæœ\n",
    "    return \" / \".join(find_results)\n",
    "\n",
    "print(\"âœ… å·²å®šç¾©ä¿®æ­£ç‰ˆ find_find_values_for_row_v2 å‡½æ•¸\")\n",
    "print(\"   æ”¹å‹•ï¼šå¡«å…¥åŸå§‹çš„ FIND å€¼ï¼ˆå¦‚ FIND_3092ï¼‰è€Œéåªå¡« 'FIND'\")\n",
    "\n",
    "# æ¸¬è©¦ä¿®æ­£ç‰ˆå‡½æ•¸\n",
    "print(\"\\nğŸ§ª æ¸¬è©¦ä¿®æ­£ç‰ˆå‡½æ•¸:\")\n",
    "\n",
    "# æ‰¾ä¸€ç­†æœ‰ FIND çš„è³‡æ–™ä¾†æ¸¬è©¦\n",
    "has_find_in_df = df[df['æœªç´å…¥æ›¸ç›®FIND'].notna()]\n",
    "if len(has_find_in_df) > 0:\n",
    "    test_id = has_find_in_df.iloc[0]['NEW_TAICCA_ID']\n",
    "    print(f\"  æ¸¬è©¦ ID: {test_id}\")\n",
    "    print(f\"  df ä¸­çš„ FIND å€¼: {has_find_in_df.iloc[0]['æœªç´å…¥æ›¸ç›®FIND']}\")\n",
    "    \n",
    "    # å‰µå»ºä¸€å€‹æ¸¬è©¦ row\n",
    "    test_row = pd.Series({'NEW_TAICCA_ID': test_id})\n",
    "    result = find_find_values_for_row_v2(test_row, df)\n",
    "    print(f\"  ä¿®æ­£ç‰ˆå‡½æ•¸è¿”å›: {result}\")\n",
    "    print(f\"  âœ… æˆåŠŸï¼è¿”å›åŸå§‹å€¼è€Œé 'FIND'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "11d88fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ é‡æ–°è™•ç† FIND æ¬„ä½ï¼ˆä½¿ç”¨ä¿®æ­£ç‰ˆå‡½æ•¸ï¼‰...\n",
      "ç¸½å…±éœ€è¦è™•ç† 9276 ç­†è³‡æ–™\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "è™•ç† FIND (ä¿®æ­£ç‰ˆ): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9276/9276 [00:03<00:00, 2526.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… FIND æ¬„ä½é‡æ–°å¡«å…¥å®Œæˆï¼\n",
      "  - ç¸½ç­†æ•¸: 9276\n",
      "  - æˆåŠŸå¡«å…¥: 3500 (37.7%)\n",
      "  - ç•™ç©ºï¼ˆæŸ¥ç„¡ FINDï¼‰: 5776 (62.3%)\n",
      "\n",
      "ğŸ“‹ æŸ¥çœ‹æœ‰ FIND å€¼çš„å‰ 10 ç­†:\n",
      "              NEW_TAICCA_ID              æœªç´å…¥æ›¸ç›®FIND\n",
      "27  E253-11465 / E253-11458  FIND_2320 / FIND_2314\n",
      "28               E253-11459              FIND_2315\n",
      "29  E253-11467 / E253-11460  FIND_2322 / FIND_2316\n",
      "30               E253-11461              FIND_2317\n",
      "31               E253-11462              FIND_2318\n",
      "32               E253-11463              FIND_2319\n",
      "33  E253-20387 / E253-11466         ç©ºç™½ / FIND_2321\n",
      "34  E253-20235 / E253-11468         ç©ºç™½ / FIND_2323\n",
      "35  E253-20168 / E253-11469         ç©ºç™½ / FIND_2324\n",
      "36  E253-20170 / E253-11470         ç©ºç™½ / FIND_2325\n",
      "\n",
      "âœ… ç¢ºèªï¼šç¬¬ä¸€ç­† FIND å€¼ç‚º 'FIND_2320 / FIND_2314'\n",
      "   é€™æ˜¯åŸå§‹å€¼ï¼Œä¸æ˜¯åªå¡« 'FIND' âœ…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ”„ é‡æ–°è™•ç† FIND æ¬„ä½ï¼ˆä½¿ç”¨ä¿®æ­£ç‰ˆå‡½æ•¸ï¼‰...\")\n",
    "print(f\"ç¸½å…±éœ€è¦è™•ç† {len(merge_df)} ç­†è³‡æ–™\\n\")\n",
    "\n",
    "# ä½¿ç”¨ä¿®æ­£ç‰ˆå‡½æ•¸é‡æ–°è™•ç†\n",
    "find_values_v2 = []\n",
    "\n",
    "for idx, row in tqdm(merge_df.iterrows(), total=len(merge_df), desc=\"è™•ç† FIND (ä¿®æ­£ç‰ˆ)\"):\n",
    "    find_result = find_find_values_for_row_v2(row, df)\n",
    "    find_values_v2.append(find_result)\n",
    "\n",
    "# å°‡çµæœå¡«å…¥ merge_df\n",
    "merge_df['æœªç´å…¥æ›¸ç›®FIND'] = find_values_v2\n",
    "\n",
    "# çµ±è¨ˆçµæœ\n",
    "total = len(merge_df)\n",
    "filled = merge_df['æœªç´å…¥æ›¸ç›®FIND'].notna().sum()\n",
    "not_filled = total - filled\n",
    "\n",
    "print(f\"\\nâœ… FIND æ¬„ä½é‡æ–°å¡«å…¥å®Œæˆï¼\")\n",
    "print(f\"  - ç¸½ç­†æ•¸: {total}\")\n",
    "print(f\"  - æˆåŠŸå¡«å…¥: {filled} ({filled/total*100:.1f}%)\")\n",
    "print(f\"  - ç•™ç©ºï¼ˆæŸ¥ç„¡ FINDï¼‰: {not_filled} ({not_filled/total*100:.1f}%)\")\n",
    "\n",
    "# é¡¯ç¤ºä¸€äº›ç¯„ä¾‹ä¾†ç¢ºèªåŸå§‹å€¼æœ‰è¢«å¡«å…¥\n",
    "print(\"\\nğŸ“‹ æŸ¥çœ‹æœ‰ FIND å€¼çš„å‰ 10 ç­†:\")\n",
    "has_find = merge_df[merge_df['æœªç´å…¥æ›¸ç›®FIND'].notna()]\n",
    "if len(has_find) > 0:\n",
    "    print(has_find[['NEW_TAICCA_ID', 'æœªç´å…¥æ›¸ç›®FIND']].head(10))\n",
    "    \n",
    "    # ç¢ºèªä¸æ˜¯åªå¡« \"FIND\" è€Œæ˜¯åŸå§‹å€¼\n",
    "    first_find_value = has_find.iloc[0]['æœªç´å…¥æ›¸ç›®FIND']\n",
    "    print(f\"\\nâœ… ç¢ºèªï¼šç¬¬ä¸€ç­† FIND å€¼ç‚º '{first_find_value}'\")\n",
    "    print(f\"   é€™æ˜¯åŸå§‹å€¼ï¼Œä¸æ˜¯åªå¡« 'FIND' âœ…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ddb65f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ“Š FIND æ¬„ä½ä¿®æ­£å¾Œçš„è©³ç´°ç¯„ä¾‹\n",
      "================================================================================\n",
      "\n",
      "ğŸ“‹ ç¯„ä¾‹ 1: å–®ä¸€ ID æœ‰ FINDï¼ˆé¡¯ç¤ºåŸå§‹å€¼ï¼‰\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. NEW_TAICCA_ID: E253-11459\n",
      "   df ä¸­çš„åŸå§‹ FIND å€¼: FIND_2315\n",
      "   merge_df ä¸­å¡«å…¥çš„å€¼: FIND_2315\n",
      "   âœ… åŸå§‹å€¼å·²æ­£ç¢ºå¡«å…¥\n",
      "\n",
      "2. NEW_TAICCA_ID: E253-11461\n",
      "   df ä¸­çš„åŸå§‹ FIND å€¼: FIND_2317\n",
      "   merge_df ä¸­å¡«å…¥çš„å€¼: FIND_2317\n",
      "   âœ… åŸå§‹å€¼å·²æ­£ç¢ºå¡«å…¥\n",
      "\n",
      "3. NEW_TAICCA_ID: E253-11462\n",
      "   df ä¸­çš„åŸå§‹ FIND å€¼: FIND_2318\n",
      "   merge_df ä¸­å¡«å…¥çš„å€¼: FIND_2318\n",
      "   âœ… åŸå§‹å€¼å·²æ­£ç¢ºå¡«å…¥\n",
      "\n",
      "ğŸ“‹ ç¯„ä¾‹ 2: å¤šå€‹ ID éƒ½æœ‰ FINDï¼ˆé¡¯ç¤ºåŸå§‹å€¼ï¼‰\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "NEW_TAICCA_ID: E253-11465 / E253-11458\n",
      "æ‹†åˆ†å¾Œ: ['E253-11465', 'E253-11458']\n",
      "\n",
      "å„ ID åœ¨ df ä¸­çš„åŸå§‹ FIND å€¼:\n",
      "  - E253-11465: FIND_2320\n",
      "  - E253-11458: FIND_2314\n",
      "\n",
      "merge_df ä¸­å¡«å…¥çš„å€¼: FIND_2320 / FIND_2314\n",
      "âœ… åŸå§‹å€¼å·²æ­£ç¢ºå¡«å…¥ï¼ˆç”¨ / é€£æ¥ï¼‰\n",
      "\n",
      "ğŸ“‹ ç¯„ä¾‹ 3: æ··åˆå‹ï¼ˆåŸå§‹å€¼ / ç©ºç™½ï¼‰\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "NEW_TAICCA_ID: E253-20387 / E253-11466\n",
      "æ‹†åˆ†å¾Œ: ['E253-20387', 'E253-11466']\n",
      "\n",
      "å„ ID åœ¨ df ä¸­çš„ FIND:\n",
      "  - E253-20387: (ç„¡) â†’ ç©ºç™½\n",
      "  - E253-11466: FIND_2321\n",
      "\n",
      "merge_df ä¸­å¡«å…¥çš„å€¼: ç©ºç™½ / FIND_2321\n",
      "\n",
      "================================================================================\n",
      "\n",
      "âœ… å·²é‡æ–°å„²å­˜æœ€çµ‚çµæœè‡³: final_complete_merged.csv\n",
      "   ç¸½ç­†æ•¸: 9276\n",
      "   æ¬„ä½æ•¸: 59\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š æœ€çµ‚å®Œæ•´çµ±è¨ˆï¼ˆä¿®æ­£ç‰ˆï¼‰\n",
      "================================================================================\n",
      "\n",
      "ğŸ“… æ—¥æœŸæ¬„ä½:\n",
      "  - æœ‰ min_publish_date: 9275 ç­†\n",
      "  - æœ‰ max_publish_date: 9275 ç­†\n",
      "\n",
      "ğŸ’° åƒ¹æ ¼æ¬„ä½:\n",
      "  - æœ‰ price è³‡æ–™: 9194 ç­†\n",
      "  - æœ‰æ•ˆæ•¸å€¼åƒ¹æ ¼: 9194 ç­†\n",
      "  - åƒ¹æ ¼ç¯„åœ: 0.00 ~ 5460.00\n",
      "  - å¹³å‡åƒ¹æ ¼: 240.14\n",
      "\n",
      "ğŸ” FIND æ¬„ä½:\n",
      "  - æœ‰å¡«å…¥ FIND: 3500 ç­†\n",
      "  - ç•™ç©ºï¼ˆç„¡ FINDï¼‰: 5776 ç­†\n",
      "  - ç´” FIND å€¼ï¼ˆå–®ä¸€æˆ–å¤šå€‹ï¼‰: 2985 ç­†\n",
      "  - åŒ…å«å¤šå€‹å€¼ï¼ˆæœ‰ /ï¼‰: 717 ç­†\n",
      "  - åŒ…å«ã€Œç©ºç™½ã€: 515 ç­†\n",
      "\n",
      "================================================================================\n",
      "ğŸ‰ æ‰€æœ‰æ¬„ä½è™•ç†å®Œæˆï¼FIND æ¬„ä½å·²ä½¿ç”¨åŸå§‹å€¼ï¼\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# é‡æ–°å„²å­˜æœ€çµ‚çµæœä¸¦é¡¯ç¤ºè©³ç´°çµ±è¨ˆ\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ“Š FIND æ¬„ä½ä¿®æ­£å¾Œçš„è©³ç´°ç¯„ä¾‹\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ç¯„ä¾‹ 1: å–®ä¸€ FIND åŸå§‹å€¼\n",
    "single_find_rows = merge_df[\n",
    "    merge_df['æœªç´å…¥æ›¸ç›®FIND'].notna() & \n",
    "    (~merge_df['æœªç´å…¥æ›¸ç›®FIND'].astype(str).str.contains('/', na=False))\n",
    "]\n",
    "if len(single_find_rows) > 0:\n",
    "    print(\"\\nğŸ“‹ ç¯„ä¾‹ 1: å–®ä¸€ ID æœ‰ FINDï¼ˆé¡¯ç¤ºåŸå§‹å€¼ï¼‰\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, (idx, example) in enumerate(single_find_rows.head(3).iterrows(), 1):\n",
    "        print(f\"\\n{i}. NEW_TAICCA_ID: {example['NEW_TAICCA_ID']}\")\n",
    "        \n",
    "        # æŸ¥è©¢ df ä¸­çš„åŸå§‹å€¼\n",
    "        matched = df[df['NEW_TAICCA_ID'] == example['NEW_TAICCA_ID']]\n",
    "        if len(matched) > 0:\n",
    "            find_val = matched.iloc[0].get('æœªç´å…¥æ›¸ç›®FIND')\n",
    "            print(f\"   df ä¸­çš„åŸå§‹ FIND å€¼: {find_val}\")\n",
    "        \n",
    "        print(f\"   merge_df ä¸­å¡«å…¥çš„å€¼: {example['æœªç´å…¥æ›¸ç›®FIND']}\")\n",
    "        print(f\"   âœ… åŸå§‹å€¼å·²æ­£ç¢ºå¡«å…¥\")\n",
    "\n",
    "# ç¯„ä¾‹ 2: å¤šå€‹ FIND åŸå§‹å€¼\n",
    "multi_find_rows = merge_df[\n",
    "    (merge_df['æœªç´å…¥æ›¸ç›®FIND'].astype(str).str.contains('/', na=False)) & \n",
    "    (~merge_df['æœªç´å…¥æ›¸ç›®FIND'].astype(str).str.contains('ç©ºç™½', na=False))\n",
    "]\n",
    "if len(multi_find_rows) > 0:\n",
    "    print(\"\\nğŸ“‹ ç¯„ä¾‹ 2: å¤šå€‹ ID éƒ½æœ‰ FINDï¼ˆé¡¯ç¤ºåŸå§‹å€¼ï¼‰\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    example = multi_find_rows.iloc[0]\n",
    "    print(f\"\\nNEW_TAICCA_ID: {example['NEW_TAICCA_ID']}\")\n",
    "    \n",
    "    # æ‹†åˆ†ä¸¦æŸ¥è©¢æ¯å€‹ ID çš„åŸå§‹å€¼\n",
    "    ids = split_taicca_ids(example['NEW_TAICCA_ID'])\n",
    "    print(f\"æ‹†åˆ†å¾Œ: {ids}\")\n",
    "    \n",
    "    print(\"\\nå„ ID åœ¨ df ä¸­çš„åŸå§‹ FIND å€¼:\")\n",
    "    for taicca_id in ids:\n",
    "        matched = df[df['NEW_TAICCA_ID'] == taicca_id]\n",
    "        if len(matched) > 0:\n",
    "            find_val = matched.iloc[0].get('æœªç´å…¥æ›¸ç›®FIND')\n",
    "            print(f\"  - {taicca_id}: {find_val if pd.notna(find_val) else '(ç„¡)'}\")\n",
    "    \n",
    "    print(f\"\\nmerge_df ä¸­å¡«å…¥çš„å€¼: {example['æœªç´å…¥æ›¸ç›®FIND']}\")\n",
    "    print(f\"âœ… åŸå§‹å€¼å·²æ­£ç¢ºå¡«å…¥ï¼ˆç”¨ / é€£æ¥ï¼‰\")\n",
    "\n",
    "# ç¯„ä¾‹ 3: æ··åˆå‹ï¼ˆæœ‰åŸå§‹å€¼ / ç©ºç™½ï¼‰\n",
    "mixed_rows = merge_df[merge_df['æœªç´å…¥æ›¸ç›®FIND'].astype(str).str.contains('ç©ºç™½', na=False)]\n",
    "if len(mixed_rows) > 0:\n",
    "    print(\"\\nğŸ“‹ ç¯„ä¾‹ 3: æ··åˆå‹ï¼ˆåŸå§‹å€¼ / ç©ºç™½ï¼‰\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    example = mixed_rows.iloc[0]\n",
    "    print(f\"\\nNEW_TAICCA_ID: {example['NEW_TAICCA_ID']}\")\n",
    "    \n",
    "    ids = split_taicca_ids(example['NEW_TAICCA_ID'])\n",
    "    print(f\"æ‹†åˆ†å¾Œ: {ids}\")\n",
    "    \n",
    "    print(\"\\nå„ ID åœ¨ df ä¸­çš„ FIND:\")\n",
    "    for taicca_id in ids:\n",
    "        matched = df[df['NEW_TAICCA_ID'] == taicca_id]\n",
    "        if len(matched) > 0:\n",
    "            find_val = matched.iloc[0].get('æœªç´å…¥æ›¸ç›®FIND')\n",
    "            has_find = pd.notna(find_val) and str(find_val).strip() != ''\n",
    "            print(f\"  - {taicca_id}: {find_val if has_find else '(ç„¡) â†’ ç©ºç™½'}\")\n",
    "    \n",
    "    print(f\"\\nmerge_df ä¸­å¡«å…¥çš„å€¼: {example['æœªç´å…¥æ›¸ç›®FIND']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# é‡æ–°å„²å­˜æœ€çµ‚çµæœ\n",
    "output_file = \"final_complete_merged.csv\"\n",
    "merge_df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"\\nâœ… å·²é‡æ–°å„²å­˜æœ€çµ‚çµæœè‡³: {output_file}\")\n",
    "print(f\"   ç¸½ç­†æ•¸: {len(merge_df)}\")\n",
    "print(f\"   æ¬„ä½æ•¸: {len(merge_df.columns)}\")\n",
    "\n",
    "# æœ€çµ‚çµ±è¨ˆ\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ“Š æœ€çµ‚å®Œæ•´çµ±è¨ˆï¼ˆä¿®æ­£ç‰ˆï¼‰\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nğŸ“… æ—¥æœŸæ¬„ä½:\")\n",
    "print(f\"  - æœ‰ min_publish_date: {merge_df['min_publish_date'].notna().sum()} ç­†\")\n",
    "print(f\"  - æœ‰ max_publish_date: {merge_df['max_publish_date'].notna().sum()} ç­†\")\n",
    "\n",
    "print(\"\\nğŸ’° åƒ¹æ ¼æ¬„ä½:\")\n",
    "price_numeric = pd.to_numeric(merge_df['price'], errors='coerce')\n",
    "valid_prices = price_numeric.dropna()\n",
    "print(f\"  - æœ‰ price è³‡æ–™: {merge_df['price'].notna().sum()} ç­†\")\n",
    "if len(valid_prices) > 0:\n",
    "    print(f\"  - æœ‰æ•ˆæ•¸å€¼åƒ¹æ ¼: {len(valid_prices)} ç­†\")\n",
    "    print(f\"  - åƒ¹æ ¼ç¯„åœ: {valid_prices.min():.2f} ~ {valid_prices.max():.2f}\")\n",
    "    print(f\"  - å¹³å‡åƒ¹æ ¼: {valid_prices.mean():.2f}\")\n",
    "\n",
    "print(\"\\nğŸ” FIND æ¬„ä½:\")\n",
    "print(f\"  - æœ‰å¡«å…¥ FIND: {merge_df['æœªç´å…¥æ›¸ç›®FIND'].notna().sum()} ç­†\")\n",
    "print(f\"  - ç•™ç©ºï¼ˆç„¡ FINDï¼‰: {merge_df['æœªç´å…¥æ›¸ç›®FIND'].isna().sum()} ç­†\")\n",
    "\n",
    "# çµ±è¨ˆä¸åŒé¡å‹çš„ FIND å€¼\n",
    "has_slash = merge_df['æœªç´å…¥æ›¸ç›®FIND'].astype(str).str.contains('/', na=False).sum()\n",
    "has_blank = merge_df['æœªç´å…¥æ›¸ç›®FIND'].astype(str).str.contains('ç©ºç™½', na=False).sum()\n",
    "only_find_values = filled - has_blank  # ç´” FIND å€¼ï¼ˆä¸å«ç©ºç™½ï¼‰\n",
    "\n",
    "print(f\"  - ç´” FIND å€¼ï¼ˆå–®ä¸€æˆ–å¤šå€‹ï¼‰: {only_find_values} ç­†\")\n",
    "print(f\"  - åŒ…å«å¤šå€‹å€¼ï¼ˆæœ‰ /ï¼‰: {has_slash} ç­†\")\n",
    "print(f\"  - åŒ…å«ã€Œç©ºç™½ã€: {has_blank} ç­†\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ‰ æ‰€æœ‰æ¬„ä½è™•ç†å®Œæˆï¼FIND æ¬„ä½å·²ä½¿ç”¨åŸå§‹å€¼ï¼\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "86d12846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ“Š åˆä½µè³‡æ–™çµ±è¨ˆåˆ†æ\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ˆ ç¸½é«”çµ±è¨ˆ:\n",
      "  - ç¸½ç­†æ•¸: 9276\n",
      "  - åˆä½µçš„è³‡æ–™: 1993 ç­† (21.5%)\n",
      "  - æœªåˆä½µçš„è³‡æ–™: 7283 ç­† (78.5%)\n",
      "\n",
      "ğŸ“Š åˆä½µè³‡æ–™çš„è©³ç´°åˆ†æ:\n",
      "\n",
      "  ä¾æ“šåŒ…å«çš„ ID æ•¸é‡åˆ†é¡:\n",
      "    - åŒ…å« 2 å€‹ ID: 1993 ç­†\n",
      "\n",
      "  æœ€å¤šåŒ…å«çš„ ID æ•¸: 2 å€‹\n",
      "  ç¯„ä¾‹: E253-00004 / E253-00003\n",
      "\n",
      "ğŸ’¡ é€™ 1993 ç­†åˆä½µè³‡æ–™ï¼Œä¾†è‡ªåŸå§‹çš„ 3986 ç­†è³‡æ–™\n",
      "   æ¸›å°‘äº† 1993 ç­†é‡è¤‡è³‡æ–™\n",
      "\n",
      "ğŸ“‹ åˆä½µè³‡æ–™ç¯„ä¾‹ï¼ˆå‰ 10 ç­†ï¼‰:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "0. E253-00004 / E253-00003\n",
      "   åŒ…å« 2 å€‹ ID: ['E253-00004', 'E253-00003']\n",
      "   æ›¸å: (é™)Promiseï¼šä¼Šè—¤èˆé›ªå¯«çœŸé›†\n",
      "\n",
      "17. E253-04757 / E253-04756\n",
      "   åŒ…å« 2 å€‹ ID: ['E253-04757', 'E253-04756']\n",
      "   æ›¸å: ç•°é„‰çš„ä¸‰å°‘çˆºâ€”æ—ç»å ‚(ã€Œæ—¥æ²»æ™‚ä»£è‡ºç£äººç‰©ã€è‡ºèªç¹ªæœ¬ç³»åˆ—ç¬¬ä¸‰å½ˆï¼)(é›»å­æ›¸)\n",
      "\n",
      "19. E253-04760 / E253-04759\n",
      "   åŒ…å« 2 å€‹ ID: ['E253-04760', 'E253-04759']\n",
      "   æ›¸å: ç•°æ•¸(é›»å­æ›¸)\n",
      "\n",
      "27. E253-11465 / E253-11458\n",
      "   åŒ…å« 2 å€‹ ID: ['E253-11465', 'E253-11458']\n",
      "   æ›¸å: ç››ä¸–ç‹æœ(13)\n",
      "\n",
      "29. E253-11467 / E253-11460\n",
      "   åŒ…å« 2 å€‹ ID: ['E253-11467', 'E253-11460']\n",
      "   æ›¸å: ç››ä¸–ç‹æœ(15)\n",
      "\n",
      "33. E253-20387 / E253-11466\n",
      "   åŒ…å« 2 å€‹ ID: ['E253-20387', 'E253-11466']\n",
      "   æ›¸å: ç››ä¸–ç‹æœ(14)\n",
      "\n",
      "34. E253-20235 / E253-11468\n",
      "   åŒ…å« 2 å€‹ ID: ['E253-20235', 'E253-11468']\n",
      "   æ›¸å: ç››ä¸–ç‹æœ(16)\n",
      "\n",
      "35. E253-20168 / E253-11469\n",
      "   åŒ…å« 2 å€‹ ID: ['E253-20168', 'E253-11469']\n",
      "   æ›¸å: ç››ä¸–ç‹æœ(17)\n",
      "\n",
      "36. E253-20170 / E253-11470\n",
      "   åŒ…å« 2 å€‹ ID: ['E253-20170', 'E253-11470']\n",
      "   æ›¸å: ç››ä¸–ç‹æœ(18)\n",
      "\n",
      "37. E253-20034 / E253-11471\n",
      "   åŒ…å« 2 å€‹ ID: ['E253-20034', 'E253-11471']\n",
      "   æ›¸å: ç››ä¸–ç‹æœ(19)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ğŸ“‹ æœªåˆä½µè³‡æ–™ç¯„ä¾‹ï¼ˆå‰ 5 ç­†ï¼‰:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. E253-00010\n",
      "   æ›¸å: ã€ŠFlameshootç”·ç‚ä¹‹å½±No.006ã€‹æš—é»‘æ˜ŸåŸŸï¼å°ç‹å­â€”è±ªHao\n",
      "\n",
      "2. E253-00011\n",
      "   æ›¸å: ã€ŠFlameshootç”·ç‚ä¹‹å½±No.006ã€‹æš—é»‘æ˜ŸåŸŸï¼å°ç‹å­â€”è±ªHaoï¼šå°ç‹å­å…¨è¦‹ç‰ˆ\n",
      "\n",
      "3. E253-00050\n",
      "   æ›¸å: ã€å±±è‘‰éŸ³æ¨‚å‡ºå“ï¼Œçµ¦å¤§å®¶çš„éŸ³æ¨‚ç´ é¤Šèª²ã€å¹½é»˜æ¼«ç•«ç‰ˆã€‘å¥—æ›¸ã€‘ï¼ˆäºŒå†Šï¼‰ï¼šã€Šçµ¦å¤§å®¶ã®å¤å…¸éŸ³æ¨‚å°å²â… ã€å¤ä»£â”€å·´æ´›...\n",
      "\n",
      "4. E253-05218\n",
      "   æ›¸å: çµ¦å¤§å®¶ã®å¤å…¸éŸ³æ¨‚å°å²â… ã€å¤ä»£â”€å·´æ´›å…‹ç¯‡ã€‘ï¼šå±±è‘‰éŸ³æ¨‚å‡ºå“ï¼Œforå¤§äººï¼†å°å­©çš„éŸ³æ¨‚ç´ é¤Šèª²ï¼Œæ¼«ç•«å¹½é»˜é–‹å ´ï¼Œ...\n",
      "\n",
      "5. E253-05219\n",
      "   æ›¸å: çµ¦å¤§å®¶ã®å¤å…¸éŸ³æ¨‚å°å²â…¡ã€å¤å…¸æµªæ¼«â”€è¿‘ç¾ä»£ç¯‡ã€‘ï¼šå±±è‘‰éŸ³æ¨‚å‡ºå“ï¼Œforå¤§äººï¼†å°å­©çš„éŸ³æ¨‚ç´ é¤Šèª²ï¼Œæ¼«ç•«å¹½é»˜é–‹...\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"ğŸ“Š åˆä½µè³‡æ–™çµ±è¨ˆåˆ†æ\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# åˆ¤æ–·æ˜¯å¦ç‚ºåˆä½µè³‡æ–™ï¼ˆNEW_TAICCA_ID åŒ…å«ã€Œ/ã€ï¼‰\n",
    "is_merged = merge_df['NEW_TAICCA_ID'].astype(str).str.contains('/', na=False)\n",
    "\n",
    "# çµ±è¨ˆ\n",
    "total_records = len(merge_df)\n",
    "merged_records = is_merged.sum()\n",
    "non_merged_records = total_records - merged_records\n",
    "\n",
    "print(f\"\\nğŸ“ˆ ç¸½é«”çµ±è¨ˆ:\")\n",
    "print(f\"  - ç¸½ç­†æ•¸: {total_records}\")\n",
    "print(f\"  - åˆä½µçš„è³‡æ–™: {merged_records} ç­† ({merged_records/total_records*100:.1f}%)\")\n",
    "print(f\"  - æœªåˆä½µçš„è³‡æ–™: {non_merged_records} ç­† ({non_merged_records/total_records*100:.1f}%)\")\n",
    "\n",
    "# åˆ†æåˆä½µçš„è³‡æ–™åŒ…å«å¹¾å€‹ ID\n",
    "print(f\"\\nğŸ“Š åˆä½µè³‡æ–™çš„è©³ç´°åˆ†æ:\")\n",
    "\n",
    "merged_df = merge_df[is_merged].copy()\n",
    "\n",
    "if len(merged_df) > 0:\n",
    "    # è¨ˆç®—æ¯ç­†è³‡æ–™åŒ…å«å¤šå°‘å€‹ ID\n",
    "    merged_df['id_count'] = merged_df['NEW_TAICCA_ID'].apply(lambda x: len(split_taicca_ids(x)))\n",
    "    \n",
    "    # çµ±è¨ˆä¸åŒ ID æ•¸é‡çš„åˆ†å¸ƒ\n",
    "    id_count_stats = merged_df['id_count'].value_counts().sort_index()\n",
    "    \n",
    "    print(\"\\n  ä¾æ“šåŒ…å«çš„ ID æ•¸é‡åˆ†é¡:\")\n",
    "    for id_count, count in id_count_stats.items():\n",
    "        print(f\"    - åŒ…å« {id_count} å€‹ ID: {count} ç­†\")\n",
    "    \n",
    "    # æœ€å¤šåŒ…å«å¹¾å€‹ ID\n",
    "    max_ids = merged_df['id_count'].max()\n",
    "    max_id_example = merged_df[merged_df['id_count'] == max_ids].iloc[0]\n",
    "    \n",
    "    print(f\"\\n  æœ€å¤šåŒ…å«çš„ ID æ•¸: {max_ids} å€‹\")\n",
    "    print(f\"  ç¯„ä¾‹: {max_id_example['NEW_TAICCA_ID']}\")\n",
    "    \n",
    "    # è¨ˆç®—ç¸½å…±åˆä½µäº†å¤šå°‘ç­†åŸå§‹è³‡æ–™\n",
    "    total_original_ids = merged_df['id_count'].sum()\n",
    "    print(f\"\\nğŸ’¡ é€™ {merged_records} ç­†åˆä½µè³‡æ–™ï¼Œä¾†è‡ªåŸå§‹çš„ {total_original_ids} ç­†è³‡æ–™\")\n",
    "    print(f\"   æ¸›å°‘äº† {total_original_ids - merged_records} ç­†é‡è¤‡è³‡æ–™\")\n",
    "\n",
    "# é¡¯ç¤ºä¸€äº›åˆä½µè³‡æ–™çš„ç¯„ä¾‹\n",
    "print(f\"\\nğŸ“‹ åˆä½µè³‡æ–™ç¯„ä¾‹ï¼ˆå‰ 10 ç­†ï¼‰:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "merged_examples = merge_df[is_merged][['NEW_TAICCA_ID', 'title']].head(10)\n",
    "for idx, row in merged_examples.iterrows():\n",
    "    ids = split_taicca_ids(row['NEW_TAICCA_ID'])\n",
    "    title = row['title'][:50] + '...' if len(str(row['title'])) > 50 else row['title']\n",
    "    print(f\"\\n{idx}. {row['NEW_TAICCA_ID']}\")\n",
    "    print(f\"   åŒ…å« {len(ids)} å€‹ ID: {ids}\")\n",
    "    print(f\"   æ›¸å: {title}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# é¡¯ç¤ºæœªåˆä½µè³‡æ–™çš„ç¯„ä¾‹\n",
    "print(f\"\\nğŸ“‹ æœªåˆä½µè³‡æ–™ç¯„ä¾‹ï¼ˆå‰ 5 ç­†ï¼‰:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "non_merged_examples = merge_df[~is_merged][['NEW_TAICCA_ID', 'title']].head(5)\n",
    "for idx, row in non_merged_examples.iterrows():\n",
    "    title = row['title'][:50] + '...' if len(str(row['title'])) > 50 else row['title']\n",
    "    print(f\"\\n{idx}. {row['NEW_TAICCA_ID']}\")\n",
    "    print(f\"   æ›¸å: {title}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_embedding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
